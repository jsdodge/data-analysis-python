{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting complex functions\n",
    "\n",
    "Python activities to complement [*Measurements and their Uncertainties*](https://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 6, \"Least-squares fitting of complex functions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Linear fits with non-uniform errors](#Linear-fits-with-non-uniform-errors)\n",
    "    * [Explicit formulas](#Explicit-formulas)\n",
    "    * [Non-uniform errors with `polyfit`](#Non-uniform-errors-with-polyfit)\n",
    "* [Residual analysis with non-uniform errors](#Residual-analysis-with-non-uniform-errors)\n",
    "    * [Exercise 1](#Exercise-1)\n",
    "* [Polynomial fits](#Polynomial-fits)\n",
    "* [Least-squares fits that are nonlinear in the parameters](#Least-squares-fits-that-are-nonlinear-in-the-parameters)\n",
    "    * [Plot the data](#Plot-the-data)\n",
    "    * [Choose initial parameter values for the fit](#Choose-initial-parameter-values-for-the-fit)\n",
    "    * [Fit the model to the data and examine the residuals](#Fit-the-model-to-the-data-and-examine-the-residuals)\n",
    "    * [Exercise 2](#Exercise-2)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [previous notebook](5.0-Visualization-and-modeling.ipynb) (see also the additional notebook on [linear fits and residuals](5.1-Linear-fits.ipynb)) and read *MU* Ch. 6, \"Least-squares fitting of complex functions,\" with the following [goals](A.0-Reading-goals.ipynb#Data-visualization-and-reduction) in mind.\n",
    "\n",
    "1. Be able to recall and use the *&chi;*<sup>2</sup> goodness-of-fit parameter (6.1) to fit a model to data, including:\n",
    "    1. determine the best fit parameters by constructing and minimizing *&chi;*<sup>2</sup>;\n",
    "    2. account for non-uniform uncertainties, for example for measurements drawn from a Poisson distribution; and\n",
    "    3. recognize the need for the weights in Equations (6.3)-(6.7) for a linear fit with non-uniform uncertainties.\n",
    "2. Be able to identify measurement strategies that will minimize the uncertainty of a particular fit parameter, for example the slope or the intercept in a linear fit.\n",
    "3. Recognize the need to use normalized residuals to evaluate fits with non-uniform uncertainties.\n",
    "4. Recognize the distinction between linear and nonlinear fits; specifically, that:\n",
    "    1. model functions that are linear in the parameters possess closed-form solutions like Equations (6.3)-(6.7) with a single set of optimal parameters; \n",
    "    2. model functions that are nonlinear in the parameters require numerical optimization techniques that are not guaranteed to have a single global solution;\n",
    "    3. nonlinear fit algorithms require an initial guess for the model parameters, while linear fits do not; and\n",
    "    4. the initial parameter guess may influence the solution found in a nonlinear fit.\n",
    "5. Be able to construct contours of the *&chi;*<sup>2</sup> function in a two-parameter fit, and use them to estimate parameter uncertainties.\n",
    "6. Recognize the value and practical challenge of combining results from different measurements, each fit with different models, but using the same underlying fit parameters, e.g., fitting the complex function (6.15) to measurements of the real and imaginary part of a frequency response.\n",
    "7. Be able to test for correlations in fit residuals using lag plots and the Durbin-Watson statistic, Eq. (6.16); be able to explain why such correlations may indicate a poor fit of the model to the experimental data.\n",
    "\n",
    "The following code cell includes previously used initialization commands that we will need here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear fits with non-uniform errors\n",
    "### Explicit formulas\n",
    "\n",
    "Now we consider the general case of the goodness-of-fit statistic,\n",
    "\n",
    "$$\\chi^2(m,c;\\{x_k\\},\\{y_k\\},\\{\\alpha_k\\}) = \\sum_{k=1}^N\\frac{[y_k - (mx_k +  c)]^2}{\\alpha_k^2}, \\label{eq:lsq}\\tag{1}$$\n",
    "\n",
    "where the errors $\\{\\alpha_k\\}$ are non-uniform. Differentiating with respect to the fit parameters $\\{m, c\\}$ to find the extremum, we obtain\n",
    "\n",
    "\\begin{align}\n",
    "\\left.\\frac{\\partial{\\chi^2}}{\\partial m}\\right|_{\\hat{m},\\hat{c}} &= -2\\sum_{k=1}^N[y_k - (\\hat{m}x_k + \\hat{c})]\\,x_k\\,\\alpha_{k}^{-2} = 0,\\\\\n",
    "\\left.\\frac{\\partial{\\chi^2}}{\\partial c}\\right|_{\\hat{m},\\hat{c}} &= -2\\sum_{k=1}^N[y_k - (\\hat{m}x_k + \\hat{c})]\\,\\alpha_{k}^{-2} = 0.\n",
    "\\end{align}\n",
    "\n",
    "We define $w_k = \\alpha_k^{-2}$ as the *weight* of point $k$ in the sum; points with larger uncertainty have smaller weight. Solving this system of equations for $\\hat{m}$ and $\\hat{c}$ give *MU* Eqs.&nbsp;(6.3) and (6.4) (rewritten for consistency with the notation here),\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\frac{\\left(\\sum_k w_k x_k^2\\right)\\left(\\sum_k w_k y_k\\right) - \\left(\\sum_k w_k x_k\\right)\\left(\\sum_k w_k x_k y_k\\right)}{\\Delta'}, \\label{eq:c-hat}\\tag{2}\\\\\n",
    "\\hat{m} &= \\frac{\\sum_k w_k \\sum_k w_k x_k y_k - \\left(\\sum_k w_k x_k\\right)\\left(\\sum_k w_k y_k\\right)}{\\Delta'},\\quad\\text{with}\\\\\n",
    "\\Delta' &= \\sum_k w_k \\sum_k w_k x_k^2 - \\left(\\sum_k w_k x_k\\right)^2. \\label{eq:m-hat}\\tag{3}\n",
    "\\end{align}\n",
    "\n",
    "Propagating the error as we did for uniform measurement uncertainty, we get the parameter uncertainties\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{\\hat{m}} &= \\sqrt{\\frac{\\sum_k w_k}{\\Delta'}}, & \\alpha_{\\hat{c}} &= \\sqrt{\\frac{\\sum_{k=1}^N w_k x_k^2}{\\Delta'}}. \\label{eq:alpha-m-c}\\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "We can demonstrate these computations for the data tabulated in *MU* Exercise 6.1 and shown in Fig. 6.1(d), which is contained in the file [`Example-Data.csv`](data/Example-Data.csv). Compare the results to those for the weighted fits in Fig. 6.2(b) given in *MU* Table 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "frequency, voltage, alpha_voltage = np.genfromtxt('data/Example-Data.csv', skip_header=1, delimiter=',',unpack=True)\n",
    "\n",
    "# Compute weights and sum over weights\n",
    "w = 1/alpha_voltage**2\n",
    "sw = np.sum(w)\n",
    "\n",
    "# Weighted sums over x and y\n",
    "swx = np.sum(w*frequency)\n",
    "swy = np.sum(w*voltage)\n",
    "\n",
    "# Weighted sums over x**2 and x*y\n",
    "swx2 = np.sum(w*frequency**2)\n",
    "swxy = np.sum(w*frequency*voltage)\n",
    "\n",
    "# Evaluate Delta'\n",
    "Delta_prime = sw*swx2 - swx**2\n",
    "\n",
    "# Put it all together for m_hat and c_hat\n",
    "c_hat = (swx2*swy - swx*swxy)/Delta_prime\n",
    "m_hat = (sw*swxy - swx*swy)/Delta_prime\n",
    "\n",
    "# Compute standard error for m_hat and c_hat\n",
    "alpha_c_hat = np.sqrt(swx2/Delta_prime)\n",
    "alpha_m_hat = np.sqrt(sw/Delta_prime)\n",
    "\n",
    "# Print results\n",
    "print(\"Intercept estimate with given uncertainty: ({0:.0f} ± {1:.0f}) mV\".format(c_hat, alpha_c_hat))\n",
    "print(\"Slope estimate with given uncertainty: ({0:.2f} ± {1:.2f}) mV/Hz\".format(m_hat, alpha_m_hat))\n",
    "print()\n",
    "\n",
    "# Plot data and fit\n",
    "frequency_model = np.linspace(0, 120)\n",
    "plt.errorbar(frequency, voltage, yerr=alpha_voltage, fmt='o')\n",
    "plt.plot(frequency_model, m_hat*frequency_model + c_hat, '-')\n",
    "\n",
    "# Format plot\n",
    "plt.xlim(0,120)\n",
    "plt.ylim(0,250)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.xlabel('Frequency (Hz)', fontsize=12)\n",
    "plt.ylabel('Voltage (mV)', fontsize=12)\n",
    "\n",
    "plt.legend(['Data','Fit'], fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-uniform errors with `polyfit`\n",
    "\n",
    "As we saw in the earlier notebook on [linear fits and residuals](5.1-Linear-fits.ipynb), the [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) function includes an optional `w` parameter that allows for non-uniform errors. Note [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) uses a non-standard definition for the weights, `w = 1/alpha_voltage` (the NumPy developers are transitioning to the newer [`Polynomial.fit`](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit) class method that will fix this problem, but currently this method lacks the ability to return the parameter covariance matrix, so we will stick with [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)).\n",
    "\n",
    "The code cell below shows how to use [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) to compute $\\hat{m}$, $\\hat{c}$, $\\alpha_\\hat{m}$, and $\\alpha_\\hat{c}$ for the data in [`Example-Data.csv`](data/Example-Data.csv). Compare with the previous results from the explicit calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fit\n",
    "p, V = np.polyfit(frequency, voltage, 1, w=1/alpha_voltage, cov='unscaled')\n",
    "\n",
    "# Print results\n",
    "print(\"Intercept estimate: ({0:.0f} ± {1:.0f}) mV\".format(p[1], np.sqrt(V[1][1])))\n",
    "print(\"Slope estimate: ({0:.2f} ± {1:.2f}) mV/Hz\".format(p[0], np.sqrt(V[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual analysis with non-uniform errors\n",
    "\n",
    "As discussed in *MU* Sec. 6.3.2 and shown in *MU* 6.4, it is easier to use normalized residuals to assess the fit quality when the measurement errors are not uniform.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "In the cell below, copy the residual plot code from the *Example: Residuals for Ohm's law measurement* section of the [linear fits and residuals](5.1-Linear-fits.ipynb) notebook, and modify it to show the fit and normalized residuals for the data in [`Example-Data.csv`](data/Example-Data.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial fits\n",
    "\n",
    "Now let's download another data set from the [NIST Statistical Reference Database](https://www.itl.nist.gov/div898/strd/index.html) and determine a suitable polynomial model for it using the [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "disp, load = np.genfromtxt('https://www.itl.nist.gov/div898/strd/lls/data/LINKS/DATA/Pontius.dat',\n",
    "                     skip_header=60, unpack=True)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(load, disp, 'k.')\n",
    "plt.xlabel('Load (arb. units)')\n",
    "plt.ylabel('Displacement (arb. units)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks linear, so we can fit it with a linear model and examine the residuals. Note that we use the [`polyval`](https://numpy.org/doc/stable/reference/generated/numpy.polyval.html) function to evaluate the polynomial with parameters `p` at each value of the `load` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fit\n",
    "p, V = np.polyfit(load, disp, 1, cov=True)\n",
    "\n",
    "# Show optimized fit parameters and uncertainties\n",
    "print(\"p[0] = ({0:.3f} ± {1:.3f}) x 1e-7\".format(1e7*p[0], 1e7*np.sqrt(V[0][0])))\n",
    "print(\"p[1] = ({0:.1f} ± {1:.1f}) x 1e-3\".format(1e3*p[1], 1e3*np.sqrt(V[1][1])))\n",
    "\n",
    "# Show fit with residuals\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Make a grid with 2 rows, 1 colum, a 3:1 height ratio, and no \n",
    "# height space between the rows\n",
    "gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0)\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure()\n",
    "load_range = [0, np.max(load)]\n",
    "\n",
    "# Compute the measurement residuals\n",
    "residuals = disp - np.polyval(p, load)\n",
    "\n",
    "# Compute the model curve\n",
    "load_model = np.linspace(load_range[0], load_range[1])\n",
    "disp_model = np.polyval(p, load_model)\n",
    "\n",
    "# Make the main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "plt.plot(load, disp, 'k.')\n",
    "plt.plot(load_model, disp_model, 'r-')\n",
    "plt.xlim(load_range[0],load_range[1])\n",
    "plt.ylabel('Deflection (arb. units)')\n",
    "\n",
    "# Hide x-tick labels to avoid overlap with residual plot\n",
    "ax_main.set_xticklabels([])\n",
    "\n",
    "# Make the residual plot with a dotted zero line\n",
    "# Need to adjust ylim,  yticks, ylabel for readability\n",
    "ax_res = fig.add_subplot(gs[1])\n",
    "plt.plot(load, residuals, 'k.')\n",
    "plt.plot(load_range, [0,0], 'k:')\n",
    "plt.xlim(load_range[0], load_range[1])\n",
    "# plt.ylim(-2,2)\n",
    "# plt.yticks([-1,0,1])\n",
    "plt.xlabel(\"Load (arb. units)\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "# plt.xlabel('Load (arb. units)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals reveal a clear pattern, indicating deviations from linearity that were not evident when we visually inspected the plot. To improve the fit, we can try increasing the polynomial order from 1 to 2. Note that [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) returns polynomial coefficients in decreasing order,\n",
    "\n",
    "$$\n",
    "y = a_0 x^p + a_1 x^{p-1} + \\ldots + a_{p-1}x + a_p.\n",
    "$$\n",
    "\n",
    "As we see below, the residuals for $p=2$ look relatively unstructured, indicating that a 2nd-order polynomial is a satisfactory model for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fit with deg=2\n",
    "p2, V2 = np.polyfit(load, disp, 2, cov=True)\n",
    "\n",
    "# Show optimized fit parameters and uncertainties\n",
    "print(\"p2[0] = ({0:.2f} ± {1:.2f}) x 1e-15\".format(1e15*p2[0], 1e15*np.sqrt(V2[0][0])))\n",
    "print(\"p2[1] = ({0:.3f} ± {1:.3f}) x 1e-7\".format(1e7*p2[1], 1e7*np.sqrt(V2[1][1])))\n",
    "print(\"p2[2] = ({0:.1f} ± {1:.1f}) x 1e-3\".format(1e3*p2[2], 1e3*np.sqrt(V2[2][2])))\n",
    "\n",
    "# Show fit with residuals\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Make a grid with 2 rows, 1 colum, a 3:1 height ratio, and no \n",
    "# height space between the rows\n",
    "gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0)\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure()\n",
    "load_range = [0, np.max(load)]\n",
    "\n",
    "# Compute the measurement residuals\n",
    "residuals = disp - np.polyval(p2, load)\n",
    "\n",
    "# Compute the model curve\n",
    "load_model = np.linspace(load_range[0], load_range[1])\n",
    "disp_model = np.polyval(p2, load_model)\n",
    "\n",
    "# Make the main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "plt.plot(load, disp, 'k.')\n",
    "plt.plot(load_model, disp_model, 'r-')\n",
    "plt.xlim(load_range[0],load_range[1])\n",
    "plt.ylabel('Deflection (arb. units)')\n",
    "\n",
    "# Hide x-tick labels to avoid overlap with residual plot\n",
    "ax_main.set_xticklabels([])\n",
    "\n",
    "# Make the residual plot with a dotted zero line\n",
    "# Need to adjust ylim,  yticks, ylabel for readability\n",
    "ax_res = fig.add_subplot(gs[1])\n",
    "plt.plot(load, residuals, 'k.')\n",
    "plt.plot(load_range, [0,0], 'k:')\n",
    "plt.xlim(load_range[0], load_range[1])\n",
    "# plt.ylim(-2,2)\n",
    "# plt.yticks([-1,0,1])\n",
    "plt.xlabel(\"Load (arb. units)\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "# plt.xlabel('Load (arb. units)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-squares fits that are nonlinear in the parameters\n",
    "\n",
    "Typically, fitting a model with a nonlinear dependence on one or more of the parameters requires more computational effort than a model that has only linear dependence on the parameters. An example of a nonlinear model is the one for an $LCR$ circuit given by *MU* Eq.&nbsp;(6.11),\n",
    "\n",
    "$$\n",
    "V(t) = V_\\text{bgd} + V_0\\cos\\left(\\frac{2\\pi}{T}t + \\phi\\right)\\exp(-t/\\tau).\\label{eq:LCR}\\tag{1}\n",
    "$$\n",
    "\n",
    "This model is *linear* in $V_\\text{bgd}$ and $V_0$, but *nonlinear* in $T$, $\\phi$, and $\\tau$.\n",
    "\n",
    "To see how we fit such a model to data, we first define a Python function to evaluate Eq.&nbsp;(\\ref{eq:LCR}), and use this to simulate measurements that we will then fit with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def LCR_model(t, V_bgd, V0, T, phi, tau):\n",
    "    return V_bgd + V0*np.cos(2*np.pi*t/T + phi)*np.exp(-t/tau)\n",
    "\n",
    "# Define time points\n",
    "t = np.linspace(40,950,2000);\n",
    "\n",
    "# Define model parameters\n",
    "V_bgd = 0.3;\n",
    "V0 = 8;\n",
    "T = 39;\n",
    "phi = 4.5;\n",
    "tau = 200;\n",
    "\n",
    "# Define noise amplitude\n",
    "alpha_V = 0.075;\n",
    "\n",
    "# Simulate data\n",
    "# Compute ideal values\n",
    "V_ideal = LCR_model(t, V_bgd, V0, T, phi, tau)\n",
    "\n",
    "# Generate noise with amplitude alpha_V\n",
    "rg = default_rng(0)\n",
    "V_noise = alpha_V*rg.normal(size=t.size)\n",
    "\n",
    "# Simulated data is the sum of the ideal values and noise\n",
    "V_sim = V_ideal + V_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.plot(t, V_sim, 'k.')\n",
    "plt.xlabel('Time (µs)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose initial parameter values for the fit\n",
    "\n",
    "We know the parameters that we used this simulation, but let's determine them from visual inspection of the plot.\n",
    "1. The background voltage is close to zero, so choose `V_bgd = 0`.\n",
    "2. Extrapolating the envelope to $t=0$, choose `V0 = 8`.\n",
    "3. The circuit oscillates through about four cycles in 200 µs, so choose `T = 50`.\n",
    "4. The voltage is near zero at $t = 40~\\mu\\text{s}\\approx T$, so choose `phi = 3π/2`.\n",
    "5. The envelope decays by about $1/e$ after about 250 µs, so choose `tau = 250`.\n",
    "\n",
    "Compare the model with these parameters to the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign initial parameters\n",
    "V_bgd_init = 0\n",
    "V0_init = 8\n",
    "T_init = 50\n",
    "phi_init = 3*np.pi/2\n",
    "tau_init = 250\n",
    "\n",
    "# Plot simulation and preliminary model\n",
    "plt.plot(t, V_sim, 'k.')\n",
    "plt.plot(t, LCR_model(t, V_bgd_init, V0_init, T_init, phi_init, tau_init), 'r-')\n",
    "plt.xlabel('Time (µs)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the data and examine the residuals\n",
    "\n",
    "These parameters do not fit the simulated data very well, but they are close enough for the solver to find a best-fit solution that is close to the simulation. We take `alpha_V` as the measurement uncertainty and set `absolute_sigma=True` to tell the solver that it should assume the uncertainty is known independently when calculating the parameter uncertainties. The value of `V0_opt` is negative because `phi_opt` is different from `phi_init` by π; otherwise, all of the best-fit parameter values are within one standard error of the associated simulation input parameter. We show the fit and the residuals as separate plots here because it is usually better to inspect them both at full scale before combining them in a multipanel figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data with curve_fit\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "pOpt, pCov = curve_fit(LCR_model, t, V_sim, \n",
    "                       p0=[V_bgd_init, V0_init, T_init, phi_init, tau_init],\n",
    "                       sigma=alpha_V*np.ones(t.size),\n",
    "                       absolute_sigma=True)\n",
    "\n",
    "# Assign each element of pOpt to a named variable\n",
    "V_bgd_opt = pOpt[0]\n",
    "V0_opt = pOpt[1]\n",
    "T_opt = pOpt[2]\n",
    "phi_opt = pOpt[3]\n",
    "tau_opt = pOpt[4]\n",
    "\n",
    "# Compute the standard errors for the parameters\n",
    "# by taking the square root of the diagonal elements of pCov\n",
    "alpha_vec = np.sqrt(np.diag(pCov))\n",
    "\n",
    "# Assign each element of alpha_vec to a named variable\n",
    "alpha_V_bgd = alpha_vec[0]\n",
    "alpha_V0 = alpha_vec[1]\n",
    "alpha_T = alpha_vec[2]\n",
    "alpha_phi = alpha_vec[3]\n",
    "alpha_tau = alpha_vec[4]\n",
    "\n",
    "# Show optimized fit parameters and uncertainties\n",
    "print(\"V = {0:.4f} ± {1:.4f}\".format(V_bgd_opt, alpha_V_bgd))\n",
    "print(\"V0 = {0:.2f} ± {1:.2f}\".format(V0_opt, alpha_V0))\n",
    "print(\"T = {0:.3f} ± {1:.3f}\".format(T_opt, alpha_T))\n",
    "print(\"phi = {0:.3f} ± {1:.3f}\".format(phi_opt, alpha_phi))\n",
    "print(\"tau = {0:.1f} ± {1:.1f}\".format(tau_opt, alpha_tau))\n",
    "\n",
    "# Plot data\n",
    "plt.plot(t, V_sim, 'k.')\n",
    "plt.plot(t, LCR_model(t, V_bgd_opt, V0_opt, T_opt, phi_opt, tau_opt), 'r-')\n",
    "plt.xlabel('Time (µs)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.show()\n",
    "\n",
    "# Plot the residuals in a separate figure\n",
    "plt.plot(t, (V_sim - LCR_model(t, V_bgd_opt, V0_opt, T_opt, phi_opt, tau_opt))/alpha_V, 'k.')\n",
    "plt.xlabel('Time (µs)')\n",
    "plt.ylabel(f'Normalized\\nresidual (mV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The following cell includes code to import data from the [NIST Statistical Reference Database](https://www.itl.nist.gov/div898/strd/index.html) into the variables `x` and `y` and plots it.\n",
    "\n",
    "By following the example above, use [`curve_fit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) to fit the following model to the data, assuming that the uncertainty in `y` is uniform with $\\alpha = 2.5$.\n",
    "\n",
    "$$\n",
    "y(x) = p_0\\exp(-p_1 x) + p_2\\exp\\left[-\\frac{(x - p_3)^2}{p_4^2}\\right]  + p_5\\exp\\left[-\\frac{(x - p_6)^2}{p_7^2}\\right]\n",
    "$$\n",
    "\n",
    "Use format strings to display each fit parameter and its uncertainty to the appropriate precision, and plot the fit and the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 2\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "\n",
    "y, x = np.genfromtxt('https://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Gauss1.dat',\n",
    "                     skip_header=60, unpack=True)\n",
    "plt.plot(x, y, 'k.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is a list of what you should be able to do after completing this notebook.\n",
    "\n",
    "* Use Eqs.&nbsp;(2)–(4) to compute the parameters and parameter uncertainties of a linear fit when the experimental uncertainty is not uniform.\n",
    "* Use [`polyfit`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) to compute the parameters and parameter uncertainties of a linear fit when the experimental uncertainty is not uniform.\n",
    "* [Define a function](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) that represents a theoretical model for experimental results.\n",
    "* Use [`curve_fit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) to fit a theoretical model to experimental results.\n",
    "* Interpret and evaluate the results of [`curve_fit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "Notebook by J. S. Dodge, 2020. The notebook text is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. See more at [Creative Commons](http://creativecommons.org/licenses/by-nc-nd/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
