{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bd4cc5",
   "metadata": {},
   "source": [
    "# Reading goals for Hughes and Hase\n",
    "\n",
    "Below are the goals for each reading assignment in Hughes and Hase.\n",
    "\n",
    "* [Errors in the physical sciences](#Errors-in-the-physical-sciences)\n",
    "* [Random error in measurements](#Random-error-in-measurements)\n",
    "* [Uncertainties as probabilities](#Uncertainties-as-probabilities)\n",
    "* [Error propagation](#Error-propagation)\n",
    "* [Data visualization and reduction](#Data-visualization-and-reduction)\n",
    "* [Least-squares fitting of complex functions](#Least-squares-fitting-of-complex-functions)\n",
    "* [Computer minimization and the error matrix](#Computer-minimization-and-the-error-matrix)\n",
    "* [Hypothesis testing - how good are our models?](#Hypothesis-testing---how-good-are-our-models?)\n",
    "* [Topics for further study](#Topics-for-further-study)\n",
    "* [Exercises](#Exercises)\n",
    "\n",
    "\n",
    "## Errors in the physical sciences \n",
    "1.  Be able to explain what the following terms mean and be able to\n",
    "    provide representative examples:\n",
    "    1.  An accurate measurement;\n",
    "    2.  A precise measurement;\n",
    "    3.  Random errors/uncertainty;\n",
    "    4.  Systematic errors/uncertainty;\n",
    "    5.  Mistakes.\n",
    "\n",
    "## Random error in measurements \n",
    "1.  Be able to use the mean and standard deviation to characterize a\n",
    "    statistical sample. Specifically, be able to:\n",
    "    1.  Estimate the mean and standard deviation from a histogram;\n",
    "    2.  Sketch a distribution with a given mean and standard deviation;\n",
    "    3.  Calculate the mean for a set of measurements;\n",
    "    4.  Calculate the standard deviation for a set of measurements.\n",
    "2.  Know the functional form for a Gaussian distribution, be able to\n",
    "    sketch it for a given mean and standard deviation, and be able to\n",
    "    estimate the mean and standard deviation from the plot of a Gaussian\n",
    "    distribution.\n",
    "3.  Be able to discuss the relationships between a sample and its parent\n",
    "    distribution.\n",
    "4.  Be able to discuss the difference between the standard deviation and\n",
    "    the standard deviation in the mean (aka the standard error), and be\n",
    "    able to estimate both from a histogram of measurements.\n",
    "5.  Be able to report uncertainties correctly for a given measurement.\n",
    "\n",
    "## Uncertainties as probabilities \n",
    "1.  Be able to explain what a probability distribution function\n",
    "    $P_{DF}(x)$ represents and why Eqs. (3.1)–(3.6) follow from its\n",
    "    definition.\n",
    "2.  Be able to recall and use Eqs. (3.1)-(3.3) to perform simple\n",
    "    probability calculations for an arbitrary $P_{DF}(x)$, including:\n",
    "    1.  check that $P_{DF}(x)$ is properly normalized, and identify the\n",
    "        correct normalization factor if it is not;\n",
    "    2.  evaluate the expectation value of a function $f(x)$; and\n",
    "    3.  evaluate the expectation value of the mean and the variance.\n",
    "3.  Be able to recall the definitions (3.7) and (3.8) of the Gaussian\n",
    "    probability distribution function and the error function,\n",
    "    respectively, and know how to use the error function in simple\n",
    "    probability calculations like the one given in Sec. 3.2.2.\n",
    "4.  Be aware of the rules described in Sec. 3.3.2 for rejecting\n",
    "    outliers, be able to follow a well-defined procedure for doing so,\n",
    "    and be able to suggest alternatives to throwing away data points.\n",
    "5.  Be able to describe the basic properties of a Poisson distribution\n",
    "    $P(N; \\bar{N})$,\n",
    "    including:\n",
    "    1.  its functional form;\n",
    "    2.  the kind of experimental data that will be described by it;\n",
    "    3.  the expectation values of its mean and variance; and\n",
    "    4.  the Gaussian probability distribution that approximates it for\n",
    "        $N\\rightarrow\\infty$.\n",
    "6.  Be able to sketch a Poisson distribution for a given mean and\n",
    "    standard deviation, and be able to estimate the mean and standard\n",
    "    deviation from the plot of a Poisson distribution.\n",
    "7.  Be able to state the central limit theorem and recognize how it is\n",
    "    used to justify the assumption of Gaussian errors in many\n",
    "    experiments.\n",
    "\n",
    "## Error propagation\n",
    "\n",
    "1.  Be able to explain why Eq. (4.7) gives the approximate uncertainty\n",
    "    in the single-variable function $Z(A)$ when there is uncertainty\n",
    "    in the argument $A$, and be able to discuss the limitations of this\n",
    "    approximation.\n",
    "2.  Be able to derive all of the results in Table 4.1 and use Eq. (4.7)\n",
    "    in concrete examples.\n",
    "3.  Be able to explain why the component uncertainties $\\alpha_Z^A$,\n",
    "    $\\alpha_Z^B$, $\\alpha_Z^C$,..., add in quadrature to give $\\alpha_Z$\n",
    "    in Eq. (4.10), and recognize (for now) that this expression is\n",
    "    restricted to independent, uncorrelated variables (Ch. 7 discusses\n",
    "    the reason for this restriction in more depth).\n",
    "4.  Be able to explain why Eq. (4.16) gives the approximate uncertainty\n",
    "    in the multivariable function $Z\\left(A, B, C,\\ldots\\right)$ when\n",
    "    there is uncertainty in the arguments $A$, $B$, $C$,..., and be able\n",
    "    to discuss the limitations of this approximation—here, as in (3)\n",
    "    above, it is enough for you to recognize that the variables must be\n",
    "    independent and uncorrelated.\n",
    "5.  Be able to derive all of the results in Table 4.2 and use Eq. (4.16)\n",
    "    in concrete examples.\n",
    "6.  Be able to use error propagation methods to identify the dominant\n",
    "    uncertainty in an experiment.\n",
    "7.  Be able to find the weighted mean and its standard error\n",
    "    $\\alpha_{CE}$ for a set of numbers $\\left\\{x_i\\right\\}$ with\n",
    "    uncertainties $\\left\\{\\alpha_i\\right\\}$.\n",
    "\n",
    "## Data visualization and reduction\n",
    "\n",
    "1.  Be able to recall the \"Guidelines for plotting data\" in Sec. 5.1,\n",
    "    and apply them to your own graphs.\n",
    "2.  Be able to compute appropriate error bars for data in a graph.\n",
    "3.  Be able to assess the quality of a fit from the fraction of data\n",
    "    points that lie within one standard error bar from the fitted curve.\n",
    "4.  Recognize that a least-squares fit to a line can be computed from\n",
    "    the data using Eqs. (5.1) - (5.6).\n",
    "5.  Be able to explain the meaning and significance of the following\n",
    "    terms:\n",
    "    1.  interpolate;\n",
    "    2.  extrapolate;\n",
    "    3.  aliasing;\n",
    "    4.  residual;\n",
    "    5.  method of least squares; and\n",
    "    6.  goodness-of-fit parameter.\n",
    "6.  Be able to explain why $P\\left(m,c\\right)$ in (5.8) is maximized\n",
    "    when $\\chi^2$ in (5.9) is minimized, and discuss how this provides a\n",
    "    rationale for using $\\chi^2$ to determine optimal fit parameters.\n",
    "7.  Know how to graph data to help identify systematic errors.\n",
    "\n",
    "## Least-squares fitting of complex functions\n",
    "\n",
    "1.  Be able to recall and use the $\\chi^2$ goodness-of-fit parameter (6.1)\n",
    "    to fit a model to data, including:\n",
    "    1.  determine the best fit parameters by constructing and minimizing\n",
    "        $\\chi^2$;\n",
    "    2.  account for non-uniform uncertainties, for example for\n",
    "        measurements drawn from a Poisson distribution; and\n",
    "    3.  recognize the need for the weights in Equations (6.3)-(6.7) for\n",
    "        a linear fit with non-uniform uncertainties.\n",
    "2.  Be able to identify measurement strategies that will minimize the\n",
    "    uncertainty of a particular fit parameter, for example the slope or\n",
    "    the intercept in a linear fit.\n",
    "3.  Recognize the need to use normalized residuals to evaluate fits with\n",
    "    non-uniform uncertainties.\n",
    "4.  Recognize the distinction between linear and nonlinear fits;\n",
    "    specifically, that:\n",
    "    1.  model functions that are linear in the parameters possess\n",
    "        closed-form solutions like Equations (6.3)-(6.7) with a single\n",
    "        set of optimal parameters;\n",
    "    2.  model functions that are nonlinear in the parameters require\n",
    "        numerical optimization techniques that are not guaranteed to\n",
    "        have a single global solution;\n",
    "    3.  nonlinear fit algorithms require an initial guess for the model\n",
    "        parameters, while linear fits do not; and\n",
    "    4.  the initial parameter guess may influence the solution found in\n",
    "        a nonlinear fit.\n",
    "5.  Be able to construct contours of the $\\chi^2$ function in a\n",
    "    two-parameter fit, and use them to estimate parameter uncertainties.\n",
    "6.  Recognize the value and practical challenge of combining results\n",
    "    from different measurements, each fit with different models, but\n",
    "    using the same underlying fit parameters, e.g., fitting the complex\n",
    "    function (6.15) to measurements of the real and imaginary part of a\n",
    "    frequency response.\n",
    "7.  Be able to test for correlations in fit residuals using lag plots\n",
    "    and the Durbin-Watson statistic, Eq. (6.16); be able to explain why\n",
    "    such correlations may indicate a poor fit of the model to the\n",
    "    experimental data.\n",
    "\n",
    "## Computer minimization and the error matrix\n",
    "\n",
    "1.  Be able to explain qualitatively how data analysis computer programs\n",
    "    fit a model to data by minimizing the $\\chi^2$ goodness-of-fit\n",
    "    parameter as a function of the model parameters. Specifically,\n",
    "    1.  recognize that the terms *grid search*, *gradient-descent*,\n",
    "        *Newton\\'s method*, *Gauss-Newton*, and *Levenberg-Marquardt*\n",
    "        refer to different algorithms for minimizing a function;\n",
    "    2.  be able to use matrix notation to expand $\\chi^2$ to second order\n",
    "        around a particular point in space, as shown in (7.6); and\n",
    "    3.  be able to write the gradient vector, Hessian matrix, and\n",
    "        Jacobian matrix for a function of multiple variables, and\n",
    "        explain how they appear in the context of computer minimization\n",
    "        routines.\n",
    "2.  Be able to describe how the curvature matrix relates to the error\n",
    "    surface near $\\chi^2_{\\min}$, and how it can be used to estimate the\n",
    "    parameter values at which $\\chi^2=\\chi_{\\text{min}}^2+1$.\n",
    "3.  Be able to describe the meaning and significance of the covariance\n",
    "    matrix and the correlation matrix.\n",
    "4.  Recognize that the covariance matrix can be estimated from a fit by\n",
    "    inverting the curvature matrix at the minimum of the error surface.\n",
    "\n",
    "## Hypothesis testing - how good are our models?\n",
    "\n",
    "1.  Be able to explain how to use the $\\chi^2$ statistic to test whether a\n",
    "    set of measurements are consistent with a theoretical model, and why\n",
    "    it is useful in this context; also, be able to apply the $\\chi^2$ test\n",
    "    to assess the agreement between an experiment and a theoretical\n",
    "    description of it. Specifically,\n",
    "    1.  be able to state the null hypothesis that is being tested with\n",
    "        the $\\chi^2$ test: that the model function $f(\\theta; x)$ with the\n",
    "        best-fit parameters\n",
    "        $\\mathbf{\\theta} = \\left\\{\\theta_1,\\ldots,\\theta_{\\mathcal{N} }\\right\\}$\n",
    "        and independent variables $\\mathbf{x} = \\{x_1,\\ldots,x_N\\}$ is\n",
    "        consistent with measurements $\\mathbf{y} = \\{y_1,\\ldots,y_N\\}$\n",
    "        that have independently known Gaussian uncertainties\n",
    "        $\\left\\{\\alpha_1,\\ldots,\\alpha_N\\right\\}$ for each measurement;\n",
    "    2.  be able to explain what the term \\\"degrees of freedom\\\" means in\n",
    "        the context of a statistical estimate;\n",
    "    3.  be able to calculate the number of degrees of freedom in a fit;\n",
    "    4.  given $N$ data points with known uncertainties and a model with\n",
    "        $\\mathcal{N}$ parameters that are optimized to yield a\n",
    "        goodness-of-fit statistic $\\chi_\\mathrm{min}^2$, be able to\n",
    "        assess the quality of the fit, both qualitatively (by comparing\n",
    "        $\\chi_\\mathrm{min}^2$ with the number of degrees of freedom) and\n",
    "        quantitatively (by computing $P(\\chi_\\mathrm{min}^2;\\nu)$).\n",
    "2.  Be able to describe a variety of methods, including the $\\chi^2$ test,\n",
    "    to assess the quality of a fit (see Sec. 8.5.3).\n",
    "3.  Recognize that the confidence limits for a parameter estimate will\n",
    "    be given by the Student's *t*-distribution when the uncertainty is\n",
    "    not known independently, and that this is usually the implicit\n",
    "    assumption of curve-fitting software; also, be able to explain why\n",
    "    the Student *t*-distribution will approach the Gaussian distribution\n",
    "    when the number of degrees of freedom is sufficiently large.\n",
    "4.  Recognize that curve-fitting software often scales fit parameter\n",
    "    uncertainties automatically, and how this can lead to erroneous\n",
    "    assessments of both the fit quality and the parameter uncertainties\n",
    "    (see the two **health warnings** in Sec. 8.9); also, recognize the\n",
    "    limited context in which scaling uncertainties is appropriate.\n",
    "\n",
    "## Topics for further study\n",
    "\n",
    "1.  Be able to describe a few alternatives to the conventional\n",
    "    least-squares method for curve fitting, and discuss situations for\n",
    "    which they may be useful; for example,\n",
    "    1.  Eq. (9.1) and (9.2) for a fit a straight line when there are\n",
    "        uncertainties in both variables;\n",
    "    2.  Eq. (9.3) for a fit to a more general function when there are\n",
    "        uncertainties in both variables;\n",
    "    3.  Eq. (9.6) for a fit that optimizes the orthogonal distances\n",
    "        instead of the differences in the dependent variable; and\n",
    "2.  Recognize that simulated annealing and genetic algorithms are\n",
    "    techniques for finding global minima, and can be useful for complex\n",
    "    minimization problems.\n",
    "3.  Recognize the value of using computer simulations to evaluate\n",
    "    statistical hypotheses; in particular,\n",
    "    1.  Monte Carlo simulations for estimating statistical quantities\n",
    "        from a known model;\n",
    "    2.  Bootstrap methods for estimating statistical quantities directly\n",
    "        from data, without the need for a model.\n",
    "4.  Recognize that the terms *frequentist* and *Bayesian* refer to\n",
    "    different approaches to statistical inference problems, and that\n",
    "    both approaches involve the principle of maximum likelihood (see\n",
    "    Sec. 5.3).\n",
    "5. Be aware of the [Guide to the Expression of Uncertainty in\n",
    "Measurement](https://www.bipm.org/en/publications/guides/gum.html), or\n",
    "GUM. Note that a significant revision of the GUM is under way: the current version classifies uncertainties into two categories, \"Type A\" and \"Type B\", and uses a frequentist approach to Type A and a Bayesian approach for Type B. The revised version will use a Bayesian framework for everything.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "[Selected solutions to exercises in Hughes and\n",
    "Hase](https://wiki.its.sfu.ca/departments/phys-students/index.php/Selected_solutions_to_exercises_in_Hughes_and_Hase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
