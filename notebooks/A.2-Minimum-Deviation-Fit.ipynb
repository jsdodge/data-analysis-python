{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimum deviation fit example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This example is based on [Template-Fit.ipynb](https://gitlab.rcg.sfu.ca/jsdodge/data-analysis-python/blob/master/notebooks/Template-Fit.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Load the data\n",
    "### Load the data: examine the format\n",
    "We will import a set of simulated data produced by [simulate-minimum-deviation.ipynb](https://gitlab.rcg.sfu.ca/jsdodge/data-analysis-python/blob/master/notebooks/simulate-minimum-deviation.ipynb). Change the file name from `mindevdatasim.csv` to the name of the file containing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Display file contents\n",
    "with open(\"data/mindevdatasim.csv\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load the data: import the data into an array\n",
    "Change the file name from `mindevdatasim.csv` to the name of the file containing your data and adjust the options to match your data file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load file into array\n",
    "data = np.genfromtxt(\"data/mindevdatasim.csv\", delimiter=\",\", skip_header=1)\n",
    "print(\"data =\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assign the columns of `data` to new, more meaningful variables. We assign a uniform error of 0.5 minutes of arc here, but you may want to assign an uncertainty to each of your measurements and record it as a separate column in your data file. Also, the input angles here are chosen so that there is no need for an additional column to specify additional minutes of arc, but you may prefer to include this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assign data columns to theta_deg and delta_deg and assign a uniform error of 0.5 minutes of arc to err_deg\n",
    "theta_deg = data[:, 0]\n",
    "delta_deg = data[:, 1] + data[:, 2] / 60\n",
    "err_deg = (0.5 / 60) * np.ones(theta_deg.shape)\n",
    "print(\"theta (deg) =\", theta_deg)\n",
    "print(\"delta (deg) =\", delta_deg)\n",
    "print(\"err (deg) =\", err_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Plot the data\n",
    "Note that the error bars are smaller than the data markers in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.errorbar(theta_deg, delta_deg, yerr=err_deg, fmt=\"ko\")\n",
    "\n",
    "# Revise the following lines to change the plot format\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"$\\\\delta$ (deg)\")\n",
    "plt.title(\"Minimum deviation data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Define the model function\n",
    "The Python `def` statement below defines our model function,\n",
    "\n",
    "$$\\delta = \\theta_i + \\sin^{-1}[(n^2 - \\sin^2\\theta_i)^{1/2}\\sin\\alpha - \\sin\\theta_i\\cos\\alpha] - \\alpha.$$\n",
    "\n",
    "We include both `n` and `alpha_deg` as parameters, so that we can adjust the apex angle if necessary. The trigonometric functions in NumPy assume radian units, not degrees, so we must use the `deg2rad` function from NumPy to convert `theta_deg` and `alpha_deg` into radians for computation, then use `rad2deg` to convert back to degrees at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# theta_deg: Incident angle in degrees\n",
    "# n        : Refractive index\n",
    "# alpha_deg: Apex angle in degrees\n",
    "\n",
    "\n",
    "def model(theta_deg, n, alpha_deg):\n",
    "    # Convert from degrees to radians and treat it as a complex variable\n",
    "    theta = np.astype(np.deg2rad(theta_deg), complex)\n",
    "    alpha = np.deg2rad(alpha_deg)\n",
    "    delta = (\n",
    "        theta + np.arcsin(np.sin(alpha) * np.sqrt(n**2 - np.sin(theta) ** 2) - np.sin(theta) * np.cos(alpha)) - alpha\n",
    "    )\n",
    "    # Warn the user if the argument of arcsin exceeds one\n",
    "    if any(np.sin(alpha) * np.sqrt(n**2 - np.sin(theta) ** 2) - np.sin(theta) * np.cos(alpha) > 1):\n",
    "        warnings.warn(\"Argument to arcsin exceeds one, returning the complex norm\")\n",
    "    #  Take the absolute value and convert to back to degrees\n",
    "    delta_deg = np.rad2deg(np.absolute(delta))\n",
    "    return delta_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that we have introduced a trick to handle cases for which a light ray incident at `theta_deg` will be totally internally reflected by the prism. Mathematically, these values of `theta_deg` will cause the argument of the `arcsin` function to exceed one. As the [arcsin documentation](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.arcsin.html?highlight=arcsin) indicates, the `arcsin` function will return `nan` (ie, not a number) when this occurs for real-valued input. If the `curve_fit` function tries to evaluate the model function for a value of `n` that produces this output, it will generate an error and stop. For *complex-valued input,* however, `arcsin` will return a well-defined *complex-valued output,* such that\n",
    "\n",
    "$$z = \\arcsin u\\rightarrow u = \\sin z = \\frac{e^{iz} - e^{-iz}}{2i}.$$\n",
    "\n",
    "To convert the argument of `arcsin` from real-valued to complex-valued, we add `0j` to the `theta` variable (Python uses the engineering convention `1j` to represent the complex number $i$), as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.arcsin(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.arcsin(2 + 0j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This change will allow the fit routine to continue if the argument to the `arcsin` function exceeds one, but since the result in that case is physically impossible, we issue a warning so that the user knows there is a problem. The result for `delta` in this case will have a nonzero imaginary part, which we handle by taking the absolute value; if the argument to the `arcsin` function is less than one, then `delta` will be a real, positive number and the absolute value operation leaves it unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Choose initial parameter values for the model\n",
    "Select your initial parameters, select an appropriate range for your independent variable, and check that the resulting model curve is reasonably close to your data. Adust and repeat if they don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set initial parameter n and constant alpha\n",
    "nInit = 1.5\n",
    "alpha_deg_const = 60\n",
    "\n",
    "# Define new frequency array, fModel, for displaying the model\n",
    "thetaModel = np.linspace(45, 80, 100)\n",
    "\n",
    "# Make the plot\n",
    "plt.plot(thetaModel, model(thetaModel, nInit, alpha_deg_const), \"r-\")\n",
    "plt.errorbar(theta_deg, delta_deg, yerr=err_deg, fmt=\"ko\")\n",
    "\n",
    "# Label axes and give it a title for notebook (remove it when including the plot in a report)\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"$\\\\delta$ (deg)\")\n",
    "plt.title(\"Data with model, initial parameters\")\n",
    "\n",
    "plt.show()\n",
    "# Now go back to the top and change nInit to improve the fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 6: Fit the model to the data\n",
    "See the [help](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) for details on the `curve_fit` function. Its basic syntax is\n",
    "\n",
    "`curve_fit(f, xdata, ydata)`,\n",
    "\n",
    "where `f` is the model function, `xdata` the measurements of the independent variable, and `ydata` the measurements of the dependent variable. Moreover, the first input to `f` must be the independent variable, and `curve_fit` interprets all remaining inputs to `f` as fit parameters. If we were to use our current definition of `model`, for example, the first input `theta_deg` would correctly be understood by `curve_fit` to be the independent variable, and both `n` and `alpha_deg` would be understood to be fit parameters.\n",
    "\n",
    "While it's nice to have control over `alpha_deg` in our model, it is specified by the manufacturer to be 60 degrees, so it is preferable to fix `alpha_deg` at `alpha_deg_const` and just fit for `n`. To do this without writing an entirely new `def` expression, we use the [lambda expression](https://docs.python.org/3/reference/expressions.html#lambda) `lambda theta_deg, n: model(theta_deg,n,alpha_deg_const)` to define a new, *anonymous function* (ie, it does not have a name) as the first input to `curve_fit`, which in turn has input parameters `theta_deg, n` and returns `model(theta_deg,n,alpha_deg_const)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fix alpha_deg to alpha_deg_const, fit the resulting model to the data,\n",
    "# and display the results. Each input to curve_fit is shown on a separate\n",
    "# line with explanatory comments.\n",
    "nOpt, nVar = curve_fit(\n",
    "    lambda theta_deg, n: model(theta_deg, n, alpha_deg_const),  # Model with alpha_deg fixed\\\n",
    "    theta_deg,  # Independent variable\n",
    "    delta_deg,  # Dependent variable\n",
    "    p0=nInit,  # Initial parameter guess\n",
    "    sigma=err_deg,  # Uncertainty in dependent variable\n",
    "    absolute_sigma=True,\n",
    ")  # Assume uncertainties in delta_deg are known independently\n",
    "print(\"nOpt =\", nOpt)\n",
    "print(\"nVar =\", nVar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Convert output from arrays to scalars, determine standard error, and display formatted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nOpt = nOpt[0]\n",
    "nAlpha = np.sqrt(nVar[0, 0])\n",
    "print(\"Default precision:\")\n",
    "print(\"Refractive index estimate:    \", nOpt, \" ± \", nAlpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_**Note that you should always round such parameter estimates and uncertainties to the appropriate number of significant figures!**_\n",
    "\n",
    "Follow the \"golden rules\" described in Sec. 2.9 of Hughes and Hase, adapted to the model-fitting context.\n",
    "\n",
    "1. *If and only if the fit between a model and the data is good* (see below for methods to assess fit quality), the best estimate of a parameter is the one returned by the fit, and may be understood as the mean value of the distribution that you would get if you were to conduct the same experiment many times.\n",
    "2. The error in each parameter is given by the square root of the associated diagonal element of the covariance matrix, and may be understood as the standard error for the parameter. Correlation coefficients between parameters may also be derived from the covariance matrix, using Eq. (7.30) in Hughes and Hase.\n",
    "3. Round up the error for each parameter to the appropriate number of significant figures (usually just one, but sometimes more—see the discussion on p. 17 of Hughes and Hase, including Footnote #7).\n",
    "4. Match the number of decimal places in the mean to the standard eror.\n",
    "5. Include units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can print a result that follows these rules using the syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Refractive index estimate:    {nOpt:.5f} ± {nAlpha:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `f` that precedes the expression in quotation marks indicates that this is a formatted string literal, or [f-string](https://docs.python.org/3/reference/lexical_analysis.html#f-strings). F-strings have the format `f\"Text {variable:format}\"`, where the curly brackets `{}` include instructions for printing the value of a variable with a specified format. For example, the specifier `.5f` will represent the argument in floating-point notation to five decimal places. Examples of the most common numerical format strings are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Field width=10, precision=5, floating point format: {nOpt:.5f} ± {nAlpha:.5f}\")\n",
    "print(f\"Field width=10, precision=5, scientific notation format: {nOpt:.5e} ± {nAlpha:.5e}\")\n",
    "print(f\"Field width=10, precision=5, the more compact of e or f format: {nOpt:.5g} ± {nAlpha:.5g}\")\n",
    "print(f\"Field width=10, signed integer format: {nOpt.astype(int):d} ± {nAlpha.astype(int):d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 7: Assess the fit\n",
    "### Assessing the fit: visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot data with best-fit model\n",
    "plt.plot(thetaModel, model(thetaModel, nOpt, alpha_deg_const), \"r-\")\n",
    "plt.errorbar(theta_deg, delta_deg, yerr=err_deg, fmt=\"ko\")\n",
    "\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"$\\\\delta$ (deg)\")\n",
    "plt.title(\"Data with model, optimal parameters\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing the fit: compute the $\\chi^2$ statistic\n",
    "Change the definition of `res` to match your dependent variable and model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and display chi-squared\n",
    "res = delta_deg - model(theta_deg, nOpt, alpha_deg_const)\n",
    "normres = res / err_deg\n",
    "chisq = np.sum(normres**2)\n",
    "print(f\"chi-squared = {chisq:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compare to the number of statistical degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and display DOF\n",
    "Ndata = np.size(delta_deg)\n",
    "Npar = np.size(nOpt)\n",
    "dof = Ndata - Npar\n",
    "print(f\"dof = {dof:d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This gives a *reduced chi-squared* of $\\chi_\\nu^2=\\chi^2_\\text{min}/\\nu \\approx 2$. As discussed in Sec. 8.4 of Hughes and Hase, consistency requires $\\chi_\\nu^2 \\approx 1$, but this does not provide very clear guidance in this case: is $2\\approx 1$, or is $2\\gg 1$?  A more quantitative approach is to use the *cumulative distribution function* of the $\\chi^2$ distribution, `chi2.cdf(chisq,dof)`, to determine the fraction of times that a fit with `dof` degrees of freedom will yield a value for $\\chi_\\text{min}^2$ less than or equal to `chisq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cdf = chi2.cdf(chisq, dof)\n",
    "print(f\"Cumulative probability = {cdf:.3f}\")\n",
    "print(f\"Significance: {1 - cdf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tells us that we should expect to get $\\chi^2 = 14.1$ or less 95.1&nbsp;% of the time when the data is well-described by the model, and greater than this only 4.9&nbsp;% of the time. We call this 4.9&nbsp;% probability the *significance* of the $\\chi^2$ test for $\\chi^2_\\text{min} = 14.1$ and $\\nu = 7$. The significance level may be understood as the probability of rejecting a fit erroneously, as a result of an unusually large statistical fluctuation in the data. A common convention is to reject any fit that falls below 5&nbsp;% significance. By this standard our fit is marginal: it's not an obviously bad fit, but neither is it particularly good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assessing the fit: plot the (normalized) residuals\n",
    "Finally, we can check for statistical consistency by confirming that the residuals do not show any systematic pattern.\n",
    "\n",
    "The following cell uses the PyPlot `stem` routine to produce the residual plot, which often shows deviations from the zero line more effectively than a regular scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Show residuals\n",
    "plt.stem(theta_deg, res)\n",
    "\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"Residual (deg)\")\n",
    "plt.title(\"Fit residuals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plotting the normalized residuals makes it easier to evaluate the fit. In this case we have assumed the point uncertainty is constant for each measurement, so the main simplification is to rescale the ordinate so that the estimated uncertainty is equal to one. See [Example-Fit.ipynb](https://gitlab.rcg.sfu.ca/teaching/data-analysis-python/Template-Fit.ipynb) for an example in which the point uncertainty varies across the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Show normalized residuals\n",
    "plt.stem(theta_deg, normres)\n",
    "\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "plt.title(\"Normalized fit residuals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be helpful to stack the data plot and the residual plot for comparison. The simplest way to do this is with the [`subplots`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html) and [`subplot`](https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=False, squeeze=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(thetaModel, model(thetaModel, nOpt, alpha_deg_const), \"r-\")\n",
    "plt.errorbar(theta_deg, delta_deg, yerr=err_deg, fmt=\"ko\")\n",
    "\n",
    "plt.ylabel(\"$\\\\delta$ (deg)\")\n",
    "plt.tick_params(  # Use this to disable tick labels for x-axis\n",
    "    axis=\"x\",  # changes apply to the x-axis\n",
    "    which=\"both\",  # both major and minor ticks are affected\n",
    "    bottom=True,  # ticks along the bottom edge are on\n",
    "    top=False,  # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    ")  # labels along the bottom edge are off\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.stem(theta_deg, normres)\n",
    "\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can customize the layout with [`GridSpec`](https://matplotlib.org/users/gridspec.html) by using the `subplot2grid` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True, sharey=False, squeeze=True)\n",
    "plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "plt.plot(thetaModel, model(thetaModel, nOpt, alpha_deg_const), \"r-\")\n",
    "plt.errorbar(theta_deg, delta_deg, yerr=err_deg, fmt=\"ko\")\n",
    "\n",
    "plt.ylabel(\"$\\\\delta$ (deg)\")\n",
    "plt.tick_params(  # Use this to disable tick labels for x-axis\n",
    "    axis=\"x\",  # changes apply to the x-axis\n",
    "    which=\"both\",  # both major and minor ticks are affected\n",
    "    bottom=True,  # ticks along the bottom edge are on\n",
    "    top=False,  # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    ")  # labels along the bottom edge are off\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.stem(theta_deg, normres)\n",
    "\n",
    "plt.xlabel(\"$\\\\theta_i$ (deg)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you are happy with the layout, you can save the figure using the [`savefig`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.savefig.html?highlight=savefig#matplotlib.pyplot.savefig) method. Note that we apply this method to the figure handle `fig` that was created by the `subplots` function in the preceding cell. We could also use `plt.savefig`, but only *before* executing `plt.show()`, since that function resets the current figure handle to a blank figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"mindevfit.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 8: Decide what to do next\n",
    "Ideally you will get to the end of this whole process and find that the model fits the data well, and you can report your results with confidence. If time allows, you might even go back and take more data to improve the precision of your parameter estimates. But what happens if your $\\chi^2$ statistic is too big or too small, or if the normalized residuals do not appear random? If that's the case, you have a few options:\n",
    "\n",
    "* Look for defects in your experimental procedure, including the procedure you used to prepare the raw data for analysis (ie, incorrect unit conversion).\n",
    "* Consider a different model. This may include changes that are based on physical reasoning, such as eliminating approximations to make it more realistic, or purely empirical, such as adding a constant background, or an extra term in a polynomial fit. The residuals should provide guidance on how to change the functional relationship to improve agreement. Just don't overdo it: if you find yourself needing to add many arbitrary fit parameters to achieve a good fit, that is usually a sign that you need to consider a different approach.\n",
    "* Consider restricting your fit to a narrower range of measurements (i.e., focus on limiting values of $x$ and/or $y$, or on the heights, widths and locations of peaks instead of their detailed shape).\n",
    "* Consider rescaling the uncertainties, as described in Sec. 8.9 of Hughes and Hase (though note their \"health warning\" in Footnote 9).\n",
    "* Accept that you may be unable to remove all sources of systematic error in the given time, and do your best to make a quantitative estimate of your parameter uncertainties in light of them. You don't have to throw all of your data away just because you don't have a model that fits it well."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### About this notebook\n",
    "© J. Steven Dodge, 2019. The notebook text is licensed under CC BY 4.0. See more at [Creative Commons](https://creativecommons.org/licenses/by/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
