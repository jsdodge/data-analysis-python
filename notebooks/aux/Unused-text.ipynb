{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes\n",
    "The option `edgecolor='k'` defines the edge of each bar with a black line; the default behavior is to omit this. Delete this option and run the cell again to see how the appearance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard normal distribution\n",
    "NumPy offers random number generators that produce numbers with many other distributions, but the most important of these is [`randn`](https://www.numpy.org/doc/1.16/reference/generated/numpy.random.randn.html), which produces samples from the *standard normal distribution*,\n",
    "$$\\mathcal{N}(x;\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right],$$\n",
    "in which $\\mu = 0$ and $\\sigma^2=1$. The cell below uses `randn` to create an array of 1000 normally distributed random numbers and plot them as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44be6afa9793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "random.seed(0)\n",
    "x = random.randn(N)\n",
    "\n",
    "plt.hist(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Occurrence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python basics\n",
    "\n",
    "Review the following documentation pages to familiarize yourself with the basic elements of the Python language, integrated development environment (IDE) and Python libraries for data analysis.\n",
    "\n",
    "* [Python Lectures](https://sfu.syzygy.ca/jupyter/hub/user-redirect/git-pull?repo=https://github.com/nleehone/PythonLectures.git&branch=master)\n",
    "\n",
    "Additional resources: \n",
    "* [Getting Started With Jupyter Notebook for Python](https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46)\n",
    "\n",
    "For MATLAB users:\n",
    "\n",
    "* [NumPy for Matlab users](https://www.numpy.org/devdocs/user/numpy-for-matlab-users.html)\n",
    "\n",
    "Python online documentation: \n",
    "* [Python 3.7.4 documentation](https://docs.python.org/3/tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sections in the worksheet are numbered to be consistent with the text. Please read the corresponding section of the book before beginning the associated section of the worksheet and then do the questions, using the book as a reference as you work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The *standard error*, denoted by $\\alpha$ in *Measurements and their uncertainties,* provides an estimate of the standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation and consistency\n",
    "For consistency, we should expect that as the number of measurements $N$ increases, $\\lim_{N\\rightarrow\\infty}\\bar{x}_N = \\mu$ (including the subscript $N$ to make its role explicit) and $\\lim_{N\\rightarrow\\infty}\\sigma_{N-1} = \\sigma$, since the sample distribution should converge to the parent distribution in this limit.\n",
    "\n",
    "Starting with $\\bar{x}_N$, we wish to evaluate\n",
    "\n",
    "$$ \\lim_{N\\rightarrow\\infty}\\bar{x}_N = \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_{i=1}^{N}x_i. $$\n",
    "\n",
    "To accomplish this, we note that in the $N\\rightarrow\\infty$ limit we expect $x_i$ to take on *all possible values* for $x$, with a frequency given by the probability density $P_\\text{DF}(x)$. This allows us to replace the sum over random $x_i$ (in this limit) with an integral over the domain of $P_\\text{DF}(x)$ that we can readily evaluate.\n",
    "\n",
    "\\begin{align}\n",
    "\\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_{i=1}^{N}x_i\n",
    "&= \\int_{-\\infty}^{\\infty}\\text{d}x\\,x P_\\text{DF}(x)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}\\text{d}x\\,x\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}\\text{d}u\\,u\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right) + \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}\\text{d}x\\,\\mu\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\quad(\\text{with}\\ u = x - \\mu)\\\\\n",
    "&= \\frac{\\mu}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}\\text{d}x\\,\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\\\\n",
    "&= \\mu.\n",
    "\\end{align}\n",
    "\n",
    "As expected, we find that the sample mean converges to the mean of the parent distribution as the number of samples increases.\n",
    "\n",
    "We call the integral $\\int_{-\\infty}^{\\infty}\\text{d}x\\,x P_\\text{DF}(x)$ the *expectation* of $x$ and denote it by $\\text{E}[X]$, where again the upper-case $X$ refers to the set of all values that $x$ can have.\n",
    "\n",
    "Next we check that $\\lim_{N\\rightarrow\\infty}\\sigma^2_{N-1} = \\sigma^2$, where we focus on the *variance* $\\sigma^2$ instead of $\\sigma$ to avoid the square root in the [definition](#std-pop) of $\\sigma_{N-1}$. Converting the finite sum into an expectation, we have\n",
    "\n",
    "$$\\lim_{N\\rightarrow\\infty}\\sigma^2_{N-1} = \\lim_{N\\rightarrow\\infty}\\sigma^2_{N} = \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\bar{x}_n)^2 = \\text{E}[(X - \\bar{X}_N)^2] = \\text{E}[(X - \\mu)^2],$$\n",
    "\n",
    "where $\\bar{X}_N$ denotes the set of all averages $\\bar{x}_N$ of $N$ numbers, and $\\text{E}[\\bar{X}] = \\mu$. Evaluating the final expectation in the above expression yields the desired result,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}[(X - \\mu)^2] &= \\int_{-\\infty}^{\\infty}\\text{d}x\\,(x-\\mu)^2 P_\\text{DF}(x)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty}\\text{d}x\\,(x-\\mu)^2 \\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}\\text{d}u\\,u^2 e^{-u^2/2}\\quad[\\text{with}\\ u = (x - \\mu)/\\sigma]\\\\\n",
    "&= \\frac{\\sigma^2}{\\sqrt{2\\pi}}\\left\\{\\left[u(-e^{-u^2/2}\\right|_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty}\\text{d}u\\,e^{-u^2/2}\\right\\}\\quad(\\text{integrating by parts})\\\\\n",
    "&= \\sigma^2.\n",
    "\\end{align}\n",
    "\n",
    "This equivalence is of course no accidentâ€”in general, we *define* the variance as $\\text{Var}(X) = \\text{E}[(X-\\mu)^2]$ for an arbitrary probability distribution, so we also associate the variance with the parameter $\\sigma^2$ in the normal distribution.\n",
    "\n",
    "These expectation values justify our use of $\\bar{x}_N$ as an estimate for $\\mu$ and $\\sigma_{N-1}$ for $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how much variation we *expect* to see in $\\bar{x}$, we can calculate the expectation $\\text{Var}(\\bar{X}_N) = \\text{E}[(\\bar{X}_N-\\mu)^2]$ (now for fixed, finite $N$) just as we calculated expectations for $X$ and $(X-\\mu)^2$. We'll do this in steps, demonstrating some useful properties of the variance along the way. \n",
    "\n",
    "First, we derive a common alternative expression for the variance,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(X) &= \\text{E}[(X-\\mu)^2] \\\\\n",
    "&= \\text{E}[X^2 - 2\\mu X + \\mu^2] \\\\\n",
    "&= \\text{E}[X^2] - 2\\mu\\,\\text{E}[X] + \\mu^2 \\\\\n",
    "&= \\text{E}[X^2] - \\mu^2 \\\\\n",
    "&= \\text{E}[X^2] - \\mu^2.\n",
    "\\end{align}\n",
    "\n",
    "The difference between $\\text{E}[X^2]$ and $\\mu^2 = (\\text{E}[X])^2$ arises because the contributions to $\\text{E}[X^2]$ from $x>\\mu$ pull the expectation up much more than the contributions from $x<\\mu$ pull it down.\n",
    "\n",
    "Next, we derive the important result that the variance of a sum $x_1 + x_2$ is equal to the sum of their variances, $\\text{Var}(X_1) + \\text{Var}(X_2)$, so long as $x_1$ and $x_2$ are uncorrelated.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(X_1 + X_2) &= \\text{E}\\{[(X_1 + X_2) - (\\mu_1 + \\mu_2)]^2\\}\\\\\n",
    "&=\\text{E}[(X_1 + X_2)^2] - (\\mu_1 + \\mu_2)^2\\\\\n",
    "&=\\text{E}[X_1^2 + 2X_1X_2 + X_2^2] - \\mu_1^2 -2\\mu_1\\mu_2 - \\mu_2^2\\\\\n",
    "&=\\text{E}[X_1^2] - \\mu_1^2 + 2\\text{E}[X_1X_2] - 2\\mu_1\\mu_2 + \\text{E}[X_2^2] - \\mu_2^2\\\\\n",
    "&=\\text{E}[X_1^2] - \\mu_1^2 + 2\\mu_1\\mu_2 - 2\\mu_1\\mu_2 + \\text{E}[X_2^2] - \\mu_2^2\\\\\n",
    "&=\\text{E}[X_1^2] - \\mu_1^2 + \\text{E}[X_2^2] - \\mu_2^2\\\\\n",
    "&=\\text{Var}[X_1] + \\text{Var}[X_2].\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(\\bar{X}_N) &= \\text{E}\\left\\{\\left[\\left(\\frac{1}{N}\\sum_{i=1}^NX_i\\right) - \\mu\\right]^2\\right\\}\\\\\n",
    "&= \\text{E}\\left\\{\\frac{1}{N^2}\\left[\\left(\\sum_{i=1}^N X_i\\right) - N\\mu\\right]^2\\right\\}\\\\\n",
    "&= \\frac{1}{N^2}\\text{E}\\left\\{\\left[\\left(\\sum_{i=1}^N X_i\\right) - N\\mu\\right]^2\\right\\}\\\\\n",
    "&= \\frac{1}{N^2}\\text{E}\\left\\{\\left[\\sum_{i=1}^N \\left(X_i - \\mu\\right)\\right]^2\\right\\}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3010299956639812"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log10(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
