{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "Python activities to complement [*Measurements and their Uncertainties*](https://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 2, \"Random errors in measurements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Basic statistics with NumPy](#Basic-statistics-with-NumPy)\n",
    "    * [Location and width](#Location-and-width)\n",
    "        * [Programming notes 1](#Programming-notes-1)\n",
    "        * [Programming notes 2](#Programming-notes-2)\n",
    "    * [Exercise 1](#Exercise-1)\n",
    "* [Parent distributions](#Parent-distributions)\n",
    "    * [Goals of statistics](#Goals-of-statistics)\n",
    "* [The parent distribution for `random`](#The-parent-distribution-for-random)\n",
    "    * [Exercise 2](#Exercise-2)\n",
    "    * [Continuous distributions and the probability density function](#Continuous-distributions-and-the-probability-density-function)\n",
    "    * [The standard uniform distribution](#The-standard-uniform-distribution)\n",
    "    * [Exercise 3](#Exercise-3)\n",
    "    * [The general uniform distribution](#The-general-uniform-distribution)\n",
    "    * [Exercise 4](#Exercise-4)\n",
    "    * [Exercise 5](#Exercise-5)\n",
    "* [The normal distribution](#The-normal-distribution)\n",
    "    * [The standard normal distribution](#The-standard-normal-distribution)\n",
    "        * [Programming notes 3](#Programming-notes-3)\n",
    "        * [Programming notes 4](#Programming-notes-4)\n",
    "    * [Aside on the Gaussian integral](#Aside-on-the-Gaussian-integral)\n",
    "    * [Sampling the standard normal distribution with `normal`](#Sampling-the-standard-normal-distribution-with-normal)\n",
    "    * [The general normal distribution](#The-general-normal-distribution)\n",
    "        * [Programming notes 5](#Programming-notes-5)\n",
    "    * [Exercise 6](#Exercise-6)\n",
    "* [Back to the mean and standard deviation](#Back-to-the-mean-and-standard-deviation)\n",
    "    * [Estimating the parent distribution from a sample](#Estimating-the-parent-distribution-from-a-sample)\n",
    "    * [Consistency of the estimates for the mean and standard deviation](#Consistency-of-the-estimates-for-the-mean-and-standard-deviation)\n",
    "    * [Exercise 7](#Exercise-7)\n",
    "    * [The standard error](#The-standard-error)\n",
    "        * [Programming notes 6](#Programming-notes-6)\n",
    "    * [The error in the error](#The-error-in-the-error)\n",
    "    * [A word on words](#A-word-on-words)\n",
    "* [Reporting results](#Reporting-results)\n",
    "    * [How to interpret the mean ± the standard error](#How-to-interpret-the-mean-±-the-standard-error)\n",
    "        * [Programming notes 7](#Programming-notes-7)\n",
    "    * [Determining the mean and variance from the distribution](#Determining-the-mean-and-variance-from-the-distribution)\n",
    "    * [Formatting numbers in Python](#Formatting-numbers-in-Python)\n",
    "    * [Exercise 8](#Exercise-8)\n",
    "* [Summary](#Summary)\n",
    "* [Further reading](#Further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [previous notebook](1.0-Random-numbers.ipynb) and read *MU* Ch. 2, \"Random errors in measurements,\" with the following [goals](A.0-Reading-goals.ipynb#Random-error-in-measurements) in mind.\n",
    "\n",
    "1. Be able to use the mean and standard deviation to characterize a statistical sample. Specifically, be able to:\n",
    "    1. Estimate the mean and standard deviation from a histogram;\n",
    "    2. Sketch a distribution with a given mean and standard deviation;\n",
    "    3. Calculate the mean for a set of measurements;\n",
    "    4. Calculate the standard deviation for a set of measurements.\n",
    "2. Know the functional form for a Gaussian distribution, be able to sketch it for a given mean and standard deviation, and be able to estimate the mean and standard deviation from the plot of a Gaussian distribution.\n",
    "3. Be able to discuss the relationships between a sample and its parent distribution.\n",
    "4. Be able to discuss the difference between the standard deviation and the standard deviation in the mean (aka the standard error), and be able to estimate both from a histogram of measurements.\n",
    "5. Be able to report uncertainties correctly for a given measurement.\n",
    "\n",
    "You already know how to import libraries and tell the Jupyter notebook server how to display plots from Matplotlib, so we will get this out of the way now in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Location and width\n",
    "NumPy includes several basic [statistics routines](https://numpy.org/devdocs/reference/routines.statistics.html) for characterizing a sample from a statistical distribution. The following code cell applies several of them to the results in *MU* Table 2.1. The most common ones, the mean and the standard deviation, provide estimates of the *location* and *width*, respectively, of the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 1\n",
    "In both the [`std`](https://numpy.org/devdocs/reference/generated/numpy.std.html) and the [`var`](https://numpy.org/devdocs/reference/generated/numpy.var.html) functions it is important to set the option `ddof=1` for consistency with *MU* Eq. (2.3). The documentation for both functions describes this option as follows.\n",
    "> “Delta Degrees of Freedom”: the divisor used in the calculation is `N - ddof`, where `N` represents the number of elements. By default `ddof` is zero.\n",
    "\n",
    "What this means is that the default usage of `std` with `ddof=0` returns what is known as the  *population standard  deviation*,\n",
    "\n",
    "<a id=\"std-pop\"></a>$$\\sigma_N = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2}\\quad\\text{(Default, ddof=0)},$$\n",
    "\n",
    "and with `ddof=1` it returns the expression given in Eq. (2.3), also known as the *sample standard deviation*,\n",
    "\n",
    "<a id=\"std-samp\"></a>$$\\sigma_{N-1} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N(x_i - \\bar{x})^2}\\quad\\text{(Preferred, ddof=1)}.$$\n",
    "\n",
    "The options `ddof=0` and `ddof=1` are also called *uncorrected* and *corrected*, respectively, for reasons that we will discuss shortly.\n",
    "\n",
    "Both options are shown below, and you will see that the difference is not that large. Since we will normally only need to know the standard deviation to one significant figure anyway, the error we get by forgetting to set `ddof=1` is usually unimportant. The two options are quite different conceptually, however, so you should get in the habit of using `ddof=1` in this context.\n",
    "\n",
    "And just to be sure that `mean` and `std` are doing what we think they are doing, we use the NumPy [`sum`](https://numpy.org/devdocs/reference/generated/numpy.sum.html) function and the built-in Python [`len`](https://docs.python.org/3/library/functions.html#len) function to evaluate them explicitly. These expressions are long, so we break them and continue (with indentation) on the next line—see the [Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008) for more on this and other conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = [10.0, 9.4, 9.8, 9.6, 10.5, 9.8, 10.3, 10.2, 10.4, 9.3]\n",
    "\n",
    "print(\"Mean (s): \", np.mean(period))\n",
    "print(\"Mean (s, explicit): \", np.sum(period)/len(period))\n",
    "print(\"Median (s): \", np.median(period))\n",
    "print()\n",
    "\n",
    "print(\"Standard deviation (s, uncorrected): \", np.std(period))\n",
    "print(\"Standard deviation (s, uncorrected, explicit): \", \n",
    "      np.sqrt(np.sum((period - np.mean(period))**2)/len(period)))\n",
    "print(\"Variance (s^2, uncorrected): \", np.var(period))\n",
    "print()\n",
    "\n",
    "print(\"Standard deviation (s, corrected): \", np.std(period, ddof=1))\n",
    "print(\"Standard deviation (s, corrected, explicit): \", \n",
    "      np.sqrt(np.sum((period - np.mean(period))**2)/(len(period) - 1)))\n",
    "print(\"Variance (s^2, corrected): \", np.var(period, ddof=1))\n",
    "print()\n",
    "\n",
    "print(\"Minimum (s): \", np.amin(period))\n",
    "print(\"Maximum (s): \", np.amax(period))\n",
    "print(\"Range (s, peak-to-peak): \", np.ptp(period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 2\n",
    "The last result in the above cell looks worrisome: shouldn't we get 10.5 – 9.3 = 1.2? But it turns out that this is [just what we expect](https://docs.python.org/3/tutorial/floatingpoint.html), once we recognize that the computer uses a  binary representation of floating-point numbers that has finite precision. To see this, we calculate 10.5 – 9.3 – 1.2 in a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10.5 - 9.3 - 1.2)\n",
    "print((10.5 - 9.3) - 1.2)\n",
    "print(10.5 - (9.3 + 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most computers have 53 bits of floating-point precision, so the best we can expect for any number is a relative precision of 2<sup>-53</sup>. In this calculation,  10.5 has an exact binary representation, but 9.3 and 1.2 are both infinitely repeating fractions that must be truncated. These errors cancel in the last of the three calculations, but not in the first two (which are equivalent, given Python's order-of-operations rules). The cell below shows that the magnitude of overall truncation error is exactly $6\\times 2^{-53}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2**(-53))\n",
    "print(10.5 - 9.3 - 1.2 + 6*2**(-53))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few experiments ever approach this level of precision, of course. The only reason this came up in our simple  calculation is because numbers are expressed by default with excessive precision—later we will see how to format numerical expressions to the correct precision in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "*MU* Prob. (2.1) gives a set of volume measurements, all in milliliters: 25.8, 26.2,  26.0, 26.5, 25.8, 26.1, 25.8, and 26.3. Use NumPy to calculate the mean, the standard deviation $\\sigma_{N-1}$, and the total range of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent distributions\n",
    "\n",
    "*MU* Sec. 2.6 introduces the concepts of a *sample* and a *parent distribution*, fundamental concepts in statistics that allows us to connect *what we actually measure* (the sample) to a *mathematical model for the measurement* (the parent distribution). A canonical example that illustrates this difference is a coin flip. Before we flip the coin, we *expect* it to come up *Heads* or *Tails* with equal probability: our parent distribution is then\n",
    "\n",
    "$$ P(x) = \\begin{cases}\n",
    "  \\frac{1}{2} & \\text{for}\\ x =\\ \\text{Heads,} \\\\[1ex]\n",
    "    \\frac{1}{2} & \\text{for}\\ x =\\ \\text{Tails,}\n",
    "  \\end{cases} $$\n",
    "  \n",
    "where $P(x)$ denotes the probability of obtaining a result $x$. When we actually flip a coin, however, the result is always *either* *Heads* or *Tails*, and it no longer makes sense to describe this probabilistically. The result is a *sample* that has been *drawn* from the parent distribution.\n",
    "\n",
    "### Goals of statistics\n",
    "\n",
    "Now that we have defined these two concepts, we can identify some of the most important goals of statistics:\n",
    "1. Determine the properties of a parent distribution from knowledge of a sample drawn from it,\n",
    "2. Predict the properties of a sample from knowledge of the parent distribution, and\n",
    "3. Determine the probability that a given sample was drawn from a given parent distribution.\n",
    "\n",
    "In the context of experimental physics, this translates into\n",
    "1. Develop a mathematical model to describe a set of experimental measurements,\n",
    "2. Predict the results of an experiment using a mathematical model, and\n",
    "3. Assess the agreement between a mathematical model and an experiment.\n",
    "\n",
    "It is not an exaggeration to say that most of science can be reduced to these three activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parent distribution for `random`\n",
    "Let's start with the first item: determining the parent distribution of `random` from the samples that it generates. This is a bit artificial since we know that `random` was *designed* to produce random numbers that are  uniformly distributed over \\[0.0, 1.0), but it serves as an example of how we would proceed if the underlying parent distribution were unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Run the code cell below to produce a histogram of `n = 10` samples from `random`. Next, duplicate the entire cell, increase `n` by a factor of 10, and run it again. Repeat this procedure until you are satisfied that the sample histogram accurately reflects the underlying parent distribution. (To duplicate the cell you may use the Jupyter menu selections *`Edit > Copy Cells`* and *`Edit > Paste Cells Below`*, or you can just select the *`+`* item from the menu bar to create a new cell, then copy and paste the contents from the one above it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 2\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "n = 10\n",
    "rg = default_rng()\n",
    "x_rand = rg.random(n)\n",
    "\n",
    "plt.hist(x_rand, bins=20, range=(0.0, 1.0), ec='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Occurrence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Continuous distributions and the probability density function\n",
    "These histograms reveal an important difference between `random` and our earlier example of a coin flip. With a coin flip we have only two possible outcomes, *Heads* or *Tails*, so when we specify the parent distribution we can just assign a discrete probability to each one. And while this is also true of the numbers returned by `random`, since the number of floating-point numbers between 0.0 and 1.0 is large but [ultimately finite](https://lemire.me/blog/2017/02/28/how-many-floating-point-numbers-are-in-the-interval-01/), it is more convenient to consider them as elements of the continuous, *infinite* set $\\{x|x\\in\\mathbb{R}, 0\\le x \\le 1\\}$, where $\\mathbb{R}$ represents the set of real numbers. In this case, the probability of drawing any *specific* number between 0 and 1 is infinitesimal, so the approach that we used with the coin flip is unsuitable. Instead of assigning probabilities to specific *values* of $x$, then, we assign probabilities to specific *intervals*. We define the *probability density function* (also known as the probability distribution function, or PDF), $P_\\text{DF}(x)$, through the integral relationship\n",
    "\n",
    "$$P(x_1 \\le x \\le x_2) = \\int_{x_1}^{x_2}\\text{d}x\\,P_\\text{DF}(x),$$\n",
    "\n",
    "for all $x_1$ and $x_2$ (see also *MU* Sec. 3.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard uniform distribution\n",
    "We can now describe the parent distribution for `random` as\n",
    "\n",
    "$$P_\\text{DF}(x) =\\begin{cases}\n",
    "  1 & \\text{for}\\ 0 \\le x \\le 1, \\\\[1ex]\n",
    "  0 & \\text{otherwise,}\n",
    "  \\end{cases}$$\n",
    "  \n",
    "which is known as the *standard uniform distribution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Use the definition of $P_\\text{DF}(x)$ for the standard uniform distribution to determine the following probabilities and record them in the following Markdown cell. Do these integrals analytically—later, we will compare them with numerical results obtained with the [`scipy.integrate`](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html) package.\n",
    "1. $P(0 \\le x \\le 0.5)$\n",
    "2. $P(0.2 \\le x \\le 0.25)$\n",
    "3. $P(0 \\le x \\le 1)$\n",
    "4. $P(-1 \\le x \\le 0)$\n",
    "5. $P(0.75 \\le x \\le 2)$\n",
    "6. $P(-\\infty \\le x \\le \\infty)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markdown cell for Exercise 3**\n",
    "\n",
    "Select this cell and enter your response here.\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. \n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general uniform distribution\n",
    "The *general uniform distribution* has a PDF that is constant over the interval $[a,b]$ and zero elsewhere:\n",
    "\n",
    "$$\\mathcal{U}(x;a,b) = P_\\text{DF}(x;a,b) =\\begin{cases}\n",
    "  \\frac{1}{b - a} & \\text{for}\\ a \\le x \\le b, \\\\[8pt]\n",
    "  0 & \\text{otherwise,}\n",
    "  \\end{cases}$$\n",
    "  \n",
    "In this notation, the standard uniform distribution may be written as $\\mathcal{U}(x;0,1)$, or $X\\sim\\mathcal{U}(0,1)$, where the upper-case $X$ refers to the population of all possible $x$ (the real numbers in this case) and the tilde notation means \"$X$ is distributed as $\\mathcal{U}(0,1)$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Using the following Markdown cell, show that $\\int_{-\\infty}^\\infty \\text{d}x\\, \\mathcal{U}(x;a,b) = 1$ for arbitrary $a<b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markdown cell for Exercise 4**\n",
    "\n",
    "Select this cell and enter your response here.\n",
    "\n",
    "To help you get started, the expression you have been asked to prove is written below using $\\LaTeX$ [expressions](https://www.overleaf.com/learn/latex/Mathematical_expressions).\n",
    "\n",
    "$$\\int_{-\\infty}^\\infty \\text{d}x\\, \\mathcal{U}(x;a,b) = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "In the following code cell, initialize the RNG with zero as the seed and construct a histogram of 1000 samples from a general uniform distribution over the interval $[0.9, 1.1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 5\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard normal distribution\n",
    "The NumPy `Generator` object provides methods to produce samples with many other distributions, but the most important of these is the [`normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html) method with the default arguments `loc=0.0` and `scale=1.0`. This produces samples from the *standard normal distribution*,\n",
    "\n",
    "$$P_\\text{DF}(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right).$$\n",
    "\n",
    "This functional form is also known as a \"Gaussian,\" after the mathematician [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss), who identified its importance in probability theory (see [here](https://en.wikipedia.org/wiki/Normal_distribution#History) for a brief history). It is also known informally as \"the  bell curve\" because of its shape, shown in the following code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 3\n",
    "Note that we use the NumPy constant [`pi`](https://docs.scipy.org/doc/numpy/reference/constants.html) for the numerical value of $\\pi$. Other NumPy constants include `inf`, for $\\infty$; `nan`  for \"not a number\", which refers to expressions such as `0.0/0.0` that are undefined; `ninf` for $-\\infty$; `nzero` for $0^-$; `pzero` for $0^+$; `e` for $e$; and `euler_gamma` for $\\gamma$, the [Euler-Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant). A large number of physical constants are available through the [`scipy.constants`](https://docs.scipy.org/doc/scipy/reference/constants.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-4.0, 4.05, 0.1)\n",
    "std_norm_pdf = np.exp(-x**2/2)/np.sqrt(2*np.pi)\n",
    "\n",
    "plt.plot(x, std_norm_pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('PDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 4\n",
    "The normal distribution is part of [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) statistical sub-package in SciPy, which contains [dozens of statistical distributions](https://docs.scipy.org/doc/scipy/reference/stats.html). Each distribution is defined as a Python object that includes a standard set of functions (or *methods*, in object-oriented programming jargon), including `pdf` for the PDF. Functions for the normal distribution are part of the [`norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) object, and the function that computes its PDF is [`norm.pdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), following the standard object-oriented `<Object>.<Method>` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x = np.arange(-4.0, 4.05, 0.1)\n",
    "std_norm_pdf_alt = norm.pdf(x)\n",
    "\n",
    "plt.plot(x, std_norm_pdf_alt)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('PDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on the Gaussian integral\n",
    "You might wonder about the factor of $\\sqrt{2\\pi}$ in the definition of the normal distribution—it is there to ensure that the probability that $-\\infty < x < \\infty$ is equal to one [see also *MU* Sec. 3.1, especially Eq. (3.1)]:\n",
    "\n",
    "$$ \\int_{-\\infty}^{\\infty}\\text{d}x\\, P_\\text{DF}(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}\\text{d}x\\,\\exp\\left(-\\frac{x^2}{2}\\right) = 1.$$\n",
    "\n",
    "See the supplementary notebook [2.1](2.1-Evaluating-the-Gaussian-integral.ipynb) for details about how to evalute this integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the standard normal distribution with `normal`\n",
    "The cell below uses `normal` to create an array of 1000 normally distributed random numbers and plots them as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "rg = default_rng()\n",
    "x_normal = rg.normal(size=n)\n",
    "\n",
    "plt.hist(x_normal, bins=20, range=(-4.0, 4.0), ec='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Occurrence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general normal distribution\n",
    "As with the uniform distribution, we can define a *general normal distribution*,\n",
    "\n",
    "<a id=\"eq-normal-pdf\"></a>$$\\mathcal{N}(x;\\mu, \\sigma^2) = P_\\text{DF}(x;\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right],$$\n",
    "\n",
    "which has mean $\\mu$ and variance $\\sigma^2$, where $\\sigma$ is the standard deviation. In this notation the standard normal distribution is written as $\\mathcal{N}(x;0,1)$, or $X\\sim\\mathcal{N}(0,1)$, and again, the upper-case $X$ refers to the population of all $x$, the real numbers. The general normal distribution is related to the standard normal distribution by the substitution\n",
    "\n",
    "$$ x\\rightarrow \\frac{x-\\mu}{\\sigma},$$\n",
    "\n",
    "which shows that $\\mu$ controls the *location* of the distribution and $\\sigma$ controls its *width*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 5\n",
    "You can use the  `loc` and `scale` keywords to control $\\mu$ and $\\sigma$, respectively, in the [`norm.pdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) function, as shown in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10\n",
    "sigma = 0.5\n",
    "\n",
    "dx = sigma/100\n",
    "xmin =  mu - 4*sigma\n",
    "xmax =  mu + 4*sigma\n",
    "x = np.arange(xmin, xmax + dx, dx)\n",
    "gen_norm_pdf = norm.pdf(x, loc=mu, scale=sigma)\n",
    "\n",
    "plt.plot(x, gen_norm_pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('PDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "In the following code cell, initialize the RNG with zero and construct a histogram of 1000 samples from a general normal distribution with $\\mu = 10, \\sigma = 0.5$. Use 20 bins and choose a range that positions the distribution well in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 6\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the mean and standard deviation\n",
    "Earlier we used `mean` and `std` to characterize the [location and width](#Location-and-width) of a *sample*. Now we are in a position to return to these calculations and relate them to the mean and standard deviation of the *parent distribution.* We reproduce the earlier calculations with `mean` and `std` in the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = [10.0, 9.4, 9.8, 9.6, 10.5, 9.8, 10.3, 10.2, 10.4, 9.3]\n",
    "print(\"Mean (s): \", np.mean(period))\n",
    "print(\"Standard deviation (s, corrected): \", np.std(period, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the parent distribution from a sample\n",
    "The standard interpretation of these summary statistics is that the measurements are *normally distributed*, and $\\bar{x} = 9.93~\\text{s}$ and $\\sigma_{N-1} = 0.42~\\text{s}$ are our *estimates* of the mean and standard deviation, respectively. (We are ignoring, for the moment, the uncertainty in each of these quantities, which we will get to at the end of the notebook. For now, we just express them both to two decimal places.) We can represent this graphically by plotting a histogram of the measurements and laying the estimated parent distribution on top of it. Note that the probability density has units that are the inverse of the argument to ensure that the probability integral $\\int_{x_1}^{x_2}\\text{d}x\\,P_\\text{DF}(x)$ is dimensionless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the bins to be 0.1 wide\n",
    "bin_width = 0.1\n",
    "edges = np.arange(np.amin(period), np.amax(period) + bin_width, bin_width)\n",
    "\n",
    "# Plot the histogram as a probability density\n",
    "plt.hist(period, bins = edges, density=True, ec='k')\n",
    "\n",
    "# Set the mean and standard deviation\n",
    "mu = 9.93\n",
    "sigma = 0.42\n",
    "\n",
    "# Calculate the distribution at a dt = 0.01 spacing\n",
    "dt = 0.01\n",
    "tmin = mu - 4*sigma\n",
    "tmax = mu + 4*sigma\n",
    "t_dist = np.arange(tmin, tmax + dt, dt)\n",
    "pdf = norm.pdf(t_dist, loc=mu, scale=sigma)\n",
    "\n",
    "# Plot the density and label axes\n",
    "plt.plot(t_dist, pdf)\n",
    "plt.xlabel('Period (s)')\n",
    "plt.ylabel('Probability density (1/s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency of the estimates for the mean and standard deviation\n",
    "As the number of measurements $N$ increases, we expect $\\lim_{N\\rightarrow\\infty}\\bar{x}_N = \\mu$ (including the subscript $N$ to make its role explicit) and $\\lim_{N\\rightarrow\\infty}\\sigma_{N-1} = \\sigma$, since the sample distribution should converge to the parent distribution in this limit. Complete [Exercise 7](#Exercise-7) below to check this numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Evaluate the code cell below to show the standard normal PDF superposed on a histogram of `n_lim = 10` samples from `normal` with  `n_bin` bins, together with the mean and standard deviation of the sample. Increase `n_lim` by a factor of 10 and reevaluate the cell. Repeat this process until the sample mean and sample standard deviation are within 0.01 of their expected values for a few different choices of the seed. Once you have selected `n_lim`, adjust `n_bin` to allow a good comparison between the sample distribution and the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code cell for Exercise 7\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "n_lim = 10\n",
    "n_bin = 10\n",
    "\n",
    "rg = default_rng(0)\n",
    "x_n = rg.normal(size=n_lim)\n",
    "\n",
    "dx = 0.01\n",
    "x_n_dist = np.arange(-4.0, 4.0 + dx, dx)\n",
    "x_n_pdf = norm.pdf(x_n_dist)\n",
    "\n",
    "plt.hist(x_n, bins=n_bin, density=True, ec='k')\n",
    "plt.plot(x_n_dist, x_n_pdf)\n",
    "\n",
    "print('N: ', n_lim)\n",
    "print('Mean: ', np.mean(x_n))\n",
    "print('Standard deviation: ', np.std(x_n, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the mean and variance from the distribution\n",
    "So far, we have referred to the mean $\\mu$, the standard deviation $\\sigma$, and the variance $\\sigma^2$ as *parameters* that we use to [define the normal distribution](#eq-normal-pdf), but we have referred to the mean $\\bar{x}$, the standard deviation $\\sigma_{N-1}$, and the variance $\\sigma_{N-1}^2$ as *operations* that we perform on a sample from some parent distribution. Now we will close the loop, and define generalizations of these operations to determine the mean $\\mu$, the standard deviation $\\sigma$, and the variance $\\sigma^2$ of arbitrary parent distributions. This will provide us with tools to predict the properties of a sample drawn from a given distribution (see Item 2 in [Goals of statistics](#Goals-of-statistics)). Some of these concepts will be introduced and developed more fully in *MU* Ch. 3 and its associated notebook, [3.0 Basic probability](3.0-Basic-probability.ipynb). Consider this a preview, and refer to those materials if you would like to learn more.\n",
    "\n",
    "Starting with the mean, $\\mu$, we define it as the *expectation* of $x$, $\\langle x\\rangle$, for a given $P_\\text{DF}(x)$, where\n",
    "\n",
    "$$ \\mu = \\langle x\\rangle = \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_{i=1}^{N}x_i\n",
    "= \\int_{-\\infty}^{\\infty}\\text{d}x\\,x P_\\text{DF}(x). $$\n",
    "\n",
    "Note that in the $N\\rightarrow\\infty$ limit we expect $x_i$ to take on *all possible values* for $x$, with a frequency given by the probability density $P_\\text{DF}(x)$, which allows us to replace the sum over random $x_i$ with an integral over the domain of $P_\\text{DF}(x)$.\n",
    "\n",
    "Similarly, we define the *variance*, $\\sigma^2$, of $P_\\text{DF}(x)$ as\n",
    "\n",
    "$$ \\sigma^2 = \\lim_{N\\rightarrow\\infty}\\sigma^2_{N-1} = \\lim_{N\\rightarrow\\infty}\\sigma^2_{N} = \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\bar{x}_N)^2 = \\langle(x - \\mu)^2\\rangle. $$\n",
    "\n",
    "For consistency, the normal distribution is actually *defined* so that the *parameters* $\\mu$ and $\\sigma^2$ are equal to the *expectations* of the distribution $\\langle x\\rangle$ and $\\langle(x - \\mu)^2\\rangle$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard error\n",
    "\n",
    "Now let us consider the variance of a sum $S$ over a set of uncorrelated random numbers  $\\{x_1, x_2, \\ldots, x_N\\}$,\n",
    "\n",
    "$$ S = \\sum_{i=1}^N x_i. $$\n",
    "\n",
    "An important [property](https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)) of the variance is that the variance of $S$ *is equal to the sum of the individual variances,*\n",
    "\n",
    "$$ \\left\\langle\\left[S - \\left\\langle S\\right\\rangle\\right]^2\\right\\rangle = \\sum_{i=1}^N\\left\\langle (x_i-\\mu)^2\\right\\rangle.$$\n",
    "\n",
    "This allows us to determine the *variance of the sample mean*, $\\bar{x}_N$, assuming each sample in the mean is drawn from a single distribution with known $\\mu$ and $\\sigma$:\n",
    "\n",
    "\\begin{align}\n",
    "\\langle(\\bar{x}_N - \\mu)^2\\rangle &= \\left\\langle\\left[\\frac{S}{N} - \\mu\\right]^2\\right\\rangle\\\\\n",
    "&= \\frac{1}{N^2}\\left\\langle\\left(S - N\\mu\\right)^2\\right\\rangle\\\\\n",
    "&= \\frac{1}{N^2}\\sum_{i=1}^N\\langle(x_i-\\mu)^2\\rangle\\\\\n",
    "&= \\frac{\\sigma^2}{N}.\n",
    "\\end{align}\n",
    "\n",
    "The *standard deviation of the mean,* or *standard error,* is then\n",
    "\n",
    "$$ \\sqrt{\\langle(\\bar{x}_N - \\mu)^2\\rangle} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "Typically, the true value of $\\sigma$ is unknown, so the standard error is estimated from measurements [see *MU* Eq. (2.7)],\n",
    "\n",
    "$$\\alpha = \\frac{\\sigma_{N-1}}{\\sqrt{N}}.$$\n",
    "\n",
    "This result justifies the practice of averaging measurements to improve the uncertainty. Note, however, that the uncertainty decreases inversely with $\\sqrt{N}$, so to reduce the uncertainy by a factor of two it is necessary to perform *four times* as many measurements.\n",
    "\n",
    "Let's illustrate this by simulating a new set of measurements based on *MU* Table 2.1, assuming $\\mu = 9.93$ and $\\sigma = 0.42$ are the true values for the parent distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mean and standard deviation\n",
    "mu = 9.93\n",
    "sigma = 0.42\n",
    "\n",
    "# Seed the RNG and simulate N_meas = 10 measurements\n",
    "N_meas = 10\n",
    "rg = default_rng(0)\n",
    "period_sim = mu + sigma*rg.normal(size=N_meas)\n",
    "\n",
    "# Return the mean and standard deviation\n",
    "print(\"Mean (s): \", np.mean(period_sim))\n",
    "print(\"Standard deviation (s, corrected): \", np.std(period_sim, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With  `rg = default_rng(0)`, we get $\\bar{x} \\approx 9.97~\\text{s}$ instead of 9.93 s, and $\\sigma_{N-1} \\approx 0.33~\\text{s}$ instead of 0.42 s. Change the seed and rerun the cell to see how much these two quantities vary.\n",
    "\n",
    "In the next code cell we simulate 1000 repetitions, each time taking the average $\\bar{x}_{N=10}$ of 10 samples. The resulting histogram shows the distribution for $\\bar{x}_{N=10}$, which we plot with the expected PDF $\\mathcal{N}(x;\\mu, \\sigma^2/N)$, where the variance has been reduced by $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate $\\bar{x}$ and $\\sigma_{N-1}$ from the data, so while the true (but unknown) values of $\\mu$ and $\\sigma$ in $\\mathcal{N}(\\mu,\\sigma^2)$ are understood to be *fixed*, our estimates of them will be *variable*, since they will depend on a particular set of measurements. This variability represents a fundamental source of uncertainty in both estimates, as discussed in *MU* Sec. 2.7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 6\n",
    "One way to run the simulation would be to put the original simulation in a `for` loop with 1000 iterations and store each result. We take a different approach here that does not require a loop. We use `normal` to generate a `10 x 1000` *array* of random numbers, all normally distributed with `mu = 9.93` and `sigma = 0.42`, then take the mean along the first dimension by setting the option `axis=0` (in a multidimensional array `axis=1` corresponds to the second dimension, etc.). This gives us a one-dimensional array with 1000 elements, `period_mean`, each of which represents one set of 10 measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mean and standard deviation to the\n",
    "# values estimated from Table 2.1\n",
    "mu = 9.93\n",
    "sigma = 0.42\n",
    "\n",
    "# Seed the RNG and simulate N_rep = 1000 repetitions\n",
    "# of N_meas = 10 measurements\n",
    "N_meas = 10\n",
    "N_rep = 1000\n",
    "rg = default_rng(0)\n",
    "period_sim_array = mu + sigma*rg.normal(size=(N_meas, N_rep))\n",
    "\n",
    "# Compute the mean along the first dimension\n",
    "period_mean = np.mean(period_sim_array, axis=0)\n",
    "\n",
    "# Compute the sample standard deviation of the mean\n",
    "std_mean = np.std(period_mean, ddof=1)\n",
    "\n",
    "# Compute the (true) standard error\n",
    "alpha = sigma/np.sqrt(N_meas)\n",
    "\n",
    "# Set the bins to be 0.05 wide\n",
    "bin_width = 0.05\n",
    "edges = np.arange(np.amin(period_mean), \n",
    "                  np.amax(period_mean) + bin_width, bin_width)\n",
    "\n",
    "# Make histogram\n",
    "plt.hist(period_mean, bins=edges, density=True, ec='k')\n",
    "plt.xlabel('Period (s)')\n",
    "plt.ylabel('Probability density (1/s)')\n",
    "\n",
    "# Calculate the PDF N(xbar,alpha^2) at a dt = 0.01 spacing\n",
    "# and superpose on the histogram\n",
    "dt = 0.01\n",
    "tbar_min = mu - 4*sigma/np.sqrt(N_meas)\n",
    "tbar_max = mu + 4*sigma/np.sqrt(N_meas)\n",
    "t_bar_dist = np.arange(tbar_min, tbar_max + dt, dt)\n",
    "pdf_tbar = norm.pdf(t_bar_dist, loc=mu, scale=alpha)\n",
    "\n",
    "plt.plot(t_bar_dist, pdf_tbar)\n",
    "\n",
    "# Print the standard deviation of the N_rep = 1000 simulated measurements of\n",
    "# the mean and the standard error computed from the original measurement of\n",
    "# N_meas = 10 periods\n",
    "print('Standard deviation of the mean: ', std_mean)\n",
    "print('Standard error of original measurement: ', sigma/np.sqrt(N_meas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The error in the error\n",
    "As discussed in *MU* Sec. 2.7.1, this uncertainty has its own uncertainty, which we express here as $\\sigma(\\alpha)$ [see *MU* Eq. (2.8)]:\n",
    "\n",
    "$$ \\sigma(\\alpha) = \\frac{\\alpha}{\\sqrt{2N-2}}$$\n",
    "\n",
    "The next code cell evaluates $\\alpha$ for each of the 1000 repetitions of our simulated experiment and compares the distribution to $\\mathcal{N}[\\alpha,\\alpha^2/(2N-2)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mean and standard deviation to the\n",
    "# values estimated from Table 2.1\n",
    "mu = 9.93\n",
    "sigma = 0.42\n",
    "\n",
    "# Seed the RNG and simulate N_rep = 1000 repetitions\n",
    "# of N_meas = 10 measurements\n",
    "N_meas = 10\n",
    "N_rep = 1000\n",
    "rg = default_rng(0)\n",
    "period_sim_array = mu + sigma*rg.normal(size=(N_meas, N_rep))\n",
    "\n",
    "# Estimate the standard error along the first dimension\n",
    "period_stderr = np.std(period_sim_array, axis=0, ddof=1)/np.sqrt(N_meas)\n",
    "\n",
    "# Compute the sample standard deviation of the standard error\n",
    "std_stderr = np.std(period_stderr, ddof=1)\n",
    "\n",
    "# Compute the (true) standard error\n",
    "alpha = sigma/np.sqrt(N_meas)\n",
    "\n",
    "# Compute the error in the error from (2.8)\n",
    "sigma_alpha = alpha/np.sqrt(2*N_meas-2)\n",
    "\n",
    "# Set the bins to be 0.01 wide\n",
    "bin_width = 0.01\n",
    "edges = np.arange(np.amin(period_stderr), np.amax(period_stderr) + bin_width, bin_width)\n",
    "\n",
    "# Make histogram\n",
    "plt.hist(period_stderr, bins=edges, density=True, ec='k')\n",
    "plt.xlabel('Standard error (s)')\n",
    "plt.ylabel('Probability density (1/s)')\n",
    "\n",
    "# Calculate the PDF N(alpha,sigma_alpha^2) at a dt = 0.001 spacing\n",
    "# and superpose on the histogram\n",
    "dt = 0.001\n",
    "sigma_alpha_min = alpha - 4*sigma_alpha\n",
    "sigma_alpha_max = alpha + 4*sigma_alpha\n",
    "sigma_alpha_dist = np.arange(sigma_alpha_min, sigma_alpha_max + dt, dt)\n",
    "pdf_sigma_alpha = norm.pdf(sigma_alpha_dist, loc=alpha, scale=sigma_alpha)\n",
    "\n",
    "plt.plot(sigma_alpha_dist, pdf_sigma_alpha)\n",
    "\n",
    "# Print the standard deviation of the N_rep = 1000 simulated measurements of\n",
    "# the standard error, and the uncertainty computed with MU Eq. (2.8) with data \n",
    "# from the original measurement of N_meas = 10 periods\n",
    "print('Standard deviation of the standard error (simulation): ', std_stderr)\n",
    "print('Standard deviation of the standard error (estimated from original measurement): ', sigma_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that this histogram is a little asymmetrical. This is because the *true* distribution for $\\alpha$ in this case is not really Gaussian, just approximately so. This approximation gets better as the number of measurements in the sample increases—and it gets worse when it decreases. You can reexecute the cell with `N_meas = 2` to see the worst-case scenario (the corrected standard deviation is infinite for `Nmeas = 1`). This is worth keeping in mind when you report the mean and standard error of a set of measurements. If you are going to rely on the standard error in other statistical computations, such as a curve fit, it is best to have a sample of at least five measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on words\n",
    "By now you have seen that the world of statistics is a bit of a linguistic minefield. We have the standard deviation of the *population* $\\sigma$; the corrected and  uncorrected standard deviations of the *sample* $\\sigma_{N-1}$ and $\\sigma_{N}$, respectively, sometimes distinguished from $\\sigma$ by using the notation $s$; and the standard deviation of the *mean*, $\\alpha$. We have the mean of the sample $\\bar{x}$ and the population mean $\\mu$. And the *true* standard deviation of the mean, $\\sigma/\\sqrt{N}$,  when $\\mu$ and $\\sigma$ are *known,* and the *estimated* standard deviation of the mean, $\\sigma_{N-1}/\\sqrt{N}$, when they are *unknown*. Browse through the internet and you will find many lost souls who are struggling to understand these seemingly mystical distinctions, along with equally many overconfident pedants who prescribe their personal usage preferences as gospel (see also: split infinitives).\n",
    "\n",
    "The root cause of most of this confusion is related to one of the basic activities of statistics, known as *statistical inference,*  which is to make *general* statements about the world from a *particular* set of observations of it. To do this, we must compare the quantities that we actually measure (the sample) with our idealized expectations of those quantities (over the population), and it can be easy to confuse them because we like to think that they ultimately refer to same thing (reality). We are also prone to use the same words to refer to them, just as I might refer to my language in this sentence as an example of the English language. In statistical usage, the words *mean,* *standard deviation,* and *variance* may be applied to *any* set of numerical quantities, real or imagined, so their meaning will vary with the context.\n",
    "\n",
    "Statisticians are keenly aware of the opportunities for confusion here, and they have developed a variety of notational devices to help communicate clearly. A particularly helpful notation in this context is to use the caret (\"^\", also called a \"hat\") to distinguish a fixed, typically unknown quantity $p$ from its *statistical estimate* $\\hat{p}$, which is known but will vary with the data that contribute to the estimate. In this notation, the population mean is $\\mu$ and the sample mean is $\\hat{\\mu} = \\bar{x}$. We are also free to construct other functions to estimate $\\mu$—not just the arithmetic average $\\bar{x}$—and refer to it as $\\hat{\\mu}$; examples include the mode, the median, the geometric mean, and the trimmed mean, which can all be useful in the right circumstances. Similarly, $\\hat{\\sigma}$ is an estimate of the standard deviation of the population $\\sigma$, which we would typically specify as either $\\sigma_{N-1}$ or $\\sigma_N$. We also have $\\alpha = \\sigma/\\sqrt{N}$ and $\\hat{\\alpha} = \\hat{\\sigma}/\\sqrt{N}$. We will use this notation occasionally in these notebooks to improve clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting results\n",
    "We are now in a position to summarize the results from Table 2.1 of *MU*, following the steps given in *MU* Sec. 2.8.\n",
    "\n",
    "1. Compute the mean, keeping all significant figures: `xbar = 9.93` (s).\n",
    "2. Compute the standard error, keeping all significant figures: `alpha = 0.13281566172707193` (s).\n",
    "3. Determine the appropriate number of significant figures to report: here, we have `sigma_alpha = 0.03130495168499706`, so we report both `alpha` and `xbar` to two decimal places (*MU* recommends only one decimal place, which is also reasonable in this instance).\n",
    "4. Round the mean to the appropriate decimal place and report the result with an uncertainty of one standard error: $T = (9.93\\pm 0.13)~\\text{s}$ [or, following *MU*, $T = (9.9\\pm 0.1)~\\text{s}$]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret the mean ± the standard error\n",
    "The following code cell plots the mean of 100 of our simulated experiments with vertical error bars that extend over $\\bar{x}\\pm\\hat{\\alpha}$, and the population mean $\\mu$ is shown with a red horizontal line. Several things are worth noticing.\n",
    "\n",
    "1. Many (25 of the first 100) of the sample means lie more than one standard error from the true mean.\n",
    "2. The standard errors fluctuate.\n",
    "3. Whether a sample mean lies within one standard error of the true mean depends on *both* the distance of the sample mean to the true mean *and* the size of the standard error.\n",
    "4. If you set `N_meas = 3` and repeat the cell computation, you should see a greater number of sample means that lie more than one standard error from the true mean, and that the standard error fluctuations are larger.  Set `N_meas = 2` and both effects become even more pronounced.\n",
    "\n",
    "These observations provide some guidance toward correctly interpreting $\\bar{x}\\pm\\hat{\\alpha}$. Most importantly, it is a statement about *a particular set of measurements, not the quantity that is being measured.* The relationship between the two involves assumptions that can change as you continue the measurement process—for example, you might discover a systematic error in the measurement that causes you to change how you define $\\bar{x}$ in the first place. More typically, $\\bar{x}$ and $\\hat{\\alpha}$ will fluctuate as you acquire new data, so our interpretation of them should reflect this variability.\n",
    "\n",
    "> The expression $\\bar{x}\\pm\\hat{\\alpha}$ is an *estimate* of the mean and width of the distribution that we *would expect to observe* if we were to perform repeated identical measurements. This distribution is usually understood to be $P_\\text{DF}(\\bar{x}) = \\mathcal{N}(\\bar{x};\\mu,\\alpha^2)$, where $\\mu$ and $\\alpha$ are the true mean and standard error, respectively.\n",
    "\n",
    "We expect the range $[\\bar{x}-\\hat{\\alpha},\\bar{x}+\\hat{\\alpha}]$ to include the true value most of the time, assuming that the measurements are limited by statistical uncertainty with normally-distributed errors. If $N\\gtrsim  6$, the range will include the true value in about 2/3 of the measurements, as expected for a normal distribution with a standard deviation given by the standard error. For $N<6$, however, the fluctuations in $\\sigma_{N-1}$ become large enough to introduce significant deviations from these expectations. (The true distribution is not quite normal, but a Student *t*-distribution, which is discussed in *MU* Sec. 8.8.)\n",
    "\n",
    "The simulation also reveals the flaws in some common misconceptions about the standard error.\n",
    "\n",
    "1. The probability that the true value either lies within a *given* $\\bar{x}\\pm\\hat{\\alpha}$ range is either 0% or 100%, and your measurement doesn't tell you which it is. It is *incorrect* to say that $\\bar{x}\\pm\\hat{\\alpha}$ provides a range that has a 68% (or other intermediate) probability of containing the true value.\n",
    "2. A measurement that lies more than one or even two standard errors from the expected value is *not necessarily wrong,* as it may arise from a statistical fluctuation.\n",
    "3. The estimated standard error fluctuates, especially if $N$ is small. A change in $\\hat{\\alpha}$ of a factor of two is not necessarily an indication that something changed in the experiment.\n",
    "4. Although the mean ± the standard error is usually understood to represent a normal distribution, it really only approaches one when $N\\gtrsim 6$. For $N<6$, significant deviations from normality become important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 7\n",
    "We use the [`errorbar`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.errorbar.html) function to make this plot. The first two arguments, `np.arange(n_show), period_mean[:n_show]`, correspond to the *x* and *y* values of each point, where we use `arange` to assign integer indices `[0, 1, ..., n_show - 1]` to the *x* values and `period_mean[:n_show]` to assign the first `n_show` values of `period_mean` to the *y* values. The values for the standard error are assigned to the `yerr` keyword to create symmetric error bars of length `period_stderr[:n_show]`. We assign `.` to the `fmt` keyword to represent each mean value as a point, with no connecting line.\n",
    "\n",
    "The last three lines place a horizontal line across the plot to show the true mean. The [`xlim`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.xlim.html) function in pyplot returns *two* arguments, `xmin` and `xmax`, when it is called with no input arguments. We use these to overlay a red line at the true mean with `plot`, which in turn rescales the *x*-axis, which you will see if you comment out the last line. We use `xlim` again to set the limits to their original values, this time with `xmin` and `xmax` as input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mean and standard deviation to the\n",
    "# values estimated from Table 2.1\n",
    "mu = 9.93\n",
    "sigma = 0.42\n",
    "\n",
    "# Seed the RNG and simulate N_rep = 1000 repetitions\n",
    "# of N_meas = 10 measurements\n",
    "N_meas = 10\n",
    "N_rep = 1000\n",
    "rg = default_rng(0)\n",
    "period_sim_array = mu + sigma*rg.normal(size=(N_meas, N_rep))\n",
    "\n",
    "# Compute the mean along the first dimension\n",
    "period_mean = np.mean(period_sim_array, axis=0)\n",
    "\n",
    "# Compute the standard error along the first dimension\n",
    "period_stderr = np.std(period_sim_array, axis=0, ddof=1)/np.sqrt(N_meas)\n",
    "\n",
    "# Make the histogram\n",
    "n_show = 100\n",
    "plt.errorbar(np.arange(n_show), period_mean[:n_show], yerr=period_stderr[:n_show], fmt='.')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Mean and uncertainty')\n",
    "\n",
    "# Plot a horizontal red line to indicate the true mean\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot([xmin, xmax], [mu, mu], 'r-')\n",
    "plt.xlim(xmin, xmax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting numbers in Python\n",
    "Below are the \"golden rules\" described in *MU* Sec. 2.9.\n",
    "\n",
    "1. The best estimate of a parameter is the mean.\n",
    "2. The error is the standard error in the mean.\n",
    "3. Round up the error for each parameter to the appropriate number of significant figures.\n",
    "4. Match the number of decimal places in the mean to the standard error.\n",
    "5. Include units.\n",
    "\n",
    "The cell below uses Python [format specifiers](https://docs.python.org/3/library/string.html#formatspec) to display the variables with the correct precision. The last line uses a [formatted string literal](https://docs.python.org/3/reference/lexical_analysis.html#f-strings), more commonly known as an *f-string*, which are designed for this purpose. F-strings have the format `f\"Text {variable:format}\"`, where the curly brackets `{}` include instructions for printing the value of a variable with a specified format. For example, `{period_mean:.2f}` indicates that `period_mean` should be displayed as a floating point number to two digits of precision (`.2f`). Similarly, `{period_stderr:.2f}` indicates that `period_stderr` should also be displayed as a floating point number to two digits of precision (`.2f`). See [here](https://docs.python.org/3/tutorial/inputoutput.html#tut-f-strings) for the official Python tutorial on f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display formatted results at correct level of significance\n",
    "period = [10.0, 9.4, 9.8, 9.6, 10.5, 9.8, 10.3, 10.2, 10.4, 9.3]\n",
    "period_mean = np.mean(period)\n",
    "period_stderr = np.std(period, ddof=1)/np.sqrt(len(period))\n",
    "\n",
    "print(\"Without formatting\")\n",
    "print(\"Period (s): \", period_mean, \" ± \", period_stderr)\n",
    "print()\n",
    "\n",
    "print(\"Formatted:\")\n",
    "print(f\"Period (s): {period_mean:.2f} ± {period_stderr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "**Adapted from *MU* Prob. (2.2)**\n",
    "\n",
    "Twelve measurements of the sensitivity of a photodiode circuit (in amps/watt) are: 5.33, 4.95, 4.93, 5.08, 4.95, 4.96, 5.02, 4.99, 5.24, 5.25, 5.23, and 5.01. Use the code cell below to summarize the mean and uncertainty of these measurements, using a Python format string to print it to the correct precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code cell for Exercise 8\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is a list of what you should be able to do after completing this notebook.\n",
    "* Compute basic statistics on a set of numbers using NumPy, including `mean`, `median`, `std`, `var`, `amin`, `amax`, and `ptp`.\n",
    "* Recognize the significance of the `ddof` option in `std` and `var` and use it appropriately.\n",
    "* Use `sum` and `len` from NumPy and the Python standard library, respectively, to compute the standard deviation and variance of a set of numbers without using `std` or `var`.\n",
    "* Describe the limitations of floating-point calculations and be able to recognize them when you encounter them.\n",
    "* Distinguish between a parent distribution and a sample from it.\n",
    "* Recognize the three basic goals of statistics described above.\n",
    "* Recognize the distinction between probability and probability density.\n",
    "* Compute the probability that a random variable $x$ will lie in an interval by referring to its probability density.\n",
    "* Express the parent distributions for `random` and `normal` in mathematical notation.\n",
    "* Use `random` and `normal` to generate samples from general uniform and general normal distributions, respectively, and use them to simulate experiments.\n",
    "* Recognize and implement NumPy constants such as `pi`.\n",
    "* Import and use the `norm` distribution object from SciPy to compute the normal PDF for given values of $x$, $\\mu$, and $\\sigma$.\n",
    "* Use a sample repeated measurements to estimate the mean and standard deviation of its parent distribution, assuming they are normally distributed.\n",
    "* Compute the standard error and the error in the error for a set of measurements.\n",
    "* Recognize the distinction between a parameter $p$ that describes a parent distribution and an estimate of it, $\\hat{p}$.\n",
    "* Summarize a set of measurements in mean ± standard error format with the correct precision.\n",
    "* Make an `errorbar` plot.\n",
    "* Correctly interpret a summary of measurements expressed in mean ± standard error format.\n",
    "* Use f-strings with Python format specifiers to express numerical expressions to a desired precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "Wikipedia, [Variance](https://en.wikipedia.org/wiki/Variance). Discusses many of the properties of the variance, including proofs of important theorems and identities. As with all Wikipedia references, use caution. Wikipedia is a wonderful resource for many things, but the technical entries are often remixes of textbooks that leave out or misrepresent important context. If you want anything more than a general impression, find a book or lecture notes from a known, reliable source. If you need help with this, ask your instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "© J. Steven Dodge, 2019.The notebook text is licensed under CC BY 4.0. See more at [Creative Commons](https://creativecommons.org/licenses/by/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
