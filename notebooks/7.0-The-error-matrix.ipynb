{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The error matrix\n",
    "\n",
    "Python activities to complement [*Measurements and their Uncertainties*](https://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 7, \"Computer minimization and the error matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [The error surface](#The-error-surface)\n",
    "    * [Example of a linear fit](#Example-of-a-linear-fit)\n",
    "* [Curvature and covariance](#Curvature-and-covariance)\n",
    "    * [Interpreting confidence limits](#Interpreting-confidence-limits)\n",
    "    * [Exercise 1](#Exercise-1)\n",
    "* [The error matrix for a linear fit](#The-error-matrix-for-a-linear-fit)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [Example Fit notebook](A.1-Example-Fit.ipynb) and read *MU* Ch. 7, \"Computer minimization and the error matrix\", with the following [goals](A.0-Reading-goals.ipynb#Computer-minimization-and-the-error-matrix) in mind.\n",
    "\n",
    "1. Be able to explain qualitatively how data analysis computer programs fit a model to data by minimizing the *&chi;*<sup>2</sup> goodness-of-fit parameter as a function of the model parameters. Specifically,\n",
    "    * recognize that the terms *grid search*, *gradient-descent*, *Newton's method*, *Gauss-Newton*, and *Levenberg-Marquardt* refer to different algorithms for minimizing a function;\n",
    "    * be able to use matrix notation to expand *&chi;*<sup>2</sup> to second order around a particular point in space, as shown in (7.6); and\n",
    "    * be able to write the gradient vector, Hessian matrix, and Jacobian matrix for a function of multiple variables, and explain how they appear in the context of computer minimization routines.\n",
    "2. Be able to describe how the curvature matrix relates to the error surface near $\\chi^2_{\\min}$, and how it can be used to estimate the parameter values at which $\\chi^2=\\chi_{\\text{min}}^2+1$.\n",
    "3. Be able to describe the meaning and significance of the covariance matrix and the correlation matrix.\n",
    "4. Recognize that the covariance matrix can be estimated from a fit by inverting the curvature matrix at the minimum of the error surface.\n",
    "\n",
    "The following code cell includes the initialization commands needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The error surface\n",
    "\n",
    "We saw in [NB 6.0, Fitting complex functions](6.0-Fitting-complex-functions.ipynb#Linear-fits-with-non-uniform-errors) that to fit a line $y = mx + c$ to measurements $\\{x_k\\}, \\{y_k\\}$ with uncertainties $\\{\\alpha_k\\}$, $k = 1, 2, \\ldots, N$, we can minimize the function\n",
    "\n",
    "$$\n",
    "\\chi^2(m,c;\\{x_k\\},\\{y_k\\},\\{\\alpha_k\\}) = \\sum_{k=1}^N\\frac{[y_k - (mx_k +  c)]^2}{\\alpha_k^2} \\label{eq:lsq}\\tag{1}\n",
    "$$\n",
    "\n",
    "with respect to the parameters $m$ and $c$. We use a semicolon to emphasize the distinction between the *model parameters*, $m$ and $c$, and the *measurements*, $\\{x_k\\},\\{y_k\\}$, and $\\{\\alpha_k\\}$. Typically, we are interested only in the functional dependence of $\\chi^2$ on the model parameters, holding the measurement variables fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a linear fit\n",
    "The code cell below reproduces *MU* Fig.&nbsp;6.9, which shows the contours of this function for the measurements shown in *MU* Fig.&nbsp;6.1(d). The data are taken from *MU* Exercise (6.1), which is shown in *MU* Fig. 6.1(d) and discussed in *MU* Sec. 7.4.1, \"Worked example 1—a straight-line fit.\" The [Example Fit](A.1-Example-Fit.ipynb) notebook uses the same data.\n",
    "\n",
    "We also show the $1\\sigma$ and $2\\sigma$ uncertainty bounds as blue dotted lines, which *circumscribe* the elliptical contours corresponding to $\\chi_{1\\sigma}^2 = \\chi^2_\\text{min} + 1$ and $\\chi_{2\\sigma}^2 = \\chi^2_\\text{min} + 4$, respectively. As discussed in *MU* Sec. 6.5.2, the reason for this is that we define the uncertainty bounds on $\\hat{m}$ to be *independent* of the value of $\\hat{c}$; similarly, the uncertainty bounds on $\\hat{c}$ are understood to be independent of the value of $\\hat{m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into arrays\n",
    "frequency, voltage, err = np.genfromtxt(\"data/Example-Data.csv\", delimiter=\",\", skip_header=1, unpack=True)\n",
    "\n",
    "# Define a linear model\n",
    "\n",
    "\n",
    "def model(m, c):\n",
    "    return m * frequency + c\n",
    "\n",
    "\n",
    "# Define the chi-squared function for m and c, given the data\n",
    "\n",
    "\n",
    "def chi2fun(m, c):\n",
    "    normres = (voltage - model(m, c)) / err\n",
    "    return np.sum(normres**2)\n",
    "\n",
    "\n",
    "# Compute chi-squared over a 2D grid of equally-spaced values of m and c\n",
    "Nm = 50\n",
    "Nc = 50\n",
    "m = np.linspace(1.85, 2.19, num=Nm)\n",
    "c = np.linspace(-9, 9, num=Nc)\n",
    "\n",
    "chi2grid = np.zeros([Nc, Nm])\n",
    "for i in range(Nc):\n",
    "    for j in range(Nm):\n",
    "        chi2grid[i, j] = chi2fun(m[j], c[i])\n",
    "\n",
    "# Fit a linear model to the data and assign results to new variables\n",
    "pHat, pCov = np.polyfit(frequency, voltage, 1, w=1 / err, cov=\"unscaled\")\n",
    "\n",
    "mHat = pHat[0]\n",
    "cHat = pHat[1]\n",
    "mAlpha = np.sqrt(pCov[0, 0])\n",
    "cAlpha = np.sqrt(pCov[1, 1])\n",
    "\n",
    "# Display formatted results\n",
    "print(f\"Model slope (mV/Hz):     {mHat:.2f} ± {mAlpha:.2f}\")\n",
    "print(f\"Model intercept (mV):    {cHat:.0f} ± {cAlpha:.0f}\")\n",
    "print()\n",
    "\n",
    "# Evaluate the chi-squared function at the minimum and define contour levels\n",
    "#    level_low: standard 1D confidence levels (see caption of MU Fig. 6.9)\n",
    "#               (Note that we convert the list to a NumPy array to make it clear\n",
    "#               that the \"+\" operator represents arithmetic addition, not list\n",
    "#               concatenation.)\n",
    "#    level_high: additional contours to show behavior far from minimum\n",
    "chi2min = chi2fun(mHat, cHat)\n",
    "level_low = chi2min + np.array([1, 2.71, 4, 9])\n",
    "level_high = chi2min + np.arange(11, 121, 10)\n",
    "\n",
    "# Show minimum and contours around minimum\n",
    "plt.plot(mHat, cHat, \"h\")\n",
    "plt.contour(m, c, chi2grid, level_low, colors=\"k\", linewidths=1, linestyles=[\"solid\", \"dashed\", \"dotted\", \"dotted\"])\n",
    "plt.contour(m, c, chi2grid, level_high, linewidths=0.5)\n",
    "\n",
    "# Show 1-sigma and 2-sigma uncertainty bounds as dashed lines\n",
    "plt.hlines(cHat + cAlpha * np.array([-2, -1, 1, 2]), m[0], m[-1], linewidths=1, linestyles=\"dotted\")\n",
    "plt.vlines(mHat + mAlpha * np.array([-2, -1, 1, 2]), c[0], c[-1], linewidths=1, linestyles=\"dotted\")\n",
    "\n",
    "# Format plot to resemble MU Fig. 6.9\n",
    "plt.xticks(np.arange(1.9, 2.19, 0.05))\n",
    "plt.yticks(np.arange(-9, 10, 3))\n",
    "plt.xlabel(\"Gradient (mV/Hz)\")\n",
    "plt.ylabel(\"Intercept (mV)\")\n",
    "plt.tick_params(direction=\"in\", top=True, right=True)\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plt.gca().set_aspect(0.8 * (xmax - xmin) / (ymax - ymin))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature and covariance\n",
    "\n",
    "To see the dependence of $\\chi^2$ on $m$ and $c$ more clearly, we can rewrite Eq.&nbsp;([1](#The-error-surface)) in the form\n",
    "\n",
    "$$\n",
    "\\chi^2(m, c) = A_{mm}(m - \\hat{m})^2 + 2A_{mc}(m - \\hat{m})(c - \\hat{c}) + A_{cc}(c - \\hat{c})^2 + \\chi^2_\\text{min},\\label{eq:ellipse}\\tag{2}\n",
    "$$\n",
    "\n",
    "where $\\chi^2, A_{mm}, A_{mc}, A_{cc}, \\hat{m}, \\hat{c}$, and $\\chi^2_\\text{min}$ all depend implicitly on the measurement variables $\\{x_k\\}, \\{y_k\\}$, and $\\{\\alpha_k\\}$. This is the most general 2nd-order polynomial expression in $m$ and $c$ that we can write, so there is no need to derive Eq.&nbsp;(\\ref{eq:ellipse}) from Eq.&nbsp;(1); we can simply assert their equivalence and use it relate the parameters in Eq.&nbsp;(\\ref{eq:ellipse}) to those in Eq.&nbsp;(1). For example, by taking second derivatives of both expressions we can obtain\n",
    "\n",
    "\\begin{align*}\n",
    "A_{mm} &= \\frac{1}{2}\\frac{\\partial^2(\\chi^2)}{\\partial m^2} = \\sum_{k=1}^{N}\\frac{x_k^2}{\\alpha_k^2},\\label{eq:Amm}\\tag{3}\\\\\n",
    "A_{mc} &= \\frac{1}{2}\\frac{\\partial^2(\\chi^2)}{\\partial m\\partial c} = \\sum_{k=1}^{N}\\frac{x_k}{\\alpha_k^2},\\label{eq:Amc}\\tag{4}\\\\\n",
    "A_{cc} &= \\frac{1}{2}\\frac{\\partial^2(\\chi^2)}{\\partial c^2} = \\sum_{k=1}^{N}\\frac{1}{\\alpha_k^2}.\\label{eq:Acc}\\tag{5}\n",
    "\\end{align*}\n",
    "\n",
    "We can also see immediately that the minimum of $\\chi^2$ is $\\chi^2_\\text{min} = \\chi^2(\\hat{m}, \\hat{c})$, and we have already derived expressions for $\\hat{m}$ and $\\hat{c}$ in [NB 6.0](6.0-Fitting-complex-functions.ipynb#Explicit-formulas).\n",
    "\n",
    "Each contour of constant $\\chi^2$ in Eq.&nbsp;(\\ref{eq:ellipse}) defines an [ellipse](https://en.wikipedia.org/wiki/Ellipse#General_ellipse) centered on $(\\hat{m},\\hat{c})$, with its shape determined by $A_{mm}, A_{mc}$, and $A_{cc}$. Writing Eq.&nbsp;(\\ref{eq:ellipse}) in matrix form,\n",
    "\n",
    "\\begin{align*}\n",
    "\\chi^2(m,c) - \\chi^2_\\text{min} &= \\begin{bmatrix}(m - \\hat{m}) & (c - \\hat{c})\\end{bmatrix}\\begin{bmatrix}A_{mm} & A_{mc}\\\\ A_{mc} & A_{cc} \\end{bmatrix}\\begin{bmatrix} m - \\hat{m} \\\\ c - \\hat{c}\\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "and denoting the matrix more compactly as $\\mathbf{A}$, we can use the [principal axis theorem](https://en.wikipedia.org/wiki/Principal_axis_theorem) to show that the eigenvectors of $\\mathbf{A}$ are along the principal axes of the ellipse, and that the eccentricity of the ellipse is $e = \\sqrt{\\lambda_1/\\lambda_2}$, where $\\lambda_1$ and $\\lambda_2$ are the eigenvalues of $\\mathbf{A}$.\n",
    "\n",
    "As discussed in *MU* Sec.&nbsp;7.2.1, the covariance matrix $\\mathbf{C}$ is the inverse of the curvature matrix,\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A}^{-1},\n",
    "$$\n",
    "\n",
    "from which we can determine the parameter uncertainties\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_{\\hat{m}} &= \\sqrt{C_{mm}}, & \\alpha_{\\hat{c}} &= \\sqrt{C_{cc}}\n",
    "\\end{align}\n",
    "\n",
    "and the correlation coefficient\n",
    "\n",
    "$$\n",
    "\\rho_{mc} = \\frac{C_{mc}}{\\sqrt{C_{mm}C_{cc}}}\n",
    "$$\n",
    "\n",
    "We confirm these relationships for our example fit in the code cell below. We use the [`linalg`](https://numpy.org/doc/stable/reference/routines.linalg.html) linear algebra library in SciPy to compute the matrix inverse ([`inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)). We also use the NumPy function [`array_str`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_str.html) to format each matrix element to four digits of precision, with `suppress_small=True` to represent the entries in decimal notation instead of scientific notation. Compare with the curvature matrix $\\mathbf{A}$ at the bottom of p.&nbsp;95 and the error matrix $\\mathbf{C}$ at the top of p.&nbsp;96 in *MU*—note that our column order is reversed from theirs because our `model` function lists the parameters in order (`m`,`c`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "Amm = np.sum(frequency**2 / err**2)\n",
    "Amc = np.sum(frequency / err**2)\n",
    "Acc = np.sum(1 / err**2)\n",
    "\n",
    "A = np.array([[Amm, Amc], [Amc, Acc]])\n",
    "Ainv = linalg.inv(A)\n",
    "\n",
    "rho_mc = pCov[0, 1] / (mAlpha * cAlpha)\n",
    "\n",
    "print(\"Curvature matrix:\")\n",
    "print(np.array_str(A, precision=4, suppress_small=True))\n",
    "print()\n",
    "print(\"Inverse curvature matrix:\")\n",
    "print(np.array_str(Ainv, precision=4, suppress_small=True))\n",
    "print()\n",
    "print(\"Covariance matrix:\")\n",
    "print(np.array_str(pCov, precision=4, suppress_small=True))\n",
    "print()\n",
    "print(f\"Correlation coefficient: {rho_mc:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting confidence limits\n",
    "\n",
    "We saw in [NB 2.0](2.0-Basic-statistics.ipynb#How-to-interpret-the-mean-±-the-standard-error) that the mean and the standard error will both fluctuate with each new measurement, because we determine both from the data. On average, we expect the true mean of the distribution to lie within the confidence limit $\\bar{x}\\pm\\hat{\\alpha}$ in about 68 % of a set of repeated measurements. The code cell below shows how this generalizes to the case of two or more parameters. It simulates 25 sets of measurements like that shown in *MU* Fig.&nbsp;6.1(a) and shows the true values (red marker), the best-fit values (black diamonds), confidence contours for $\\Delta\\chi^2 = 1, 2.71, 4$, and $9$ (black curves), and the 68 % confidence limits for $m$ and $c$ (vertical and horizontal lines, respectively). In simulations (1) and (2), the true value lies outside both $\\hat{m}\\pm\\alpha_{\\hat{m}}$ and $\\hat{c}\\pm\\alpha_{\\hat{c}}$. The true value lies inside the $\\hat{m}\\pm\\alpha_{\\hat{m}}$ confidence interval and at the edge of the $\\hat{c}\\pm\\alpha_{\\hat{c}}$ confidence interval for simulation (3), and it lies within both $\\hat{m}\\pm\\alpha_{\\hat{m}}$ and $\\hat{c}\\pm\\alpha_{\\hat{c}}$ for simulation (9). The second figure shows the data and the fit for simulation (25), for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = np.arange(1, 77)\n",
    "noiseScale = 0.1\n",
    "\n",
    "mRange = [0.925, 1.075]\n",
    "cRange = [-0.35, 0.35]\n",
    "m = np.linspace(mRange[0], mRange[1], Nm)\n",
    "c = np.linspace(cRange[0], cRange[1], Nc)\n",
    "\n",
    "chi2grid_trial = np.zeros([Nc, Nm])\n",
    "\n",
    "fig, ax = plt.subplots(5, 5, sharex=True, sharey=True, figsize=[12.8, 9.6])\n",
    "\n",
    "rg = default_rng(0)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        meanVal = trial + noiseScale * trial * rg.normal(size=np.size(trial))\n",
    "        pHat_trial, pCov_trial = np.polyfit(trial, meanVal, 1, w=1 / (noiseScale * trial), cov=\"unscaled\")\n",
    "        mHat_trial = pHat_trial[0]\n",
    "        cHat_trial = pHat_trial[1]\n",
    "        mAlpha_trial = np.sqrt(pCov_trial[0, 0])\n",
    "        cAlpha_trial = np.sqrt(pCov_trial[1, 1])\n",
    "        ax[i][j].plot(mHat_trial, cHat_trial, \"kD\")\n",
    "        ax[i][j].plot(1, 0, \"ro\")\n",
    "        ax[i][j].set_xlim([mRange[0], mRange[-1]])\n",
    "        ax[i][j].set_xticks([0.95, 1.0, 1.05])\n",
    "        ax[i][j].set_ylim([cRange[0], cRange[1]])\n",
    "        ax[i][j].set_yticks([-0.3, 0.0, 0.3])\n",
    "\n",
    "        def chi2fun(m, c):\n",
    "            return np.sum((meanVal - (m * trial + c)) ** 2 / (noiseScale * trial) ** 2)\n",
    "\n",
    "        for p in range(Nc):\n",
    "            for q in range(Nm):\n",
    "                chi2grid_trial[p, q] = chi2fun(m[q], c[p])\n",
    "\n",
    "        chi2min = chi2fun(mHat_trial, cHat_trial)\n",
    "        level_low = [*chi2min, 1, 2.7, 4, 9]\n",
    "        ax[i][j].contour(\n",
    "            m,\n",
    "            c,\n",
    "            chi2grid_trial,\n",
    "            level_low,\n",
    "            colors=\"k\",\n",
    "            linewidths=1,\n",
    "            linestyles=[\"solid\", \"dashed\", \"dotted\", \"dotted\"],\n",
    "        )\n",
    "        ax[i][j].hlines(cHat_trial + cAlpha_trial * np.array([-1, 1]), m[0], m[-1], linewidths=1, linestyles=\"dotted\")\n",
    "        ax[i][j].vlines(mHat_trial + mAlpha_trial * np.array([-1, 1]), c[0], c[-1], linewidths=1, linestyles=\"dotted\")\n",
    "        ax[i][j].text(1.05, 0.25, f\"({5*i+j+1:d})\")\n",
    "\n",
    "fig.text(0.5, 0.04, \"Gradient (trial/mean)\", ha=\"center\")\n",
    "fig.text(0.04, 0.5, \"Intercept (trial)\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(trial, meanVal, yerr=noiseScale * trial, fmt=\".\")\n",
    "plt.plot(trial, mHat_trial * trial + cHat_trial, \"r-\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Mean value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In the Markdown cell below, assign each of the 25 simulated experiments in the figure above to one of the four categories listed (examples of each are already included). Note that both $\\hat{m}$ and $\\hat{c}$ are consistent with the true value in experiment (9), so it is listed in three separate categories. When you are done, tally the results and enter the fraction in the space provided, and discuss your results briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Response to Exercise 1*\n",
    "\n",
    "* Consistent $\\hat{m}$ ($1\\sigma$):   3, 4, 6, 7, 9\n",
    "* Consistent $\\hat{c}$ ($1\\sigma$):   5, 9\n",
    "* Consistent $\\hat{m}, \\hat{c}$ ($1\\sigma$):   9\n",
    "* Inconsistent $\\hat{m}, \\hat{c}$ ($1\\sigma$):   1, 2, 8\n",
    "\n",
    "\n",
    "* Fraction consistent $\\hat{m}$  ($1\\sigma$):\n",
    "* Fraction consistent $\\hat{c}$ ($1\\sigma$):   \n",
    "* Fraction consistent $\\hat{m}, \\hat{c}$ ($1\\sigma$):   \n",
    "* Fraction inconsistent $\\hat{m}, \\hat{c}$ ($1\\sigma$):   \n",
    "\n",
    "*Discuss your results briefly here. Is it what you expect?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell shows a scatterplot of $(\\hat{m}, \\hat{c})$ (blue dots) for `N_sim = 500` simulated measurements. A red marker indicates the true value, $(1, 0)$. The last simulation for $(\\hat{m}, \\hat{c})$ is shown with a black diamond instead of a blue dot, with $\\Delta\\chi^2 = 1, 2.71, 4$, and $9$ contours also shown for that fit. On average, the fits yield $(\\hat{m}, \\hat{c})$ that are centered on the true value and exhibit an elliptical distribution that resembles the shape of the countour levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sim = 500\n",
    "trial = np.arange(1, 77)\n",
    "noiseScale = 0.1\n",
    "\n",
    "mHat_sim = np.zeros(N_sim)\n",
    "cHat_sim = np.zeros(N_sim)\n",
    "\n",
    "rg = default_rng(0)\n",
    "for i in range(N_sim):\n",
    "    meanVal = trial + noiseScale * trial * rg.normal(size=np.size(trial))\n",
    "    pHat_sim, pCov_sim = np.polyfit(trial, meanVal, 1, w=1 / (noiseScale * trial), cov=\"unscaled\")\n",
    "    mHat_sim[i] = pHat_sim[0]\n",
    "    cHat_sim[i] = pHat_sim[1]\n",
    "\n",
    "\n",
    "Nm = 50\n",
    "Nc = 50\n",
    "\n",
    "mRange = [0.95, 1.06]\n",
    "cRange = [-0.35, 0.35]\n",
    "m = np.linspace(mRange[0], mRange[1], Nm)\n",
    "c = np.linspace(cRange[0], cRange[1], Nc)\n",
    "\n",
    "# Define the chi-squared function for m and c, given the data\n",
    "\n",
    "\n",
    "def chi2fun(m, c):\n",
    "    normres = (meanVal - (m * trial + c)) / (noiseScale * trial)\n",
    "    return np.sum(normres**2)\n",
    "\n",
    "\n",
    "chi2grid_sim = np.zeros([Nc, Nm])\n",
    "for i in range(Nc):\n",
    "    for j in range(Nm):\n",
    "        chi2grid_sim[i, j] = chi2fun(m[j], c[i])\n",
    "\n",
    "chi2min_sim = chi2fun(mHat_sim[-1], cHat_sim[-1])\n",
    "level = chi2min_sim + np.array([1, 2.71, 4, 9])\n",
    "\n",
    "plt.plot(mHat_sim[0:-2], cHat_sim[0:-2], \".\")\n",
    "plt.plot(mHat_sim[-1], cHat_sim[-1], \"kD\", ms=10)\n",
    "plt.plot(1, 0, \"ro\", ms=10)\n",
    "plt.contour(m, c, chi2grid_sim, level, colors=\"k\", linewidths=1, linestyles=[\"solid\", \"dashed\", \"dotted\", \"dotted\"])\n",
    "plt.xlabel(\"Gradient (mean/trial)\")\n",
    "plt.ylabel(\"Intercept (mean)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The error matrix for a linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix representation provides a convenient way to do error propagation. For example, we can rewrite *MU* Eq. (7.28) as\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_Z^2 &= \\left(\\frac{\\partial Z}{\\partial A}\\right)^2\\sigma_A^2 + 2\\left(\\frac{\\partial Z}{\\partial A}\\right)\\left(\\frac{\\partial Z}{\\partial B}\\right)\\sigma_{AB} + \\left(\\frac{\\partial Z}{\\partial B}\\right)^2\\sigma_B^2\\\\\n",
    "&= \\begin{bmatrix}\\frac{\\partial Z}{\\partial A} & \\frac{\\partial Z}{\\partial B}\\end{bmatrix}\\begin{bmatrix}\\sigma_A^2 & \\sigma_{AB} \\\\ \\sigma_{AB} & \\sigma_B^2\\end{bmatrix}\\begin{bmatrix}\\frac{\\partial Z}{\\partial A} \\\\ \\frac{\\partial Z}{\\partial B}\\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "For a linear fit $y = mx + b$, $\\partial y/\\partial m = x$ and $\\partial y/\\partial b = 1$, so we can use the covariance matrix $\\mathbf{C}$ given by the fit to estimate the uncertainty in the *functional estimate* $\\hat{y}$ at a given $x$,\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_\\hat{y}^2 &= \\begin{bmatrix}x & 1\\end{bmatrix}\\begin{bmatrix}C_{mm} & C_{mc} \\\\ C_{mc} & C_{cc}\\end{bmatrix}\\begin{bmatrix}x \\\\ 1\\end{bmatrix}\\\\\n",
    "&= C_{mm} x^2 + 2C_{mc}x + C_{cc}.\n",
    "\\end{align}\n",
    "\n",
    "Note that $\\mathbf{C}$ is determined from the fit to the data, so it represents a *particular sample* from a statistical distribution that describes all similar experiments. Likewise, our result for $\\alpha_\\hat{y}$ will also depend on the data. In contrast, our error propagation calculation involved *known, fixed parameters* $\\sigma_Z, \\sigma_A, \\sigma_B$, and $\\sigma_{AB}$ for an assumed multivariate Gaussian *parent distribution*. Our estimate $\\alpha_\\hat{y}$ essentially replaces the fixed but unknown parent distribution parameters with the mean values $\\{\\hat{m}, \\hat{b}\\}$ and covariance matrix $\\mathbf{C}$, which are known but will fluctuate from one experiment to the next.\n",
    "\n",
    "The quantity $\\alpha_\\hat{y}$ represents the uncertainty in the *model*, in contrast with the $\\alpha_y$ (without the hat on $y$) that we determine from a sample of $y$ *data*. In general these are not the same—for example, we can define $\\alpha_\\hat{y}$ (with the hat) at an arbitrary value of $x$, even where we have no measurements. Moreover, $\\alpha_y$ (without the hat) indicates the uncertainty in *a particular measurement* of $y$, whereas $\\alpha_\\hat{y}$ (with the hat) exploits information *from all measurements* of $y$—assuming, of course, that the model correctly describes the data in the first place!\n",
    "\n",
    "The code cell below plots the fit with the 1*&sigma;* functional uncertainty bounds obtained from the fit covariance matrix `pCov`. We extrapolate the fit up to $x = 150$&nbsp;Hz, well beyond the data, to show that the uncertainty bounds increase as the frequency increases beyond the upper data limit—the bounds approach $(\\hat{m}\\pm\\alpha_{\\hat{m}})x + \\hat{c}$ asymptotically with $|x|\\rightarrow\\infty$, and they are minimized near the center of the data range. We use the NumPy [`stack`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.stack.html) function to create the `2 x N` array,\n",
    "\n",
    "$$\n",
    "\\nabla_{m,b}\\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_N\\\\ 1 & 1 & \\ldots & 1\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "then compute the uncertainty $\\alpha_\\hat{f}(x_i)$ by computing the following for each column $\\nabla_{m,b}y(x_i)$ of $\\nabla_{m,b}\\mathbf{y}$:\n",
    "\n",
    "$$\n",
    "\\alpha_\\hat{f}(x_i) = \\sqrt{[\\nabla_{m,b}y(x_i)]^\\intercal\\,\\mathbf{C}\\,[\\nabla_{m,b}y(x_i)]}.\n",
    "$$\n",
    "\n",
    "Note that NumPy uses the `@` character as the matrix multiplication operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequency array for displaying the model\n",
    "N = 1000\n",
    "fModel = np.linspace(0, 150, N)\n",
    "\n",
    "gradf = np.stack((fModel, np.ones(np.shape(fModel))))\n",
    "alphafhat = np.zeros(np.shape(fModel))\n",
    "for i in np.arange(N):\n",
    "    alphafhat[i] = np.sqrt(np.transpose(gradf[:, i]) @ pCov @ gradf[:, i])\n",
    "\n",
    "# Set initial parameters m0 and b0\n",
    "mInit = 2\n",
    "bInit = 0\n",
    "\n",
    "# Define model function\n",
    "\n",
    "\n",
    "def model(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "\n",
    "fMean = model(fModel, mInit, bInit)\n",
    "fUpper = fMean + alphafhat\n",
    "fLower = fMean - alphafhat\n",
    "\n",
    "# Make the plot\n",
    "plt.plot(fModel, fMean, \"r-\")\n",
    "plt.plot(fModel, fUpper, \"c--\")\n",
    "plt.plot(fModel, fLower, \"c--\")\n",
    "plt.errorbar(frequency, voltage, yerr=err, fmt=\"ko\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Voltage (mV)\")\n",
    "plt.title(\"Data with linear model, initial parameters\")\n",
    "plt.xlim(fModel[0], fModel[-1])\n",
    "plt.ylim(0, 2 * fModel[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here is a list of what you should be able to do after completing this notebook.\n",
    "\n",
    "* Describe the meaning and significance of the error surface.\n",
    "* Construct an error surface for a theoretical model of experimental data.\n",
    "* Describe the meaning and significance of the covariance matrix for a fit.\n",
    "* Describe the relationship between the covariance matrix for a fit and the curvature matrix of the associated error surface.\n",
    "* Use the covariance matrix from a fit to estimate the parameter uncertainties.\n",
    "* Explain how to interpret parameter uncertainties obtained from a fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "© J. Steven Dodge, 2020. Available from [SFU Physics GitLab](https://gitlab.phys.sfu.ca/physcrs/course-material-phys233).  The notebook text is licensed under CC BY 4.0. See more at [Creative Commons](https://creativecommons.org/licenses/by/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
