{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The error matrix\n",
    "\n",
    "Python activities to complement [*Measurements and their Uncertainties*](http://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 7, \"Computer minimization and the error matrix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [The error matrix for a linear fit](#The-error-matrix-for-a-linear-fit)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [Example Fit notebook](A.1-Example-Fit.ipynb) and read *MU* Ch. 7, \"Computer minimization and the error matrix\", with the following [goals](https://wiki.its.sfu.ca/departments/phys-students/index.php/Reading_goals_for_Hughes_and_Hase#Computer_minimization_and_the_error_matrix) in mind.\n",
    "\n",
    "1. Be able to explain qualitatively how data analysis computer programs fit a model to data by minimizing the *&chi;*<sup>2</sup> goodness-of-fit parameter as a function of the model parameters. Specifically,\n",
    "    * recognize that the terms *grid search*, *gradient-descent*, *Newton's method*, *Gauss-Newton*, and *Levenberg-Marquardt* refer to different algorithms for minimizing a function;\n",
    "    * be able to use matrix notation to expand *&chi;*<sup>2</sup> to second order around a particular point in space, as shown in (7.6); and\n",
    "    * be able to write the gradient vector, Hessian matrix, and Jacobian matrix for a function of multiple variables, and explain how they appear in the context of computer minimization routines.\n",
    "2. Be able to describe how the curvature matrix relates to the error surface near $\\chi^2_{\\min}$, and how it can be used to estimate the parameter values at which $\\chi^2=\\chi_{\\text{min}}^2+1$.\n",
    "3. Be able to describe the meaning and significance of the covariance matrix and the correlation matrix.\n",
    "4. Recognize that the covariance matrix can be estimated from a fit by inverting the curvature matrix at the minimum of the error surface.\n",
    "\n",
    "The following code cell includes the initialization commands needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The error matrix for a linear fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example fit\n",
    "The code cell below reproduces the fit in the [Example Fit](A.1-Example-Fit.ipynb). The data are taken from *MU* Exercise (6.1), which is shown in *MU* Fig. 6.1(d) and discussed in *MU* Sec. 7.4.1, \"Worked example 1—a straight-line fit.\" We use the NumPy function [`array_str`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_str.html) in the last line to format the covariance matrix to four digits of precision, with `suppress_small=True` to represent the entries in decimal notation instead of scientific notation. Compare with the error matrix $\\mathbf{\\mathsf{C}}$ at the top of p.&nbsp;96 in *MU*—note that our column order is reversed from theirs because our `model` function lists the parameters in order (`m`,`b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into array\n",
    "frequency, voltage, err = np.genfromtxt('data/Example-Data.csv', delimiter=',', skip_header=1, unpack=True)\n",
    "\n",
    "# Define model function\n",
    "def model(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "# Set initial parameters m0 and b0\n",
    "mInit = 2\n",
    "bInit = 0\n",
    "\n",
    "# Fit the model to the data\n",
    "pOpt, pCov = curve_fit(model, frequency, voltage, p0=[mInit, bInit], sigma=err, absolute_sigma=True)\n",
    "\n",
    "# Assign results of curve_fit to new variables\n",
    "mOpt = pOpt[0]\n",
    "bOpt = pOpt[1]\n",
    "mAlpha = np.sqrt(pCov[0, 0])\n",
    "bAlpha = np.sqrt(pCov[1, 1])\n",
    "rho_mb = pCov[0, 1]/(mAlpha*bAlpha)\n",
    "\n",
    "# Display formatted results\n",
    "print(f\"Model slope (mV/Hz):     {mOpt:.2f} ± {mAlpha:.2f}\")\n",
    "print(f\"Model intercept (mV):    {bOpt:.0f} ± {bAlpha:.0f}\")\n",
    "print(f\"Correlation coefficient: {rho_mb:.1f}\")\n",
    "print(\"Covariance matrix:\")\n",
    "print(np.array_str(pCov, precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix representation provides a convenient way to do error propagation. For example, we can rewrite *MU* Eq. (7.28) as\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_Z^2 &= \\left(\\frac{\\partial Z}{\\partial A}\\right)^2\\sigma_A^2 + 2\\left(\\frac{\\partial Z}{\\partial A}\\right)\\left(\\frac{\\partial Z}{\\partial B}\\right)\\sigma_{AB} + \\left(\\frac{\\partial Z}{\\partial B}\\right)^2\\sigma_B^2\\\\\n",
    "&= \\begin{bmatrix}\\frac{\\partial Z}{\\partial A} & \\frac{\\partial Z}{\\partial B}\\end{bmatrix}\\begin{bmatrix}\\sigma_A^2 & \\sigma_{AB} \\\\ \\sigma_{AB} & \\sigma_B^2\\end{bmatrix}\\begin{bmatrix}\\frac{\\partial Z}{\\partial A} \\\\ \\frac{\\partial Z}{\\partial B}\\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "For a linear fit $y = mx + b$, $\\partial y/\\partial m = x$ and $\\partial y/\\partial b = 1$, so we can use the covariance matrix $\\mathbf{\\mathsf{C}}$ given by the fit to estimate the uncertainty in the *functional estimate* $\\hat{y}$ at a given $x$:\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_\\hat{y}^2 &= \\begin{bmatrix}x & 1\\end{bmatrix}\\begin{bmatrix}\\mathsf{C}_{11} & \\mathsf{C}_{12} \\\\ \\mathsf{C}_{21} & \\mathsf{C}_{22}\\end{bmatrix}\\begin{bmatrix}x \\\\ 1\\end{bmatrix}\\\\\n",
    "&= \\mathsf{C}_{11} x^2 + 2\\mathsf{C}_{12}x + \\mathsf{C}_{22},\n",
    "\\end{align}\n",
    "\n",
    "where in the last line we exploit the symmetry of the covariance matrix, $\\mathbf{\\mathsf{C}}^\\intercal = \\mathbf{\\mathsf{C}}\\Rightarrow \\mathsf{C}_{21} = \\mathsf{C}_{12}.$ Note that $\\alpha_\\hat{y}$ and $\\mathbf{\\mathsf{C}}$ are both determined from the fit, so they represent a *particular sample* from a statistical distribution that describes all similar experiments. In contrast, our earlier error propagation calculation involved known parameters $\\sigma_Z, \\sigma_A, \\sigma_B$, and $\\sigma_{AB}$ for a *parent distribution*. Our result for $\\alpha_\\hat{y}$ essentially treats the values estimated from the fit, $\\hat{m}$, $\\hat{b}$, $\\hat{\\sigma}_m = \\sqrt{C_{11}}$, $\\hat{\\sigma}_{mb} = C_{12}$, and $\\hat{\\sigma}_b = \\sqrt{C_{22}}$, as parameters for a hypothetical multivariate Gaussian parent distribution with mean values $\\hat{m}$, $\\hat{b}$ and covariance matrix $\\mathbf{\\mathsf{C}}$.\n",
    "\n",
    "The quantity $\\alpha_\\hat{y}$ represents the uncertainty in the *model*, in contrast with the $\\alpha_y$ (without the hat on $y$) that we determine from a sample of $y$ *data*. In general these are not the same—for example, we can define $\\alpha_\\hat{y}$ (with the hat) at an arbitrary value of $x$, even where we have no measurements. Moreover, $\\alpha_y$ (without the hat) indicates the uncertainty in *a particular measurement* of $y$, whereas $\\alpha_\\hat{y}$ (with the hat) exploits information *from all measurements* of $y$—assuming, of course, that the model correctly describes the data in the first place!\n",
    "\n",
    "The code cell below plots the fit with the 1*&sigma;* functional uncertainty bounds obtained from the fit covariance matrix `pCov`. We use the NumPy [`stack`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.stack.html) function to create the `2 x N` array,\n",
    "\n",
    "$$\n",
    "\\nabla_{m,b}\\mathbf{y} = \\begin{bmatrix} x_1 & x_2 & \\ldots & x_N\\\\ 1 & 1 & \\ldots & 1\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "then compute the uncertainty $\\alpha_\\hat{f}(x_i)$ by computing the following for each column $\\nabla_{m,b}y(x_i)$ of $\\nabla_{m,b}\\mathbf{y}$:\n",
    "\n",
    "$$\n",
    "\\alpha_\\hat{f}(x_i) = \\sqrt{[\\nabla_{m,b}y(x_i)]^\\intercal\\,\\mathbf{\\mathsf{C}}\\,[\\nabla_{m,b}y(x_i)]}.\n",
    "$$\n",
    "\n",
    "Note that NumPy uses the `@` character as the matrix multiplication operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define frequency array for displaying the model\n",
    "N = 1000\n",
    "fModel = np.linspace(0, 120, N)\n",
    "\n",
    "gradf = np.stack((fModel, np.ones(np.shape(fModel))))\n",
    "alphafhat = np.zeros(np.shape(fModel))\n",
    "for i in np.arange(N):\n",
    "    alphafhat[i] = np.sqrt(np.transpose(gradf[:,i])@pCov@gradf[:,i])\n",
    "    \n",
    "fMean = model(fModel, mInit, bInit)\n",
    "fUpper = fMean + alphafhat\n",
    "fLower = fMean - alphafhat\n",
    "\n",
    "# Make the plot\n",
    "plt.plot(fModel, fMean, 'r-')\n",
    "plt.plot(fModel, fUpper, 'c--')\n",
    "plt.plot(fModel, fLower, 'c--')\n",
    "plt.errorbar(frequency, voltage, yerr=err, fmt='o')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Voltage (mV)')\n",
    "plt.title('Data with linear model, initial parameters')\n",
    "plt.xlim(0, 120)\n",
    "plt.ylim(0, 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "The following code cell uses [`genfromtxt`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html) to import a data file from the [NIST Standard Reference Database](https://www.itl.nist.gov/div898/strd/index.html). Fit this data to the model\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{A}, \\gamma, \\mathbf{x_0}, \\mathbf{\\Delta x}) = A_0\\exp(-\\gamma x) + A_1\\exp[-(x - x_{01})^2/(\\Delta x_1)^2] + A_2\\exp[-(x - x_{02})^2/(\\Delta x_2)^2]\n",
    "$$\n",
    "\n",
    "and report the best-fit values for the fit parameters and their estimated uncertainties. Note that the data file does not contain values for $\\mathbf{\\alpha}_y$. Use $\\chi^2_\\text{min}$ to estimate $\\mathbf{\\alpha}_y$, assuming it is uniform for all $\\mathbf{y}$. Finally, determine the uncertainty $\\mathbf{\\alpha}_\\hat{y}$ in the model function, and plot the data, the fit, and the model uncertainty bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "y, x = np.genfromtxt('https://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Gauss1.dat', \n",
    "                  skip_header=60, unpack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is a list of what you should be able to do after completing this notebook.\n",
    "* Use the covariance matrix to compute the uncertainty in the model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "Notebook by J. S. Dodge, 2019. Available from [SFU GitLab](https://gitlab.rcg.sfu.ca/jsdodge/data-analysis-python). The notebook text is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. See more at [Creative Commons](http://creativecommons.org/licenses/by-nc-nd/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
