{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "Python activities to complement [*Measurements and their Uncertainties*](http://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 8, \"Hypothesis testing—how good are our models?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [The &chi;<sup>2</sup> distribution](#The-$\\chi^2$-distribution)\n",
    "    * [Basic properties](#Basic-properties)\n",
    "    * [Using &chi;<sup>2</sup> to evaluate a fit](#Using-$\\chi^2$-to-evaluate-a-fit)\n",
    "    * [Exercise 1](#Exercise-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [Example Fit notebook](A.1-Example-Fit.ipynb) and read *MU* Ch. 8, \"Hypothesis testing—how good are our models?\", with the following [goals](https://wiki.its.sfu.ca/departments/phys-students/index.php/Reading_goals_for_Hughes_and_Hase#Hypothesis_testing-how_good_are_our_models.3F) in mind.\n",
    "\n",
    "1. Be able to explain how to use the *&chi;*<sup>2</sup> statistic to test whether a set of measurements are consistent with a theoretical model, and why it is useful in this context; also, be able to apply the *&chi;*<sup>2</sup> test to assess the agreement between an experiment and a theoretical description of it. Specifically,\n",
    "     1. be able to state the null hypothesis that is being tested with the *&chi;*<sup>2</sup> test: that the model function *f*(*&theta;*;**x**)  with the best-fit parameters $\\mathbf{\\theta} = \\left\\{\\theta_1,\\ldots,\\theta_{\\mathcal{N} }\\right\\}$ and independent variables $\\mathbf{x} = \\{x_1,\\ldots,x_N\\}$ is consistent with measurements $\\mathbf{y} = \\{y_1,\\ldots,y_N\\}$ that have independently known Gaussian uncertainties $\\left\\{\\alpha_1,\\ldots,\\alpha_N\\right\\}$ for each measurement;\n",
    "     2. be able to explain what the term \"degrees of freedom\" means in the context of a statistical estimate;\n",
    "     3. be able to calculate the number of degrees of freedom in a fit;\n",
    "     4. given *N* data points with known uncertainties and a model with $\\mathcal{N}$ parameters that are optimized to yield a goodness-of-fit statistic $\\chi_\\mathrm{min}^2$, be able to assess the quality of the fit, both qualitatively (by comparing $\\chi_\\mathrm{min}^2$ with the number of degrees of freedom) and quantitatively [by computing $P(\\chi_\\mathrm{min}^2;\\nu)$].\n",
    "2. Be able to describe a variety of methods, including the *&chi;*<sup>2</sup> test, to assess the quality of a fit (see Sec. 8.5.3).\n",
    "3. Recognize that the confidence limits for a parameter estimate will be given by the Student's *t*-distribution when the uncertainty is not known independently, and that this is usually the implicit assumption of curve-fitting software; also, be able to explain why the Student *t*-distribution will approach the Gaussian distribution when the number of degrees of freedom is sufficiently large.\n",
    "4. Recognize that curve-fitting software often scales fit parameter uncertainties automatically, and how this can lead to erroneous assessments of both the fit quality and the parameter uncertainties (see the two **health warnings** in Sec. 8.9); also, recognize the limited context in which scaling uncertainties is appropriate.\n",
    "\n",
    "The following code cell includes the initialization commands needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic properties\n",
    "The $\\chi^2(x;\\nu)$ distribution is what you get when you take $\\nu$ random numbers from the normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$ and take the sum of their squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the RNG\n",
    "random.seed(0)\n",
    "\n",
    "# Generate N=1000 sums of squares of nu=10 random numbers\n",
    "N = 1000\n",
    "nu = 10\n",
    "x = random.randn(N,nu)\n",
    "c2 = np.sum(x**2, axis=1)\n",
    "\n",
    "# Compute the chi-squared PDF\n",
    "c2val = np.linspace(0,30)\n",
    "c2pdf = chi2.pdf(c2val,nu)\n",
    "\n",
    "# Compare the distribution with the chi-squared PDF\n",
    "edges = np.arange(30)\n",
    "plt.hist(c2, bins=edges, range=(0.0, 30.0), density=True, ec='k')\n",
    "plt.plot(c2val, c2pdf)\n",
    "plt.xlabel(\"$\\chi^2$\")\n",
    "plt.ylabel(\"Probability density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $\\chi^2$ to evaluate a fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussing least-squares curve fits the term $\\chi^2$ can mean one of three things, depending on the context:\n",
    "1. The function that we minimize, which involves the data $(\\mathbf{x}, \\mathbf{y})$, the standard error $\\mathbf{\\alpha}$, and the model function $f(x;\\theta)$ for a set of parameters $\\theta$ that we aim to estimate,\n",
    "$$\n",
    "\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha}) = \\sum_{i=1}^N\\frac{[y_i - f(x_i;\\theta)]^2}{\\alpha_i^2};\n",
    "$$\n",
    "\n",
    "2. The minimum value of $\\chi_\\text{min}^2 = \\mathop{\\text{arg min}}_\\theta\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha})$ with respect to the parameters $\\theta$; and\n",
    "\n",
    "3. The probability distribution function given by *MU* Eq. (8.3), which we express here in more conventional notation,\n",
    "$$\n",
    "\\chi^2(x;\\nu) = \\frac{x^{\\frac{\\nu}{2}-1}\\exp(-x/2)}{2^{\\nu/2}\\Gamma(\\nu/2)}.\n",
    "$$\n",
    "\n",
    "Using this terminology to describe the usual procedure for fitting experimental data, we minimize the $\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha})$ *function* (1) to obtain the $\\chi_\\text{min}^2$ *statistic* (2), then compare this statistic to the $\\chi^2(x;\\nu)$ *distribution* (3) to evaluate the quality of the fit.\n",
    "\n",
    "Got that? Good. Now let's put it to use. Simulate voltage measurements across a 100 &Omega; resistor for a range of currents, assuming the voltmeter has a 1 mV uncertainty and a 2 mV offset error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seed the RNG\n",
    "random.seed(0)\n",
    "\n",
    "# Simulate voltage across a 100 Ohm resistor\n",
    "R = 100\n",
    "I_mA = np.arange(0,110,10)\n",
    "I = I_mA*1e-3\n",
    "\n",
    "alphaV = 1e-3\n",
    "Voff = 2e-3\n",
    "Vm = R*I + Voff + alphaV*random.randn(np.size(I))\n",
    "\n",
    "# Plot the data\n",
    "plt.errorbar(I_mA, Vm, fmt='o')\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Voltage (V)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a model function based on Ohm's law and choose an initial guess for the resistance. The voltage uncertainty is too small to be appear on the `errorbar` plot, so we switch to the conventional `plot` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def vOhm(current, resistance):\n",
    "    return current*resistance\n",
    "\n",
    "# Assign initial guess for resistance and\n",
    "RInit = 100\n",
    "Imodel_mA = np.linspace(0,100)\n",
    "Imodel = 1e-3*Imodel_mA\n",
    "Vmodel = vOhm(Imodel, RInit)\n",
    "plt.plot(I_mA, Vm, 'o')\n",
    "plt.plot(Imodel_mA, Vmodel, '-')\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Voltage (V)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Now run `curve_fit` to improve the fit by adjusting *R* to minimize the $\\chi^2$ function (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_fit, R_fit_cov = curve_fit(vOhm, I_mA*1e-3, Vm, p0=[RInit], \n",
    "                             sigma=alphaV*np.ones(np.size(Vm)), absolute_sigma=True)\n",
    "alpha_R = np.sqrt(R_fit_cov[0,0])\n",
    "print(f\"R = {R_fit[0]:.3f} ± {alpha_R:.3f} ohms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the $\\chi^2_\\text{min}$ statistic (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display chi-squared\n",
    "res = Vm - vOhm(I_mA*1e-3, R_fit)\n",
    "normres = res/alphaV\n",
    "chisq = np.sum(normres**2)\n",
    "print(f\"chisq = {chisq:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result signals a problem: we have 11 measurements and 1 fit parameter, so there are 10 statistical degrees of freedom in the fit. The $\\chi^2(x;\\nu=10)$ distribution (3) has a mean value of 10 and a standard deviation of $\\sigma = \\sqrt{2\\nu}\\approx 4.5$, so our result is more than $7\\sigma$ above the value expected for such a fit. We can use [`chi2.cdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html) to compute the precise probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the probability of getting this fit result\n",
    "dof = 10\n",
    "cdf = chi2.cdf(chisq, dof)\n",
    "print(f\"Cumulative probability = {cdf:.6f}\")\n",
    "print(f\"Significance: {1-cdf:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a chance of 3 in a million that we would get such a result at random. Let's look at the residuals to see what might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(I_mA, normres, use_line_collection=True)\n",
    "plt.xlabel('Current (mA)')\n",
    "plt.ylabel('Normalized residual')\n",
    "plt.xlim(-5,105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals reveal a strong trend that indicates our model lacks the necessary freedom to achieve a good fit. From the pattern, we can guess that the problem is an offset error. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new model\n",
    "def vOhm_offset(current, resistance, offset):\n",
    "    return current*resistance + offset\n",
    "\n",
    "VoffInit = 0\n",
    "p_fit, p_fit_cov = curve_fit(vOhm_offset, I_mA*1e-3, Vm, p0=[RInit,VoffInit], \n",
    "                             sigma=alphaV*np.ones(np.size(Vm)), absolute_sigma=True)\n",
    "R_fit = p_fit[0]\n",
    "Voff_fit = p_fit[1]\n",
    "alpha_R = np.sqrt(p_fit_cov[0,0])\n",
    "alpha_Voff = np.sqrt(p_fit_cov[1,1])\n",
    "print(f\"R = {R_fit:.3f} ± {alpha_R:.3f} ohms\")\n",
    "print(f\"Voff = {Voff_fit:.3f} ± {alpha_Voff:.3f} V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the resistance uncertainty increased by about a factor of two. This does *not* mean that the fit is not as good as the first! If the fit quality is better, then the larger uncertainty is actually a more *accurate* estimate of the uncertainty in *R*. Let us check the fit quality now, noting that the extra fit parameter brings the number of statistical degrees of freedom down to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display chi-squared\n",
    "res_offset = Vm - vOhm_offset(I_mA*1e-3, R_fit, Voff_fit)\n",
    "normres_offset = res_offset/alphaV\n",
    "chisq_offset = np.sum(normres_offset**2)\n",
    "print(f\"chisq_offset = {chisq_offset:.1f}\")\n",
    "\n",
    "# Compute the probability of getting this fit result\n",
    "dof_offset = 9\n",
    "cdf_offset = chi2.cdf(chisq_offset, dof)\n",
    "print(f\"Cumulative probability = {cdf_offset:.6f}\")\n",
    "print(f\"Significance: {1-cdf_offset:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! To be sure, though, we need to check that the normalized residuals are consistent with a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(I_mA, normres_offset, use_line_collection=True)\n",
    "plt.xlabel('Current (mA)')\n",
    "plt.ylabel('Normalized residual')\n",
    "plt.xlim(-5,105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear pattern emerges from these residuals, so we can accept the fit and report the parameters with their uncertainties with confidence. Note that we can still get an acceptable $\\chi^2_\\text{min}$ for a poor model if we also overestimate $\\alpha$, so it is essential to examine the residuals in addition to $\\chi^2_\\text{min}$. Also, note that the *reduced* $\\chi^2$ statistic [*MU* Eq. (8.6)],\n",
    "$$\n",
    "\\chi^2_\\nu = \\frac{\\chi^2_\\text{min}}{\\nu},\n",
    "$$\n",
    "has an expectation value of 1, but a standard deviation $\\sigma = \\sqrt{2/\\nu}$, which depends on the number of degrees of freedom. This means that $\\chi^2_\\nu$ statistic can provide a false sense of security if $\\nu$ is large, since the expected deviation from 1 will be small in that case. The reduced $\\chi^2$ is useful for quickly checking if a fit for obvious problems, but for quantitative evaluation you should always use the $\\chi^2$ cumulative distribution function to compute the significance of your $\\chi^2_\\text{min}$ statistic for the given number of degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Import the data in `data/pendulum.csv`, which contains simulated measurements of a pendulum period $T$ as a function of its length $L$. Fit the data to the model\n",
    "$$\n",
    "T = 2\\pi\\sqrt{L/g},\n",
    "$$\n",
    "where $g$ is the acceleration of gravity. Report the result of the fit, its $1\\sigma$ uncertainty, the $\\chi^2_\\text{min}$ statistic, and the significance of this statistic. Plot the normalized residuals and evaluate their consistency with the assumption of Gaussian random errors with the given $\\alpha_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "Notebook by J. S. Dodge, 2019. Available from [SFU GitLab](https://gitlab.rcg.sfu.ca/jsdodge/data-analysis-python). The notebook text is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. See more at [Creative Commons](http://creativecommons.org/licenses/by-nc-nd/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
