{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "Python activities to complement [*Measurements and their Uncertainties*](https://www.oupcanada.com/catalog/9780199566334.html) (*MU*), Chapter 8, \"Hypothesis testing—how good are our models?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [The &chi;<sup>2</sup> distribution](#The-$\\chi^2$-distribution)\n",
    "    * [Basic properties](#Basic-properties)\n",
    "    * [Using &chi;<sup>2</sup> to evaluate a fit](#Using-$\\chi^2$-to-evaluate-a-fit)\n",
    "    * [Exercise 1](#Exercise-1)\n",
    "* [Scaling uncertainties](#Scaling-uncertainties)\n",
    "    * [Exercise 2](#Exercise-2)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [Example Fit notebook](A.1-Example-Fit.ipynb) and read *MU* Ch. 8, \"Hypothesis testing—how good are our models?\", with the following [goals](A.0-Reading-goals.ipynb#Hypothesis-testing---how-good-are-our-models?) in mind.\n",
    "\n",
    "1. Be able to explain how to use the *&chi;*<sup>2</sup> statistic to test whether a set of measurements are consistent with a theoretical model, and why it is useful in this context; also, be able to apply the *&chi;*<sup>2</sup> test to assess the agreement between an experiment and a theoretical description of it. Specifically,\n",
    "     1. be able to state the null hypothesis that is being tested with the *&chi;*<sup>2</sup> test: that the model function *f*(*&theta;*;**x**)  with the best-fit parameters $\\mathbf{\\theta} = \\left\\{\\theta_1,\\ldots,\\theta_{\\mathcal{N} }\\right\\}$ and independent variables $\\mathbf{x} = \\{x_1,\\ldots,x_n\\}$ is consistent with measurements $\\mathbf{y} = \\{y_1,\\ldots,y_n\\}$ that have independently known Gaussian uncertainties $\\left\\{\\alpha_1,\\ldots,\\alpha_n\\right\\}$ for each measurement;\n",
    "     2. be able to explain what the term \"degrees of freedom\" means in the context of a statistical estimate;\n",
    "     3. be able to calculate the number of degrees of freedom in a fit;\n",
    "     4. given *n* data points with known uncertainties and a model with $\\mathcal{N}$ parameters that are optimized to yield a goodness-of-fit statistic $\\chi_\\mathrm{min}^2$, be able to assess the quality of the fit, both qualitatively (by comparing $\\chi_\\mathrm{min}^2$ with the number of degrees of freedom) and quantitatively [by computing $P(\\chi_\\mathrm{min}^2;\\nu)$].\n",
    "2. Be able to describe a variety of methods, including the *&chi;*<sup>2</sup> test, to assess the quality of a fit (see Sec. 8.5.3).\n",
    "3. Recognize that the confidence limits for a parameter estimate will be given by the Student's *t*-distribution when the uncertainty is not known independently, and that this is usually the implicit assumption of curve-fitting software; also, be able to explain why the Student *t*-distribution will approach the Gaussian distribution when the number of degrees of freedom is sufficiently large.\n",
    "4. Recognize that curve-fitting software often scales fit parameter uncertainties automatically, and how this can lead to erroneous assessments of both the fit quality and the parameter uncertainties (see the two **health warnings** in Sec. 8.9); also, recognize the limited context in which scaling uncertainties is appropriate.\n",
    "\n",
    "The following code cell includes the initialization commands needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic properties\n",
    "The $\\chi^2(x;\\nu)$ distribution is what you get when you take $\\nu$ random numbers from the normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$ and take the sum of their squares."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Seed the RNG\n",
    "rng = default_rng(0)\n",
    "\n",
    "# Generate n=1000 sums of squares of nu=10 random numbers\n",
    "n = 1000\n",
    "nu = 10\n",
    "x = rng.normal(size=(n, nu))\n",
    "c2 = np.sum(x**2, axis=1)\n",
    "\n",
    "# Compute the chi-squared PDF\n",
    "c2_val = np.linspace(0, 30)\n",
    "c2_pdf = chi2.pdf(c2_val, nu)\n",
    "\n",
    "# Compare the distribution with the chi-squared PDF\n",
    "edges = np.arange(30)\n",
    "plt.hist(c2, bins=edges, range=(0.0, 30.0), density=True, ec=\"k\")\n",
    "plt.plot(c2_val, c2_pdf)\n",
    "plt.xlabel(r\"$\\chi^2$\")\n",
    "plt.ylabel(\"Probability density\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $\\chi^2$ to evaluate a fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussing least-squares curve fits the term $\\chi^2$ can mean one of three things, depending on the context:\n",
    "1. The function that we minimize, which involves the data $(\\mathbf{x}, \\mathbf{y})$, the standard error $\\mathbf{\\alpha}$, and the model function $f(x;\\theta)$ for a set of parameters $\\theta$ that we aim to estimate,\n",
    "\n",
    "$$\n",
    "\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha}) = \\sum_{i=1}^N\\frac{[y_i - f(x_i;\\theta)]^2}{\\alpha_i^2};\n",
    "$$\n",
    "\n",
    "2. The minimum value of $\\chi_\\text{min}^2 = \\mathop{\\text{arg min}}_\\theta\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha})$ with respect to the parameters $\\theta$; and\n",
    "\n",
    "3. The probability distribution function given by *MU* Eq. (8.3), which we express here in more conventional notation,\n",
    "\n",
    "$$\n",
    "\\chi^2(x;\\nu) = \\frac{x^{\\nu/2 - 1}\\exp(-x/2)}{2^{\\nu/2}\\Gamma(\\nu/2)}.\n",
    "$$\n",
    "\n",
    "Using this terminology to describe the usual procedure for fitting experimental data, we minimize the $\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha})$ *function* (1) to obtain the $\\chi_\\text{min}^2$ *statistic* (2), then compare this statistic to the $\\chi^2(x;\\nu)$ *distribution* (3) to evaluate the quality of the fit.\n",
    "\n",
    "Got that? Good. Now let's put it to use. Simulate voltage measurements across a 100 &Omega; resistor for a range of currents, assuming the voltmeter has a 1 mV uncertainty and a 2 mV offset error."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Seed the RNG\n",
    "rng = default_rng(0)\n",
    "\n",
    "# Simulate voltage across a 100 ohm resistor\n",
    "resistance = 100\n",
    "current_milliamp = np.arange(0, 110, 10)\n",
    "current_amp = current_milliamp * 1e-3\n",
    "\n",
    "alpha_v = 1e-3\n",
    "v_off = 2e-3\n",
    "v_meas = resistance * current_amp + v_off + alpha_v * rng.normal(size=np.size(current_amp))\n",
    "\n",
    "# Plot the data\n",
    "plt.errorbar(current_milliamp, v_meas, fmt=\"o\")\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Voltage (V)\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a model function based on Ohm's law and choose an initial guess for the resistance. The voltage uncertainty is too small to appear on the `errorbar` plot, so we switch to the conventional `plot` command."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define model\n",
    "\n",
    "\n",
    "def v_ohm(current_amp, resistance):\n",
    "    return current_amp * resistance\n",
    "\n",
    "\n",
    "# Assign initial guess for resistance and\n",
    "resistance_init = 100\n",
    "current_model_milliamp = np.linspace(0, 100)\n",
    "current_model_amp = 1e-3 * current_model_milliamp\n",
    "v_model = v_ohm(current_model_amp, resistance_init)\n",
    "plt.plot(current_milliamp, v_meas, \"o\")\n",
    "plt.plot(current_model_milliamp, v_model, \"-\")\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Voltage (V)\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Now run `curve_fit` to improve the fit by adjusting *R* to minimize the $\\chi^2$ function (1)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r_fit, r_fit_cov = curve_fit(\n",
    "    v_ohm,\n",
    "    current_milliamp * 1e-3,\n",
    "    v_meas,\n",
    "    p0=[resistance_init],\n",
    "    sigma=alpha_v * np.ones(np.size(v_meas)),\n",
    "    absolute_sigma=True,\n",
    ")\n",
    "alpha_r = np.sqrt(r_fit_cov[0, 0])\n",
    "print(f\"R = {r_fit[0]:.3f} ± {alpha_r:.3f} ohms\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the $\\chi^2_\\text{min}$ statistic (2)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute and display chi-squared\n",
    "res = v_meas - v_ohm(current_milliamp * 1e-3, r_fit)\n",
    "normres = res / alpha_v\n",
    "chisq = np.sum(normres**2)\n",
    "print(f\"chisq = {chisq:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result signals a problem: we have 11 measurements and 1 fit parameter, so there are 10 statistical degrees of freedom in the fit. The $\\chi^2(x;\\nu=10)$ distribution (3) has a mean value of 10 and a standard deviation of $\\sigma = \\sqrt{2\\nu}\\approx 4.5$, so our result is more than $7\\sigma$ above the value expected for such a fit. We can use [`chi2.cdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html) to compute the precise probability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Compute the probability of getting this fit result\n",
    "dof = 10\n",
    "cdf = chi2.cdf(chisq, dof)\n",
    "print(f\"Cumulative probability = {cdf:.6f}\")\n",
    "print(f\"Significance: {1-cdf:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a chance of 3 in a million that we would get such a result at random. Let's look at the residuals to see what might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.stem(current_milliamp, normres)\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "plt.xlim(-5, 105)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals reveal a strong trend that indicates our model lacks the necessary freedom to achieve a good fit. From the pattern, we can guess that the problem is an offset error. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define new model\n",
    "\n",
    "\n",
    "def v_ohm_offset(current_amp, resistance, offset):\n",
    "    return current_amp * resistance + offset\n",
    "\n",
    "\n",
    "v_off_init = 0\n",
    "p_fit, p_fit_cov = curve_fit(\n",
    "    v_ohm_offset,\n",
    "    current_milliamp * 1e-3,\n",
    "    v_meas,\n",
    "    p0=[resistance_init, v_off_init],\n",
    "    sigma=alpha_v * np.ones(np.size(v_meas)),\n",
    "    absolute_sigma=True,\n",
    ")\n",
    "r_fit = p_fit[0]\n",
    "v_off_fit = p_fit[1]\n",
    "alpha_r = np.sqrt(p_fit_cov[0, 0])\n",
    "alpha_v_off = np.sqrt(p_fit_cov[1, 1])\n",
    "print(f\"R = {r_fit:.3f} ± {alpha_r:.3f} ohms\")\n",
    "print(f\"v_off = {v_off_fit:.3f} ± {alpha_v_off:.3f} V\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the resistance uncertainty increased by about a factor of two. This does *not* mean that the fit is not as good as the first! If the fit quality is better, then the larger uncertainty is actually a more *accurate* estimate of the uncertainty in *R*. Let us check the fit quality now, noting that the extra fit parameter brings the number of statistical degrees of freedom down to 9."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute and display chi-squared\n",
    "r_offset = v_meas - v_ohm_offset(current_milliamp * 1e-3, r_fit, v_off_fit)\n",
    "normres_offset = r_offset / alpha_v\n",
    "chisq_offset = np.sum(normres_offset**2)\n",
    "print(f\"chisq_offset = {chisq_offset:.1f}\")\n",
    "\n",
    "# Compute the probability of getting this fit result\n",
    "dof_offset = 9\n",
    "cdf_offset = chi2.cdf(chisq_offset, dof_offset)\n",
    "print(f\"Cumulative probability = {cdf_offset:.6f}\")\n",
    "print(f\"Significance: {1-cdf_offset:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! To be sure, though, we need to check that the normalized residuals are consistent with a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.stem(current_milliamp, normres_offset)\n",
    "plt.xlabel(\"Current (mA)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "plt.xlim(-5, 105)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear pattern emerges from these residuals, so we can accept the fit and report the parameters with their uncertainties with confidence. Note that we can still get an acceptable $\\chi^2_\\text{min}$ for a poor model if we also overestimate $\\alpha$, so it is essential to examine the residuals in addition to $\\chi^2_\\text{min}$. Also, note that the *reduced* $\\chi^2$ statistic [*MU* Eq. (8.6)],\n",
    "$$\n",
    "\\chi^2_\\nu = \\frac{\\chi^2_\\text{min}}{\\nu},\n",
    "$$\n",
    "has an expectation value of 1, but a standard deviation $\\sigma = \\sqrt{2/\\nu}$, which depends on the number of degrees of freedom. This means that $\\chi^2_\\nu$ statistic can provide a false sense of security if $\\nu$ is large, since the expected deviation from 1 will be small in that case. The reduced $\\chi^2$ is useful for quickly checking a fit for obvious problems, but for quantitative evaluation you should always use the $\\chi^2$ cumulative distribution function to compute the significance of your $\\chi^2_\\text{min}$ statistic for the given number of degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Import the data in `data/pendulum.csv`, which contains simulated measurements of a pendulum period $T$ as a function of its length $L$. Fit the data to the model\n",
    "$$\n",
    "T = 2\\pi\\sqrt{L/g},\n",
    "$$\n",
    "where $g$ is the acceleration of gravity. Report the result of the fit, its $1\\sigma$ uncertainty, the $\\chi^2_\\text{min}$ statistic, and the significance of this statistic. Plot the normalized residuals and evaluate their consistency with the assumption of Gaussian random errors with the given $\\alpha_T$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling uncertainties\n",
    "It is worth noting that the $\\chi^2$ function is a function of three quantities, all of which are important:\n",
    "\n",
    "1. The data, $\\mathbf{x}, \\mathbf{y}$;\n",
    "2. The physical model, $\\mathbf{f}(\\mathbf{\\theta};\\mathbf{x})$; and\n",
    "3. The noise model, which in least-squares analysis we assume to be normally distributed random values with zero mean and standard deviation $\\mathbf{\\alpha}$.\n",
    "\n",
    "So far we have looked to the $\\chi^2_\\text{min}$ statistic to evaluate how well the model fits the data, assuming that $\\mathbf{\\alpha}$ is known from independent measurements. But the noise measurements are also susceptible to both random and systematic errors, and an error of only 30% or 40% in $\\mathbf{\\alpha}$ will cause $\\chi^2_\\text{min}$ to change by *a factor of two*. This can produce a value for $\\chi^2_\\text{min}$ that signals a poor fit even though the physical model is perfectly satisfactory, as the example below shows.\n",
    "\n",
    "We simulate the time $t$ for an object to fall a distance $d$ from rest. We assume that a stopwatch with 1 ms precision was used in the measurements, and take that as our initial estimate of the timing uncertainty. The true timing uncertainty, however, is 5 ms, and is limited by the stopwatch user, not the stopwatch itself."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Seed the RNG\n",
    "rng = default_rng(0)\n",
    "\n",
    "# Simulate free-fall measurements\n",
    "g = 9.81  # m/s^2\n",
    "d_cm = np.arange(10, 110, 10)  # cm\n",
    "d = 1e-2 * d_cm\n",
    "\n",
    "alpha_t = 5e-3\n",
    "alpha_t_est = 1e-3\n",
    "\n",
    "tm = np.sqrt(2 * d / g) + alpha_t * rng.normal(size=np.size(d))\n",
    "\n",
    "# Round to the nearest millisecond\n",
    "tm = np.round(tm, decimals=3)\n",
    "\n",
    "# Plot the data\n",
    "plt.errorbar(d_cm, tm, fmt=\"o\")\n",
    "plt.xlabel(\"Distance (cm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xlim(0, 105)\n",
    "plt.ylim(0, 0.5);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we define the physical model, select an initial guess, and compare with the model before proceeding with the automated fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define model\n",
    "\n",
    "\n",
    "def t_fall(distance, g):\n",
    "    return np.sqrt(2 * distance / g)\n",
    "\n",
    "\n",
    "# Assign initial guess and plot\n",
    "# Use 1000 points for the model to give\n",
    "# a smooth curve near (0,0)\n",
    "g_init = 9.8\n",
    "dmodel_cm = np.linspace(0, 100, 1000)\n",
    "dmodel = 1e-2 * dmodel_cm\n",
    "tmodel = t_fall(dmodel, g_init)\n",
    "plt.plot(d_cm, tm, \"o\")\n",
    "plt.plot(dmodel_cm, tmodel, \"-\")\n",
    "plt.xlabel(\"Distance (cm)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xlim(0, 105)\n",
    "plt.ylim(0, 0.5);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good, so we proceed with the fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "g_fit, g_fit_cov = curve_fit(\n",
    "    t_fall, d_cm * 1e-2, tm, p0=[g_init], sigma=alpha_t_est * np.ones(np.size(tm)), absolute_sigma=True\n",
    ")\n",
    "alpha_g = np.sqrt(g_fit_cov[0, 0])\n",
    "print(f\"g = {g_fit[0]:.2f} ± {alpha_g:.2f} m/s^2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the $\\chi^2_\\text{min}$ statistic."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute and display chi-squared\n",
    "res = tm - t_fall(d_cm * 1e-2, g_fit)\n",
    "normres = res / alpha_t_est\n",
    "chisq = np.sum(normres**2)\n",
    "print(f\"chisq = {chisq:.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is a problem: $\\chi^2_\\nu = \\chi^2_\\text{min}/\\nu \\gg 33$! As the code cell below shows, the probability of obtaining $\\chi^2_\\text{min}\\leq 143.1$ is so close to 1 that the difference `1 - cdf` is below the numerical precision. If we really want to know the value, we can use the *survival function* method, [`sf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.sf.html), which is designed to compute small deviations in `1 - cdf` with greater precision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Compute the probability of getting this fit result\n",
    "dof = 9\n",
    "cdf = chi2.cdf(chisq, dof)\n",
    "sig = chi2.sf(chisq, dof)\n",
    "print(f\"Cumulative probability = {cdf:.6f}\")\n",
    "print(f\"Significance (1-cdf method): {1 - cdf:.6f}\")\n",
    "print(f\"Significance (sf method): {sig:.6e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the last example that residual analysis can help identify an inaccurate physical model, which will typically produce residuals that *lack* randomness. Residual analysis can also help diagnose problems with the noise model. In this case, the residuals will typically be random, but with a distribution that is different from what we expect."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.stem(d_cm, normres)\n",
    "plt.xlabel(\"Distance (cm)\")\n",
    "plt.ylabel(\"Normalized residual\")\n",
    "plt.xlim(0, 105)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in *MU* Sec. 8.9, sometimes this can be addressed by simply rescaling your original estimate of $\\mathbf{\\alpha}$ by\n",
    "$$\n",
    "S = \\sqrt{\\frac{\\chi^2_\\text{min}}{\\nu}} = \\sqrt{\\chi^2_\\nu},\n",
    "$$\n",
    "so that the resulting $\\chi'^2_\\text{min} = S\\chi^2_\\text{min}$ matches the number of degrees of freedom exactly. Of course, you forfeit the ability to use $\\chi'^2_\\text{min}$ as evidence of fit quality when you do this, so it is usually preferable to just go back and do a more careful measurement of the noise. This is definitely the case in this example, where $\\chi^2_\\text{min}$ indicates such a poor fit.\n",
    "\n",
    "When $S\\approx 1$, though, it may be advantageous to scale the residuals to improve the accuracy of the *parameter uncertainty,* since the parameter uncertainty estimate given by `curve_fit` depends on $\\chi^2_\\text{min}$. The following code cell computes the scale factor for our example, then uses this to revise $\\alpha_g$. Another way to achieve the same result is to simply set `absolute_sigma=False` in the `curve_fit` algorithm—this will compute the fit by treating the values assigned to `sigma` as *relative uncertainties*, with an overall scale factor given by $S$ above. The defaults are `absolute_sigma=False` and `sigma` set to a vector of ones with the same size as the `y` data, so that when both `sigma` and `absolute_sigma` are omitted, `curve_fit` minimizes\n",
    "\n",
    "$$\n",
    "\\chi^2(\\mathbf{\\theta};\\mathbf{x}, \\mathbf{y}, \\mathbf{\\alpha}=\\mathbf{1}) = \\sum_{i=1}^N[y_i - f(x_i;\\theta)]^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Determine the scaling factor\n",
    "scale = np.sqrt(chisq / dof)\n",
    "print(f\"Scale factor: {scale:.3f}\")\n",
    "\n",
    "# Scale measurement uncertainty by S\n",
    "print(f\"scale*alpha_t_est = {1e3 * scale * alpha_t_est:.2f} ms\")\n",
    "\n",
    "# Compare with true measurement uncertainty\n",
    "print(f\"True alpha_t = {1e3 * alpha_t:.2f}\")\n",
    "\n",
    "# Scale parameter uncertainty by S\n",
    "print(f\"g = ({g_fit[0]:.2f} ± {scale * alpha_g:.2f}) m/s^2 (scaled uncertainty)\")\n",
    "\n",
    "# Repeat fit with absolute_sigma=False\n",
    "g_fit_scaled, g_fit_cov_scaled = curve_fit(\n",
    "    t_fall, d_cm * 1e-2, tm, p0=[g_init], sigma=alpha_t_est * np.ones(np.size(tm)), absolute_sigma=False\n",
    ")\n",
    "alpha_g_scaled = np.sqrt(g_fit_cov_scaled[0, 0])\n",
    "print(f\"g = ({g_fit_scaled[0]:.2f} ± {alpha_g_scaled:.2f}) m/s^2 (absolute_sigma=False)\")\n",
    "\n",
    "# Repeat fit without sigma or absolute_sigma keywords\n",
    "g_fit_nosig, g_fit_cov_nosig = curve_fit(t_fall, d_cm * 1e-2, tm, p0=[g_init])\n",
    "alpha_g_nosig = np.sqrt(g_fit_cov_nosig[0, 0])\n",
    "print(f\"g = ({g_fit_nosig[0]:.2f} ± {alpha_g_nosig:.2f}) m/s^2 (no sigma)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "The following code cell uses [`genfromtxt`](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html) to import a data file from the [NIST Standard Reference Database](https://www.itl.nist.gov/div898/strd/index.html). Fit this data to the model\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{A}, \\mathbf{\\kappa}) = A_1\\exp(-\\kappa_1 x) + A_2\\exp(-\\kappa_2 x) + A_3\\exp(-\\kappa_3 x)\n",
    "$$\n",
    "\n",
    "and report the best-fit values for $\\mathbf{A}$ and $\\mathbf{\\kappa}$ and their estimated uncertainties. Note that the data file does not contain values for $\\mathbf{\\alpha}_y$. Use $\\chi^2_\\text{min}$ to estimate $\\mathbf{\\alpha}_y$, assuming it is uniform for all $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Code cell for Exercise 2\n",
    "# Use this cell for your response, adding cells if necessary.\n",
    "y, x = np.genfromtxt(\n",
    "    \"https://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Lanczos3.dat\", skip_header=60, unpack=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is a list of what you should be able to do after completing this notebook.\n",
    "* Compute $\\chi^2_\\text{min}$ for a fit and use it to evaluate the fit quality\n",
    "* Use the [`cdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.cdf.html) method of the [`chi2`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html) distribution object to compute the significance of a given value for $\\chi^2_\\text{min}$\n",
    "* Use residual analysis to identify lack of fit between the data and the physical model, the data and the noise model, or both\n",
    "* Use residual analysis to guide revision of an unsatisfactory physical model or noise model\n",
    "* Recognize that the `absolute_sigma` optional keyword determines whether the `curve_fit` function uses unscaled (`absolute_sigma=False`) or scaled (`absolute_sigma=True`) uncertainties\n",
    "* Use the residuals to estimate the measurement uncertainty when an independent estimate is unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About this notebook\n",
    "© J. Steven Dodge, 2020. The notebook text is licensed under CC BY 4.0. See more at [Creative Commons](https://creativecommons.org/licenses/by/4.0/). The notebook code is open source under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
