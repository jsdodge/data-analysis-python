{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic probability\n",
    "\n",
    "Python activities to complement *Measurements and their Uncertainties* (*MU*), Chapter 3, \"Uncertainties as probabilities.\" \n",
    "\n",
    "Author: J. S. Dodge, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Probability calculations](#Probability-calculations)\n",
    "    * [Probability calculations with the uniform distribution](#Probability-calculations-with-the-uniform-distribution)\n",
    "        * [Programming notes 1](#Programming-notes-1)\n",
    "        * [Programming notes 2](#Programming-notes-2)\n",
    "        * [Programming notes 3](#Programming-notes-3)\n",
    "        * [Programming notes 4](#Programming-notes-4)\n",
    "    * [Exercise 1](#Exercise-1)\n",
    "    * [Exercise 2](#Exercise-2)\n",
    "    * [Probability calculations with the normal distribution](#Probability-calculations-with-the-normal-distribution)\n",
    "        * [Programming notes 5](#Programming-notes-5)\n",
    "    * [Exercise 3](#Exercise-3)\n",
    "    * [Outliers, or: Chauvenet is dead. Let his criterion die with him.](#Outliers,-or&#58;-Chauvenet-is-dead.-Let-his-criterion-die-with-him.)\n",
    "* [The Poisson distribution](#The-Poisson-distribution)\n",
    "* [The central limit theorem](#The-central-limit-theorem)\n",
    "* [Summary](#Summary)\n",
    "* [Further reading](#Further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Before proceeding with this notebook you should review the topics from the [previous notebook](2.0-Basic-statistics.ipynb) and read *MU* Ch. 3, \"Uncertainties as probabilities,\" with the following [goals](https://wiki.its.sfu.ca/departments/phys-students/index.php/Reading_goals_for_Hughes_and_Hase#Uncertainties_as_probabilities) in mind.\n",
    "\n",
    "1. Be able to explain what a probability distribution function $P_\\text{DF}(x)$ represents and why Eqs. (3.1) - (3.6) follow from its definition.\n",
    "2. Be able to recall and use Eqs. (3.1) - (3.3) to perform simple probability calculations for an arbitrary $P_\\text{DF}(x)$, including:\n",
    "    1. Check that $P_\\text{DF}(x)$ is properly normalized, and identify the correct normalization factor if it is not;\n",
    "    2. Evaluate the expectation value of a function $f(x)$; and\n",
    "    3. Evaluate the expectation value of the mean and the variance.\n",
    "3. Be able to recall the definitions (3.7) and (3.8) of the Gaussian probability distribution function and the error function, respectively, and know how to use the error function in simple probability calculations like the one given in Sec. 3.2.2.\n",
    "4. Be aware of the rules described in Sec. 3.3.2 for rejecting outliers, be able to follow a well-defined procedure for doing so, and be able to suggest alternatives to throwing away data points.\n",
    "5. Be able to describe the basic properties of a Poisson distribution $P(N;\\bar{N})$, including:\n",
    "    1. its functional form;\n",
    "    2. the kind of experimental data that will be described by it;\n",
    "    3. the expectation values of its mean and variance; and\n",
    "    4. the Gaussian probability distribution that approximates it for $N\\rightarrow\\infty$.\n",
    "6. Be able to sketch a Poisson distribution for a given mean and standard deviation, and be able to estimate the mean and standard deviation from the plot of a Poisson distribution.\n",
    "7. Be able to state the central limit theorem and recognize how it is used to justify the assumption of Gaussian errors in many experiments.\n",
    "\n",
    "The following code cell includes the usual initialization commands, updated to load the normal distribution object `norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability calculations\n",
    "*MU* Sec. 3.1 lists three integral expressions that describe important properties of a continuous PDF.\n",
    "\n",
    "Eq. (3.1) is a *normalization condition* that ensures that the probability summed over all possible outcomes is one:\n",
    "\n",
    "<a id=\"MU(3.1)\"></a>$$ \\int_{-\\infty}^{\\infty}\\text{d}x\\,P_\\text{DF}(x) = 1.$$\n",
    "\n",
    "Eq. (3.2) allows us to predict the probabilities of specific outcomes:\n",
    "\n",
    "<a id=\"MU(3.2)\"></a>$$P(x_1 \\le x \\le x_2) = \\int_{x_1}^{x_2}\\text{d}x\\,P_\\text{DF}(x).$$\n",
    "\n",
    "Eq. (3.3) expresses the *expectation* of $x^n$, given by the weighted average\n",
    "\n",
    "<a id=\"MU(3.3)\"></a>$$ \\left\\langle x^n\\right\\rangle = \\int_{-\\infty}^{\\infty}\\text{d}x\\,x^n P_\\text{DF}(x), $$\n",
    "\n",
    "where we use the $\\langle x^n\\rangle$ notation discussed in the [previous notebook](2.0-Basic-statistics.ipynb#Determining-the-mean-and-variance-from-the-distribution) instead of the overline notation $\\overline{x^n}$ used in *MU*. This quantity is also called the $n$th moment of $x$.\n",
    "\n",
    "We can also generalize Eq. (3.3) to describe the expectation of an arbitrary *function*, $f(x)$,\n",
    "\n",
    "<a id=\"<f(x)>\"></a>$$ \\left\\langle f(x)\\right\\rangle = \\int_{-\\infty}^{\\infty}\\text{d}x\\,f(x) P_\\text{DF}(x). $$\n",
    "\n",
    "Finally, *MU* Eq. (3.6) gives a useful identity, proven in *MU* Eq. (3.5),\n",
    "\n",
    "<a id=\"MU(3.6)\"></a>$$ \\sigma^2 = \\langle(x-\\mu)^2\\rangle = \\langle x^2\\rangle - \\langle x\\rangle^2. $$\n",
    "\n",
    "Usually we will evaluate these expressions numerically, not analytically, and there are several SciPy routines to help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability calculations with the uniform distribution\n",
    "The uniform distribution is appropriate for measurements with discrete precision (see *MU*, Sec. 1.3.2), and you evaluated the integral in *MU* Eq. (3.2) by hand for the standard uniform distribution $\\mathcal{U}(x;0,1)$ in [Exercise  3](2.0-Basic-statistics.ipynb#Exercise-3) of the last notebook. The following code cell shows how to do this numerically for $P(0.0 \\le x \\le 0.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 1\n",
    "The first line imports the [`uniform`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html) distribution object from [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html), and the second line imports [`quad`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html), which is short for [quadrature](https://en.wikipedia.org/wiki/Quadrature_(mathematics)), from [`scipy.integrate`](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html). The `quad` routine returns two numbers: the value of the integral, assigned here to `P`, and an estimate of its absolute error, assigned to `P_err`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P =  0.5\n",
      "P_err =  5.551115123125783e-15\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.integrate import quad\n",
    "\n",
    "P, P_err = quad(uniform.pdf, 0.0, 0.5)\n",
    "print(\"P = \", P)\n",
    "print(\"P_err = \", P_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we compute the mean by evaluating *MU* Eq. (3.4),\n",
    "\n",
    "<a id=\"MU(3.4)\"></a>$$ \\left\\langle x\\right\\rangle = \\int_{-\\infty}^{\\infty}\\text{d}x\\,x \\mathcal{U}(x;0,1). $$\n",
    "\n",
    "We then compute the second moment by evaluating [*MU* Eq. (3.3)](#MU(3.3)) with $n=2$, and the variance by evaluating [*MU* Eq. (3.6)](#MU(3.6))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 2\n",
    "The `quad` routine requires a [function](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) of a single argument to integrate, which we define in the first two statements below. It has the structure\n",
    "\n",
    "    def <name>(<input args>):\n",
    "        <intermediate statements>\n",
    "        return <output args>\n",
    "\n",
    "Here, the function *name* is `mean_int`, and it has a single input argument `x`. This is a simple function, so we do not need to include any intermediate statements; we just `return` the single output argument given by `x*uniform.pdf(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.5\n",
      "Second moment:  0.33333333333333337\n",
      "Variance:  0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Define integrand for the mean\n",
    "def mean_int(x):\n",
    "    return  x*uniform.pdf(x)\n",
    "\n",
    "# Integrate over domain and print result\n",
    "xbar, xbar_err  =  quad(mean_int, 0.0, 1.0)\n",
    "print(\"Mean: \", xbar)\n",
    "\n",
    "# Repeat for the second moment\n",
    "def m2_int(x):\n",
    "    return  x**2*uniform.pdf(x)\n",
    "\n",
    "m2, m2_err  =  quad(m2_int, 0.0, 1.0)\n",
    "print(\"Second moment: \", m2)\n",
    "\n",
    "# Repeat for the variance\n",
    "def var_int(x):\n",
    "    return  (x - 0.5)**2*uniform.pdf(x)\n",
    "\n",
    "var, var_err  =  quad(var_int, 0.0, 1.0)\n",
    "print(\"Variance: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 3\n",
    "The `scipy.stats` package also includes [methods](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html#common-methods) for simple statistics like these, as demonstrated in the next code cell. The `mean`, `std`, and `var` methods are listed at the end of the help file for the [`uniform`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html) distribution object, and you can use `?` to get additional help for each of them. For example, type `uniform.mean?` in a new code cell to see the help for `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for the standard uniform distribution\n",
      "================================================\n",
      "Mean:  0.5\n",
      "Standard deviation:  0.28867513459481287\n",
      "Variance:   0.08333333333333333\n",
      "Second moment:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Get mean, std, var, and 2nd moment\n",
    "u_mean = uniform.mean()\n",
    "u_std = uniform.std()\n",
    "u_var = uniform.var()\n",
    "u_m2  = uniform.moment(2)\n",
    "\n",
    "print(\"Statistics for the standard uniform distribution\")\n",
    "print(\"================================================\")\n",
    "print(\"Mean: \", u_mean)\n",
    "print(\"Standard deviation: \", u_std)\n",
    "print(\"Variance:  \", u_var)\n",
    "print(\"Second moment: \", u_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 4\n",
    "We can also use the `loc` and `scale` keywords to get information about more general uniform distributions. These keywords have different meanings for different distributions, so check the documentation. With [`uniform`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html), the distribution interval is given by `[loc, loc + scale]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P in [0.0, 0.5]:  0.25\n",
      "\n",
      "Statistics for the general uniform distribution\n",
      "with loc = -1.0 and scale = 2.0\n",
      "================================================\n",
      "Mean:  0.0\n",
      "Standard deviation:  0.5773502691896257\n",
      "Variance:   0.3333333333333333\n",
      "Second moment:  0.33333333333333326\n"
     ]
    }
   ],
   "source": [
    "# Define U(0,2)\n",
    "# Lower bound is `loc`, width is `scale`\n",
    "loc = -1.0\n",
    "scale = 2.0\n",
    "def ugen(x):\n",
    "    return uniform.pdf(x, loc, scale)\n",
    "\n",
    "# Compute P for x in [0.0, 0.5]\n",
    "Pab, Pab_err = quad(ugen, 0.0, 0.5)\n",
    "print(\"P in [0.0, 0.5]: \", Pab)\n",
    "print()\n",
    "\n",
    "# Get mean, std, var, 2nd moment\n",
    "uab_mean = uniform.mean(loc=-1.0, scale=2.0)\n",
    "uab_std = uniform.std(loc=-1.0, scale=2.0)\n",
    "uab_var = uniform.var(loc=-1.0, scale=2.0)\n",
    "uab_m2 = uniform.moment(2, loc=-1.0, scale=2.0)\n",
    "\n",
    "print(\"Statistics for the general uniform distribution\")\n",
    "print(\"with loc = -1.0 and scale = 2.0\")\n",
    "print(\"================================================\")\n",
    "print(\"Mean: \", uab_mean)\n",
    "print(\"Standard deviation: \", uab_std)\n",
    "print(\"Variance:  \", uab_var)\n",
    "print(\"Second moment: \", uab_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "A digital scale tells me that my morning coffee beans have a mass of 25.0 grams. What is the standard deviation of the parent distribution for this measurement? Compare this to the recommendation in *MU*, Sec. 1.3.2, that we report digital uncertainties as the smallest digital increment. Either this convention or the *mean* Â± *standard deviation* convention are acceptable ways to report the uncertainty, but it is important to recognize that they are different and to communicate clearly which one you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 1\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Show analytically that for $P_\\text{DF}(x) = \\mathcal{U}(x;a,b)$,\n",
    "\n",
    "$$ \\left\\langle(x-\\mu)^2\\right\\rangle = \\frac{(b-a)^2}{12}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markdown cell for Exercise 2**\n",
    "\n",
    "Select this cell and enter your response here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability calculations with the normal distribution\n",
    "By far the most common probability calculations in data analysis involve the normal distribution. Other than the [normalization integral](2.1-Evaluating-the-Gaussian-integral.ipynb), we must rely on numerical methods to evaluate definite integrals over Gaussian integrands. Below we demonstrate techniques for reproducing one of the example calculations in *MU* Sec. 3.2.2.\n",
    "\n",
    "**Important: The definition of the *error function* used in *MU* is not standard.** The function defined in *MU* Eq. (3.8) is more commonly called the [*cumulative distribution function*](https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function) (CDF) for the normal distribution, and is represented by the Greek letter $\\Phi$. We will adopt this more standard notation here (including the choice of $\\sigma^2$ instead of $\\sigma$ to parameterize it) and rewrite Eq. (3.8) as\n",
    "\n",
    "<a id=\"MU(3.8)\"></a>$$ \\Phi(x_1; \\mu, \\sigma^2) = \\int_{-\\infty}^{x_1}\\text{d}x\\,\\mathcal{N}(x; \\mu, \\sigma^2). $$\n",
    "\n",
    "The more usual definition of the [*error function*](https://en.wikipedia.org/wiki/Error_function) is\n",
    "\n",
    "<a id=\"erf\"></a>$$ \\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}\\text{d}t\\,e^{-t^2} = \\frac{1}{\\sqrt{\\pi}}\\int_{-x}^{x}\\text{d}t\\,e^{-t^2},$$\n",
    "\n",
    "which gives the area under $\\mathcal{N}(\\mu=0,\\sigma^2 =  1/2)$ between $-x$ and $x$. (The dummy variable $t$ is also commonly used in the standard definition, so we have preserved that here. *MU* uses $x$ as the dummy variable and $x_1$ as the argument to the cumulative distribution function, which we also preserve.)\n",
    "\n",
    "This, more standard definition of the error function is related to $\\Phi$ through\n",
    "\n",
    "<a id=\"erf2cdf\"></a>\\begin{align}\n",
    "\\Phi(x_1; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{x_1}\\text{d}x\\,\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\\\\\n",
    "&= \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{t_1}\\text{d}t\\,e^{-t^2},\\quad \\text{with}\\ t_1 = \\frac{x_1-\\mu}{\\sqrt{2}\\sigma}\\\\\n",
    "&= \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x_1 - \\mu}{\\sqrt{2}\\sigma}\\right)\\right].\n",
    "\\end{align}\n",
    "\n",
    "Now we are ready to reproduce the first example calculation in *MU* Sec. 3.2.2, where we have a box of resistors that are normally distributed with $\\mu = 100~\\Omega$ and $\\sigma = 2~\\Omega$, and we want to determine the probability of picking a resistor with a resistance less than 95&nbsp;&Omega;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programming notes 5\n",
    "We import the error function [`erf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erf.html) from [`scipy.special`](https://docs.scipy.org/doc/scipy/reference/special.html), the special functions package of SciPy. We use the `cdf` method of the normal distribution object [`norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) from `scipy.stats`. The result using `erf` and `norm.cdf` are within the numerical precision of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability (erf):  0.006209665325776159\n",
      "Probability (cdf):  0.006209665325776132\n"
     ]
    }
   ],
   "source": [
    "# Import error function\n",
    "from scipy.special import erf\n",
    "\n",
    "# Assign variables\n",
    "mu_R = 100\n",
    "sigma_R = 2\n",
    "R1 = 95\n",
    "\n",
    "# Compute scaled variable t1_a for the error function\n",
    "t1 = (R1 - mu_R)/(np.sqrt(2)*sigma_R)\n",
    "\n",
    "# Evaluate CDF using the error function\n",
    "P_erf = (1 + erf(t1))/2\n",
    "print(\"Probability (erf): \", P_erf)\n",
    "\n",
    "# Evaluate CDF using the cdf method of norm\n",
    "P_cdf = norm.cdf(R1, loc=mu_R, scale=sigma_R)\n",
    "print(\"Probability (cdf): \", P_cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Reproduce the second example in *MU* Sec. 3.2.2; that is, for a box of resistors that are normally distributed with $\\mu = 100~\\Omega$ and $\\sigma = 2~\\Omega$, determine the probability of picking a resistor with a resistance between 99&nbsp;&Omega; and 101&nbsp;&Omega;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell for Exercise 3\n",
    "# Use this cell for your response, adding cells if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers, or: Chauvenet is dead. Let his criterion die with him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The central limit theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3.2 Rejecting outliers\n",
    "### 3.3.3 Experimental example of a Gaussian distribution\n",
    "### 3.3.4 Comparing experimental results with an accepted value\n",
    "### Poisson probability function for discrete events \n",
    "\n",
    "The Scipy function poisson.pmf returns the Poisson distribution function.\n",
    "Reproduce Fig. 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "# define variables\n",
    "n = range(8)\n",
    "n_bar = 1.5\n",
    "# generate a Poisson distribution\n",
    "p = poisson.pmf(n, n_bar)\n",
    "\n",
    "plt.bar(n, p)\n",
    "plt.xlim([-0.5, 8.5])\n",
    "plt.xlabel('Number of counts, n')\n",
    "plt.ylabel('Posson PDF')\n",
    "plt.text(7.5, 0.3, '(a)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "n = range(40)\n",
    "n_bar = 15\n",
    "# generate a poisson distribution\n",
    "p = poisson.pmf(n, n_bar)\n",
    "\n",
    "plt.bar(n, p)\n",
    "\n",
    "plt.xlim([-0.5, 40.5])\n",
    "plt.xlabel('Number of counts, n')\n",
    "plt.ylabel('Posson PDF')\n",
    "plt.text(37, 0.08, '(b)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4.1 Worked example - Poisson counts\n",
    "### 3.4.2 Error bars and confidence limits for Poisson statistics\n",
    "### 3.4.3 Approximations for higher means\n",
    "### 3.5 The central limit theorem\n",
    "### 3.5.1 Examples of the central limit theorem\n",
    "Reproduce the top row of Fig. 3.7. The left-most plot is the PDF for a uniform distribution over (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = np.linspace(0, 1, 100)\n",
    "f = np.ones(x_var.shape)\n",
    "\n",
    "plt.plot(x_var, f)\n",
    "plt.xlabel('Varriable x')\n",
    "plt.ylabel('Probability Density, f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center plot is a histogram of 1000 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "x = np.random.rand(1000)\n",
    "\n",
    "plt.hist(x, 10, ec='black')\n",
    "plt.xlabel('Variable, x')\n",
    "plt.ylabel('Occurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-most plot is a histogram of 1000 trials, where the result of each trial is the mean of five numbers drawn from the uniform distribution. To simplify the computation, note that the mean function operates independently on the columns of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generage a 5 x 1000 matrix of random numbers\n",
    "x = np.random.rand(5, 1000)\n",
    "# calulate the mean value of each column\n",
    "x_bar = np.mean(x, axis=0)\n",
    "\n",
    "plt.hist(x_bar, 20, ec='black')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Occurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how this evolves from N=2 to N=5, using 10,000 trials to get better statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_start = 2\n",
    "n_end = 5 + 1\n",
    "trials = 10000\n",
    "\n",
    "\n",
    "for i in range(n_start, n_end):\n",
    "\n",
    "    plt.subplot(2, 2, i - n_start  + 1)\n",
    "\n",
    "    x = np.random.rand(i, trials)\n",
    "    x_bar = np.mean(x, axis=0)\n",
    "\n",
    "    plt.hist(x_bar, 20, ec='black')\n",
    "    plt.xlabel('Mean')\n",
    "    plt.ylabel('Occurrence')\n",
    "    plt.title('n = ' + str(i))\n",
    "# adjust spacing between subplots to minimize the overlaps\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
